{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2e62692",
   "metadata": {},
   "source": [
    "## CHAPTER6.다양한 데이터 타입 다루기\n",
    "- 불리언 타입\n",
    "- 수치 타입\n",
    "- 문자열 타입\n",
    "- date와 timestamp 타입\n",
    "- null값 다루기\n",
    "- 복합 데이터 타입\n",
    "- 사용자 정의 함수\n",
    "\n",
    "### API는 어디서 찾을까\n",
    "데이터 변환용 함수\n",
    "- DataFrame(Dataset) 메서드\n",
    "> DataFrame은 Row 타입을 가진 Dataset -> Dataset의 메서드를 만나게 됨(내려받음?)\\\n",
    "> DataFrameStatFunctions: 다양한 통계적 함수 제공\\\n",
    "> DataFrameNaFunctions: NULL 데이터를 다루는 데 필요한 함수 제공\n",
    "\n",
    "- Column 메서드\n",
    "> alias, contains 같은 컬럼 관련된 여러가지 메서드 제공\n",
    "> ColumnAPI는 스파크 문서 참고(https://spark.apache.org/docs/3.1.1/api/java/org/apache/spark/sql/Column.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c59302b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|2010-12-01 08:26:00|     7.65|   17850.0|United Kingdom|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|2010-12-01 08:26:00|     4.25|   17850.0|United Kingdom|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|2010-12-01 08:34:00|     1.69|   13047.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"CSV\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .option(\"inferSchema\",\"true\")\\\n",
    "    .load(\"../../assets/presentations/week06/data/retail-data/by-day/2010-12-01.csv\")\n",
    "df.printSchema()\n",
    "df.createOrReplaceTempView(\"dfTable\") # Session scope\n",
    "spark.sql(\"SELECT * FROM dfTable LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e45de4b",
   "metadata": {},
   "source": [
    "### 스파크 데이터 타입으로 변환하기\n",
    "- lit 함수 사용하여 데이터 변환\n",
    "- SQL에서는 스파크 데이터 타입으로 변환할 필요 없음 -> 값 직접 입력하여 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e7620a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[5: int, five: string, 5.0: double]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df.select(lit(5), lit(\"five\"), lit(5.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681025af",
   "metadata": {},
   "source": [
    "### 불리언 데이터 타입 다루기\n",
    "- 필터링 작업의 기반으로 데이터 분석에 필수적\n",
    "- and, or, true, false\n",
    "- 일치 조건 이외 작다, 크다와 같은 비교 연산 조건 사용 가능\n",
    "\n",
    "** 불리언 표현식을 만들 때 null 값 데이터 다루기(null값에 안전한 동치 테스트 수행 예제 제공)\n",
    "\n",
    "    df.where(col(\"Description\").eqNullSafe(\"hello\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7c4a674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------------+\n",
      "|InvoiceNo|Description                  |\n",
      "+---------+-----------------------------+\n",
      "|536366   |HAND WARMER UNION JACK       |\n",
      "|536366   |HAND WARMER RED POLKA DOT    |\n",
      "|536367   |ASSORTED COLOUR BIRD ORNAMENT|\n",
      "|536367   |POPPY'S PLAYHOUSE BEDROOM    |\n",
      "|536367   |POPPY'S PLAYHOUSE KITCHEN    |\n",
      "+---------+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df.where(col(\"InvoiceNo\") != 536365)\\\n",
    "    .select(\"InvoiceNo\", \"Description\")\\\n",
    "    .show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58f5d5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|Description                        |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |\n",
      "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "|536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER |6       |2010-12-01 08:26:00|2.55     |17850.0   |United Kingdom|\n",
      "|536365   |71053    |WHITE METAL LANTERN                |6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n",
      "|536365   |84406B   |CREAM CUPID HEARTS COAT HANGER     |8       |2010-12-01 08:26:00|2.75     |17850.0   |United Kingdom|\n",
      "|536365   |84029G   |KNITTED UNION FLAG HOT WATER BOTTLE|6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n",
      "|536365   |84029E   |RED WOOLLY HOTTIE WHITE HEART.     |6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n",
      "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+---------+-----------------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|Description                  |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |\n",
      "+---------+---------+-----------------------------+--------+-------------------+---------+----------+--------------+\n",
      "|536366   |22633    |HAND WARMER UNION JACK       |6       |2010-12-01 08:28:00|1.85     |17850.0   |United Kingdom|\n",
      "|536366   |22632    |HAND WARMER RED POLKA DOT    |6       |2010-12-01 08:28:00|1.85     |17850.0   |United Kingdom|\n",
      "|536367   |84879    |ASSORTED COLOUR BIRD ORNAMENT|32      |2010-12-01 08:34:00|1.69     |13047.0   |United Kingdom|\n",
      "|536367   |22745    |POPPY'S PLAYHOUSE BEDROOM    |6       |2010-12-01 08:34:00|2.1      |13047.0   |United Kingdom|\n",
      "|536367   |22748    |POPPY'S PLAYHOUSE KITCHEN    |6       |2010-12-01 08:34:00|2.1      |13047.0   |United Kingdom|\n",
      "+---------+---------+-----------------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(\"InvoiceNo = 536365\")\\\n",
    "    .show(5, False)\n",
    "df.where(\"InvoiceNo <> 536365\")\\\n",
    "    .show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2eb5637b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df.where(col(\"Description\").eqNullSafe(\"hello\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964d40ba",
   "metadata": {},
   "source": [
    "### 수치형 데이터 타입 다루기\n",
    "- pow: 표시된 지수만큼 컬럼 값 거듭제곱\n",
    "- round/bround: 소수점 값이 정확히 중간값 이상이면 반올림 / 내림 함수\n",
    "- DataFrame의 통계용 함수나 메서드를 사용가능(ex.피어슨 상관계수)\n",
    "- 하나 이상의 컬럼에 대한 요약 통계 확인 가능(describe() 메서드)\n",
    "    - 통계 스키마는 변경될 수 있으므로, describe 메서드는 콘솔 확인용으로만 사용할 것 권장\n",
    "- StatFunrctions 패키지\n",
    "    - stat 속성을 사용해 접근\n",
    "    - ex1.qpproxQuantile: 데이터의 백분위수 계산\n",
    "    - ex2.crosstab\n",
    "    - ex3.monotonically_increasing_id: 모든 로우에 고유 ID 값 추가(0부터 시작하는 고윳값 생성)\n",
    "- 함수는 새 버전 릴리스 마다 생겨남. API 문서 참고\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6597c766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerId|      realQuantity|\n",
      "+----------+------------------+\n",
      "|   17850.0|239.08999999999997|\n",
      "|   17850.0|          418.7156|\n",
      "+----------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pow 함수 예제\n",
    "from pyspark.sql.functions import expr, pow\n",
    "\n",
    "fabricatedQuantity = pow(col(\"Quantity\")*col(\"UnitPrice\"),2) + 5\n",
    "df.select(expr(\"CustomerId\"),fabricatedQuantity.alias(\"realQuantity\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c3621f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+\n",
      "|round(2.5, 0)|bround(2.5, 0)|\n",
      "+-------------+--------------+\n",
      "|          3.0|           2.0|\n",
      "|          3.0|           2.0|\n",
      "+-------------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# round\n",
    "from pyspark.sql.functions import lit, round, bround\n",
    "df.select(round(lit(\"2.5\")), bround(lit(\"2.5\"))).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5bb83479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|corr(Quantity, UnitPrice)|\n",
      "+-------------------------+\n",
      "|     -0.04112314436835551|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 피어슨 상관계수 예제\n",
    "from pyspark.sql.functions import corr\n",
    "\n",
    "# df.stat.corr(\"Quantity\",\"UnitPrice\")\n",
    "df.select(corr(\"Quantity\",\"UnitPrice\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4fe74c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+\n",
      "|summary|        InvoiceNo|         StockCode|         Description|          Quantity|         UnitPrice|        CustomerID|       Country|\n",
      "+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+\n",
      "|  count|             3108|              3108|                3098|              3108|              3108|              1968|          3108|\n",
      "|   mean| 536516.684944841|27834.304044117645|                null| 8.627413127413128| 4.151946589446603|15661.388719512195|          null|\n",
      "| stddev|72.89447869788873|17407.897548583845|                null|26.371821677029203|15.638659854603892|1854.4496996893627|          null|\n",
      "|    min|           536365|             10002| 4 PURPLE FLOCK D...|               -24|               0.0|           12431.0|     Australia|\n",
      "|    max|          C536548|              POST|ZINC WILLIE WINKI...|               600|            607.49|           18229.0|United Kingdom|\n",
      "+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# describe\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "12fb720b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "| StockCode_freqItems|  Quantity_freqItems|\n",
      "+--------------------+--------------------+\n",
      "|[90214E, 20728, 2...|[200, 128, 23, 32...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# crosstab\n",
    "# df.stat.crosstab(\"StockCode\",\"Quantity\").show(2)\n",
    "# freqItems\n",
    "df.stat.freqItems([\"StockCode\",\"Quantity\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "90c85847",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+---------+--------------------+\n",
      "|monotonically_increasing_id()|InvoiceNo|         Description|\n",
      "+-----------------------------+---------+--------------------+\n",
      "|                            0|   536365|WHITE HANGING HEA...|\n",
      "|                            1|   536365| WHITE METAL LANTERN|\n",
      "+-----------------------------+---------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# monotonically_increasing_id\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "df.select(monotonically_increasing_id(),\"InvoiceNo\",\"Description\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558a05b4",
   "metadata": {},
   "source": [
    "### 문자열 데이터 타입 다루기\n",
    "- 정규표현식 사용\n",
    "    - java 정규 표현식 사용\n",
    "    - regexp_extract: 정규 표현식에 해당하는 값 추출\n",
    "    - regexp_replace: 정규 표현식에 해당하는 값 치환\n",
    "- 데이터 추출\n",
    "- 데이터 치환\n",
    "- 문자열 존재 여부\n",
    "- 대/소문자 변환 처리 등\n",
    "- lower, upper, lpad, ltrim, rpad, rtrim ...\n",
    "- translate: 교체 문자열에서 색인된 모든 문자열 치환/문자 단위 연산\n",
    "- contains(스칼라) / instr(파이썬, SQL): 값 존재 여부 확인\n",
    "- locate(파이썬): 문자열의 위치를 정술 반환(1부터 시작)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2cda2118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|         color_clean|         Description|\n",
      "+--------------------+--------------------+\n",
      "|COLOR HANGING HEA...|WHITE HANGING HEA...|\n",
      "| COLOR METAL LANTERN| WHITE METAL LANTERN|\n",
      "+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 정규표현식 예제\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "regex_string = \"BLACK|WHITE|RED|GREEN|BLUE\"\n",
    "df.select(\n",
    "    regexp_replace(col(\"Description\"), regex_string, \"COLOR\").alias(\"color_clean\"),\n",
    "    col(\"Description\")).show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "51c8411d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Column<'CAST(locate(BLACK, Description, 1) AS BOOLEAN) AS is_black'>, Column<'CAST(locate(WHITE, Description, 1) AS BOOLEAN) AS is_white'>, Column<'CAST(locate(RED, Description, 1) AS BOOLEAN) AS is_red'>, Column<'CAST(locate(GREEN, Description, 1) AS BOOLEAN) AS is_green'>, Column<'CAST(locate(BLUE, Description, 1) AS BOOLEAN) AS is_blue'>]\n",
      "+----------------------------------+\n",
      "|Description                       |\n",
      "+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHITE METAL LANTERN               |\n",
      "|RED WOOLLY HOTTIE WHITE HEART.    |\n",
      "+----------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# locate 예제\n",
    "from pyspark.sql.functions import expr,locate\n",
    "\n",
    "simpleColors = [\"black\",\"white\",\"red\",\"green\",\"blue\"]\n",
    "def color_locator(column, color_string):\n",
    "    return locate(color_string.upper(), column)\\\n",
    "        .cast(\"boolean\")\\\n",
    "        .alias(\"is_\" + color_string)\n",
    "selectedColumns = [color_locator(df.Description, c) for c in simpleColors]\n",
    "print(selectedColumns)\n",
    "selectedColumns.append(expr(\"*\"))\n",
    "\n",
    "df.select(*selectedColumns).where(expr(\"is_white OR is_red\"))\\\n",
    "    .select(\"Description\").show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d742ab3",
   "metadata": {},
   "source": [
    "### 날짜와 타임스탬프 데이터 타입 다루기\n",
    "- date: 날짜\n",
    "- timestamp: 날짜와 시간 정보 모두\n",
    "- 날짜나 시간을 문자열로 저장 -> 런타임에 날짜 타입으로 변환하는 경우 있음\n",
    "    - 시간대 설정이 필요하다면 spark.conf.sessionLocalTimeZone 속성을 로컬 시간대로 지정해 적용 가능\n",
    "    - 시간대 포맷은 반드시 자바 TimeZone 포맷을 따라야 함.\n",
    "- 날짜와 시간을 최대한 올바른 형태로 읽기 위해 노력(...)\n",
    "- 특이한 포맷의 날짜와 시간데이터를 어쩔 수 없이 다뤄야 한다면, 단계별로 어떤 데이터 타입과 포맷을 유지하는지 정확히 파악 후 트랜스포메이션 적용해야 한다.\n",
    "- TimestampType 클래스는 초 단위 정밀도까지만 지원\n",
    "    - 밀리세컨드, 마이크로세컨드 단위를 다룬다면 Long 데이터 타입으로 변화냏 처리하는 우회정책 사용 필요\n",
    "- date_sub, date_add: 날짜 빼기,더하기\n",
    "- datetdiff: 두 날짜 사이의 일수 반환\n",
    "- months_between: 두 날짜 사이의 개월 수 반환\n",
    "- to_date: 문자열을 날짜로 변환, 날짜 포맷 지정 가능(날짜 포맷은 자바의 SimpleDateFormat 클래스가 지원하는 포맷 사용)\n",
    "- to_timestamp: 반드시 날짜 포맷 지정해야 함.(yyyy-MM-dd HH:mm:ss 포맷 기본값 사용)\n",
    "- 스파크가 날짜를 파싱할 수 없는 경우\n",
    "    - 에러가 아닌 null 값을 반환\n",
    "    - 데이터 포맷이 지정된 데이터에서 다른 포맷 데이터가 발생 가능\n",
    "    - 정확한 포맷 지정 권장.\n",
    "- 명시적 형변환 추천"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ebef00e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|to_date(2016-20-12)|to_date(2017-12-11)|\n",
      "+-------------------+-------------------+\n",
      "|               null|         2017-12-11|\n",
      "+-------------------+-------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# null값 반환\n",
    "from pyspark.sql.functions import current_date, current_timestamp\n",
    "from pyspark.sql.functions import to_date, lit\n",
    "dateDF = spark.range(10)\\\n",
    "    .withColumn(\"today\",current_date())\\\n",
    "    .withColumn(\"now\", current_timestamp())\n",
    "dateDF.select(to_date(lit(\"2016-20-12\")), to_date(lit(\"2017-12-11\"))).show(1) \n",
    "#년-일-월 포맷. 스파크가 의도에 맞게 인식 불가 + 파싱 불가로 null 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ecb54731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------------+\n",
      "|      date|              date2|              date3|\n",
      "+----------+-------------------+-------------------+\n",
      "|2017-11-12|2017-12-20 00:00:00|2017-11-12 00:00:00|\n",
      "+----------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 포맷 지정 # date3는 포맷 지정X로 잘못된 값 들어가는 케이스(월일 뒤바뀌었음)\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "dateFormat = \"yyyy-dd-MM\"\n",
    "cleanDateDF = spark.range(1).select(\n",
    "    to_date(lit(\"2017-12-11\"), dateFormat).alias(\"date\"),\n",
    "    to_timestamp(lit(\"2017-20-12\"), dateFormat).alias(\"date2\"),\n",
    "    to_timestamp(lit(\"2017-11-12\")).alias(\"date3\"))\n",
    "cleanDateDF.createOrReplaceTempView(\"dateTable2\")\n",
    "\n",
    "spark.sql(\"SELECT date, date2, date3 FROM dateTable2\").show()\n",
    "# spark.sql(\"SELECT to_date(date,'yyyy-dd-MM'),to_date(date2,'yyyy-dd-MM'),to_date(date) FROM dateTable2\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e45261",
   "metadata": {},
   "source": [
    "### null값 다루기\n",
    "- 스파크에서는 빈 문자열이나 대체 값 대신 null값을 사용해야 최적화를 수행할 수 있음\n",
    "- null 값을 허용하지 않는다 선언해도 강제성 없음.\n",
    "- nullable 속성은 스파크 SQL 옵티마이저가 해당 컬럼을 제어하는 동작을 단순하게 도울 뿐\n",
    "- coalesce: 인수로 지정한 여러 컬럼 중 null이 아닌 첫번 째 값을 반환. 모든 컬럼이 null이 아니라면 첫 번재 컬럼 반환\n",
    "- ifnull: 첫 번째 값이 null이면 두 번째 값 반환. 첫 번째 값이 null이 아니라면 첫 번째 값 반환\n",
    "- nullif: 두 값이 같으면 null 반환. 두 값이 다르면 첫 번째 값을 반환\n",
    "- nvl: 첫 번째 값이 null이면 두 번째 값을 반환. 첫 번째 값이 null이 아니라면 첫 번째 값 반환\n",
    "- nvl2: 첫 번째 값이 null이 아니면 두 번째 값을 반환. 첫 번째 값이 null이면 세 번째 인수로 지정된 값을 반환\n",
    "\n",
    "### drop\n",
    "- null값을 가진 로우 제거하는 가장 간단한 함수. null값을 가진 모든 로우 제거\n",
    "- drop 메서드의 인수로 any로 지정한 경우 로우의 컬럼 값 중 하나라도 null값을 가지면 해당 로우를 제거. \n",
    "- all을 지정한 경우 모든 컬럼의 값이 null 혹은 NaN인 경우에만 해당 로우를 제거\n",
    "- drop 메서드에 배열 형태의 컬럼을 인수로 전달해 적용 가능\n",
    "\n",
    "### fill\n",
    "- 하나 이상의 컬럼을 특정 값으로 채울 수 있음.\n",
    "- 채워 넣을 값과 컬럼 집합으로 구성된 맵을 인수로 사용\n",
    "- 다수의 컬럼에 적용하고 싶다면 적용하고자 하는 컬럼명을 배열로 만들어 인수로 사용\n",
    "\n",
    "### replace\n",
    "- 조건에 따라 다른 값으로 대체\n",
    "- replace 메서드를 사용하는 경우 변경하고자 하는 값과 원래 값의 데이터 타입이 같아야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5d0e4139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop\n",
    "df.na.drop()\n",
    "df.na.drop(\"any\")\n",
    "df.na.drop(\"all\")\n",
    "df.na.drop(\"all\",subset=[\"StockCode\",\"InvoiceNo\"])\n",
    "\n",
    "# fill\n",
    "df.na.fill(\"All Null values become this string\")\n",
    "\n",
    "# replace\n",
    "df.na.replace([\"\"], [\"UNKNOWN\"], \"Description\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caa3140",
   "metadata": {},
   "source": [
    "### 정렬하기\n",
    "- asc_nulls_first, desc_nulls_first, asc_nulls_last, desc_nulls_last\n",
    "- DataFrame을 정렬할 때 null 값 표시 기준 지정 가능\n",
    "\n",
    "### 복합 데이터 타입 다루기\n",
    "- 구조체(struct)\n",
    "    - DataFrame 내부의 DataFrame\n",
    "    - 쿼리문에서 다수의 컬럼을 괄호로 묶어 구조체 생성 가능\n",
    "    - 차이점: 문법에 . 혹은 getField 메서드 사용\n",
    "- 배열(array)\n",
    "    - split: 구분자를 인수로 전달하여 배열로 변환\n",
    "    - size: 배열의 길이 조회\n",
    "    - array_contains: 배열에 특정 값이 존재하는지 확인\n",
    "    - explode: 배열 타입의 컬럼을 입력 받아, 입력된 컬럼의 배열값에 포함된 모든 값을 로우로 변환. 나머지 컬럼값은 중복되어 표시\n",
    "- 맵(map)\n",
    "    - 함수와 컬럼의 키-값 쌍을 이용해 생성\n",
    "    - 배열과 동일한 방법으로 값을 선택할 수 있음.\n",
    "    - 적합한 키를 사용해 데이터 조회\n",
    "    - 해당 키가 없다면 null 값을 반환\n",
    "    - map 타입은 분해하여 컬럼으로 변환 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f406e7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "| complex.Description|\n",
      "+--------------------+\n",
      "|WHITE HANGING HEA...|\n",
      "+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+--------------------+---------+\n",
      "|         Description|InvoiceNo|\n",
      "+--------------------+---------+\n",
      "|WHITE HANGING HEA...|   536365|\n",
      "+--------------------+---------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# struct\n",
    "from pyspark.sql.functions import struct\n",
    "\n",
    "complexDF = df.select(struct(\"Description\",\"InvoiceNo\").alias(\"complex\"))\n",
    "complexDF.createOrReplaceTempView(\"complexDF\")\n",
    "complexDF.select(\"complex.Description\")\n",
    "complexDF.select(col(\"complex\").getField(\"Description\")).show(1)\n",
    "complexDF.select(\"complex.*\").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "873e213c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|array_col[0]|\n",
      "+------------+\n",
      "|       WHITE|\n",
      "|       WHITE|\n",
      "+------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------------------------------+\n",
      "|size(split(Description,  , -1))|\n",
      "+-------------------------------+\n",
      "|                              5|\n",
      "|                              3|\n",
      "+-------------------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+------------------------------------------------+\n",
      "|array_contains(split(Description,  , -1), WHITE)|\n",
      "+------------------------------------------------+\n",
      "|                                            true|\n",
      "|                                            true|\n",
      "+------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+--------------------+---------+--------+\n",
      "|         Description|InvoiceNo|exploded|\n",
      "+--------------------+---------+--------+\n",
      "|WHITE HANGING HEA...|   536365|   WHITE|\n",
      "|WHITE HANGING HEA...|   536365| HANGING|\n",
      "|WHITE HANGING HEA...|   536365|   HEART|\n",
      "|WHITE HANGING HEA...|   536365| T-LIGHT|\n",
      "|WHITE HANGING HEA...|   536365|  HOLDER|\n",
      "| WHITE METAL LANTERN|   536365|   WHITE|\n",
      "| WHITE METAL LANTERN|   536365|   METAL|\n",
      "| WHITE METAL LANTERN|   536365| LANTERN|\n",
      "|CREAM CUPID HEART...|   536365|   CREAM|\n",
      "|CREAM CUPID HEART...|   536365|   CUPID|\n",
      "+--------------------+---------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# array\n",
    "from pyspark.sql.functions import split\n",
    "df.select(split(col(\"Description\"), \" \").alias(\"array_col\"))\\\n",
    "    .selectExpr(\"array_col[0]\").show(2)\n",
    "\n",
    "from pyspark.sql.functions import size\n",
    "df.select(size(split(col(\"Description\"), \" \"))).show(2)\n",
    "\n",
    "from pyspark.sql.functions import array_contains\n",
    "df.select(array_contains(split(col(\"Description\"), \" \"), \"WHITE\")).show(2)\n",
    "\n",
    "from pyspark.sql.functions import explode\n",
    "df.withColumn(\"splitted\", split(col(\"Description\"), \" \"))\\\n",
    "    .withColumn(\"exploded\", explode(col(\"splitted\")))\\\n",
    "    .select(\"Description\",\"InvoiceNo\",\"exploded\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "89694e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         complex_map|\n",
      "+--------------------+\n",
      "|{WHITE HANGING HE...|\n",
      "|{WHITE METAL LANT...|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+--------------------------------+\n",
      "|complex_map[WHITE METAL LANTERN]|\n",
      "+--------------------------------+\n",
      "|                            null|\n",
      "|                          536365|\n",
      "+--------------------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+--------------------+------+\n",
      "|                 key| value|\n",
      "+--------------------+------+\n",
      "|WHITE HANGING HEA...|536365|\n",
      "| WHITE METAL LANTERN|536365|\n",
      "+--------------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# map\n",
    "from pyspark.sql.functions import create_map\n",
    "df.select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\"))\\\n",
    "    .show(2)\n",
    "\n",
    "df.select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\"))\\\n",
    "    .selectExpr(\"complex_map['WHITE METAL LANTERN']\").show(2)\n",
    "\n",
    "df.select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\"))\\\n",
    "    .selectExpr(\"explode(complex_map)\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8f7437",
   "metadata": {},
   "source": [
    "### JSON 다루기\n",
    "- 스파크에서 문자열 형태의 JSON 직접 조작 가능\n",
    "- JSON 파싱\n",
    "- JSON 객체로 만들 수 있음.\n",
    "- get_json_object: JSON 객체(딕셔너리나 배열)를 인라인 쿼리로 조회 가능\n",
    "- json_tuple\n",
    "- to_json: StructType을 JSON 문자열로 변경 가능\n",
    "- from_json: JSON 문자열을 다시 객체로 변환. 파라미터로 반드시 스키마 지정 필요. \n",
    "\n",
    "### 사용자 정의 함수\n",
    "- UserDefinedFunction, UDF\n",
    "- 파이썬이나 스칼라 그리고 외부 라이브러리를 사용해 사용자가 원하는 형태로 트랜스포메이션을 만들 수 있게끔 한다.\n",
    "- 여러가지 프로그래밍 언어로 개발할 수 있어 강력\n",
    "    - 스칼라, 파이썬 자바로 UDF 개발 가능. 하지만 언어별로 성능에 영향을 미칠 수 있으므로 주의 필요\n",
    "- 레코드별로 데이터를 처리하는 함수이기에 독특한 포맷이나 도메인에 특화된 언어를 사용하지 않음.\n",
    "- 특정 SparkWession이나 Context에서 사용할 수 있도록 임시 함수 형태로 등록\n",
    "<img src=\"../../assets/presentations/week06/python_udf.png\" width=\"500px\" height=\"250px\" title=\"catalyst_optimizer\"/>\n",
    "\n",
    "### Hive UDF\n",
    "- 하이브 문법을 사용해서 만든 UDF/UDAF 사용 가능\n",
    "- SparkSession.builder().enableHiveSupport(): 하이브 지원 기능 활성화(명시 필수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ace541",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83aaec6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b6b395",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6000efff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
