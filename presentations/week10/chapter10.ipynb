{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e53a46b1",
   "metadata": {},
   "source": [
    "### Chapter10.스파크 SQL\n",
    "\n",
    "#### 10.1 SQL이란\n",
    "- 구조적 질의 언어\n",
    "- 스파크는 ANSISQL:2003의 일부 구현\n",
    "\n",
    "#### 10.2 빅데이터와 SQL: 아파치 하이브\n",
    "\n",
    "#### 10.3 빅데이터와 SQL: 스파크 SQL\n",
    "- 스파크 2.0 버전 -> ANSI-SQL과 HiveQL을 모두 지원하는 자체 개발된 SQL 파서 포함\n",
    "- 스파크 SQL은 DataFrame과의 호환성 좋음\n",
    "    - SQL로 데이터 죄회, DataFrame으로 변환 후 스파크의 MLlib 활용 머신러닝 알고리즘 수행 결과를 다른 데이터 소스에 저장하는 등의 과정을 가능하게 함.\n",
    "- OLAP 데이터베이스로 동작\n",
    "\n",
    "#### 10.3.1 스파크와 하이브의 관계\n",
    "- 하이브 메타스토어를 사용\n",
    "\n",
    "#### 10.4 스파크 SQL 쿼리 실행 방법\n",
    "#### 10.4.1 스파크 SQL CLI\n",
    "- 스파크 SQL CLI는 쓰리프트 JDBC 서버와 통신 불가\n",
    "\n",
    "#### 10.4.2 스파크의 프로그래밍 SQL 인터페이스\n",
    "- SparkSession 객체의 sql 메서드 사용\n",
    "- 처리된 결과는 DataFrame을 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ea90bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "# from pyspark.sql import SparkSession\n",
    "# spark = SparkSession \\\n",
    "#     .builder \\\n",
    "#     .appName(\"SparkByExamples.com\") \\\n",
    "#     .enableHiveSupport() \\\n",
    "#     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "094fcc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|(1 + 1)|\n",
      "+-------+\n",
      "|      2|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT 1 + 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f04d393c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.json(\"../../assets/exercises/week10/2015-summary.json\")\\\n",
    "    .createOrReplaceTempView(\"some_sql_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08737f15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT DEST_COUNTRY_NAME, sum(count)\n",
    "    FROM some_sql_view GROUP BY DEST_COUNTRY_NAME\n",
    "\"\"\")\\\n",
    "    .where(\"DEST_COUNTRY_NAME like 'S%'\").where(\"`sum(count)` > 10\")\\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97adb5be",
   "metadata": {},
   "source": [
    "### 10.4.3 스파크 SQL 쓰리프트 JDBC/ODBC 서버\n",
    "- 자바 데이터베이스 연결 인터페이스를 제공\n",
    "- 사용자나 원격 프로그램은 스파크 SQL을 실행하기 위해 이 인터페이스로 스파크 드라이버에 접속 가능\n",
    "- 태블로 같은 비즈니스 인텔리전스 스포트웨어 이용하여 스파크에 접속하는 혀애가 대표적인 활용 사례\n",
    "\n",
    "### 10.5 카탈로그\n",
    "- 스파크SQL에서 가장 높은추상화 단계 카탈로그(catalog)\n",
    "- 메타데이터,데이터베이스,테이블,함수,뷰에 대한 정보를 추상화\n",
    "- 테이블,데이터베이스 그리고 함수를 조회\n",
    "\n",
    "### 10.6 테이블\n",
    "- 스파크에서 테이블을 생성하면 default 데이터베이스에 등록\n",
    "- 스파크 2.x 버전에서 테이블은 항상 데이터를 가지고 있다. -> 임시 테이블의 개념이 없으며 데이터를 가지지않은 뷰만 존재\n",
    "\n",
    "### 10.6.1 스파크 관리형 테이블\n",
    "- 관리형 테이블 / 외부 테이블\n",
    "- DataFrame의 saveAsTable 메서드\n",
    "    - 스파크가 관련된 모든 정보를 추적할 수 있는 관리형 테이블을 만들 수 있음\n",
    "    - 테이블을 읽고 데이터를 스파크 포맷으로 변환한 후 새로운 경로에 저장\n",
    "    - spark.sql.warehouse.dir 속성에 디렉토리 경로를 지정 가능\n",
    "    \n",
    "### 10.6.2 테이블 생성하기\n",
    "- SQL에서 전체 데이터소스 API를 재사용할 수 있는 기능 지원\n",
    "- 스파크는 실행 즉시 테이블을 생성\n",
    "- USING / STORED AS\n",
    "    - 포맷 지정 없을 경우 스파크는 기본적으로 하이브 SerDe 설정 사용(스파크의 자체 직렬화보다 훨씬 느림)\n",
    "    - 하이브 사용자는 STORED AS 구문으로 하이브 테이블 생성 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe15325b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/16 03:42:42 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider JSON. Persisting data source table `default`.`flights_v2` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE flights_v2 (\n",
    "    DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count LONG)\n",
    "    USING JSON OPTIONS (path '../../assets/exercises/week10/2015-summary.json')\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8483a57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"SELECT * FROM flights_v2\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1ea662",
   "metadata": {},
   "source": [
    "### 10.6.3 외부 테이블 생성하기\n",
    "- 외부 테이블 생성: CREATE EXTERNAL TABLE 구문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37eb80f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE EXTERNAL TABLE hive_flights(\n",
    "    DEST_CouNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count LONG)\n",
    "ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LOCATION '../../assets/exercises/week10/flight-data-hive/'\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24e81eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_CouNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM hive_flights\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f4c24f",
   "metadata": {},
   "source": [
    "### 10.6.4 테이블에 데이터 삽입하기\n",
    "- 표준 SQL 문법을 따른다\n",
    "- 특정 파티션에만 저장하고 싶은 경우 파티션 명세를 추가 가능\n",
    "\n",
    "### 10.6.5 테이블 메타데이터 확인하기\n",
    "- DESCRIBE 구문 사용\n",
    "- 테이블의 메타데이터 정보를 반환\n",
    "\n",
    "### 10.6.6 테이블 메타데이터 갱신하기\n",
    "- REFRESH TABLE: 테이블과 관련된 모든 캐싱된 항목(기본적으로 파일)을 갱신\n",
    "- REPAIR TABLE: 카타로그에서 관리하는 테이블의 파티션 정보를 갱신\n",
    "    - 새로운 파티션 정보를 수집하는 데 초점\n",
    "    \n",
    "### 10.6.7 테이블 제거하기\n",
    "- delete 불가 / drop만 가능\n",
    "\n",
    "`DROP TABLE [IF EXISTS]`\n",
    "\n",
    "- 외부 테이블 제거: 데이터는 삭제되지 않지만, 더는 외부 테이블명을 이용해 데이터 조회 불가\n",
    "\n",
    "### 10.6.8 테이블 캐싱하기\n",
    "- DataFrameㅇ과 같이 테이블을 캐시하거나 캐시에서 삭제 가능\n",
    "\n",
    "`CACHE TABLE [tablename]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80661965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/16 03:49:10 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[col_name: string, data_type: string, comment: string]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE TABLE flights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b6de5a",
   "metadata": {},
   "source": [
    "### 10.7 뷰\n",
    "- 단순 쿼리 실행 계획\n",
    "- 쿼리 로직을 체계화 및 재사용 편리하게 가능\n",
    "- 데이터베이스에 설정하는 전역 뷰나 세션별 뷰가 될 수 있음\n",
    "\n",
    "### 10.7.1 뷰 생성하기\n",
    "- 신규 경로에 모든 데이터를 다시 저장하는 대신 단순하게 쿼리 시점에 데이터소스에 트랜스포메이션을 수행\n",
    "    - 대규모 GROUP BY, ROLLUP 같은 트랜스포메이션\n",
    "- 임시 뷰 생성 가능\n",
    "\n",
    "### 10.7.2 뷰 제거하기\n",
    "`DROP VIEW IF EXISTS just_usa_view;`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "66e1f596",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = spark.read.format(\"json\").load(\"../../assets/exercises/week10/2015-summary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7ce77a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.explain of DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.where(\"dest_country_name = 'United States'\").createOrReplaceTempView(\"just_usa_df\")\n",
    "just_usa_df.selectExpr(\"*\").explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0d03ff6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                plan|\n",
      "+--------------------+\n",
      "|== Physical Plan ...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"EXPLAIN SELECT * FROM just_usa_view\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7e36eaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "* Filter (2)\n",
      "+- Scan json  (1)\n",
      "\n",
      "\n",
      "(1) Scan json \n",
      "Output [3]: [DEST_COUNTRY_NAME#199, ORIGIN_COUNTRY_NAME#200, count#201L]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/Users/hjb/Developer/spark-the-definitive-guide-study/assets/exercises/week10/2015-summary.json]\n",
      "PushedFilters: [IsNotNull(DEST_COUNTRY_NAME), EqualTo(DEST_COUNTRY_NAME,United States)]\n",
      "ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:bigint>\n",
      "\n",
      "(2) Filter [codegen id : 1]\n",
      "Input [3]: [DEST_COUNTRY_NAME#199, ORIGIN_COUNTRY_NAME#200, count#201L]\n",
      "Condition : (isnotnull(dest_country_name#199) AND (dest_country_name#199 = United States))\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "just_usa_df.explain(mode='formatted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "974c7ca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(just_usa_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bc2db934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "* Filter (2)\n",
      "+- Scan json  (1)\n",
      "\n",
      "\n",
      "(1) Scan json \n",
      "Output [3]: [DEST_COUNTRY_NAME#247, ORIGIN_COUNTRY_NAME#248, count#249L]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/Users/hjb/Developer/spark-the-definitive-guide-study/assets/exercises/week10/2015-summary.json]\n",
      "PushedFilters: [IsNotNull(DEST_COUNTRY_NAME), EqualTo(DEST_COUNTRY_NAME,United States)]\n",
      "ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:bigint>\n",
      "\n",
      "(2) Filter [codegen id : 1]\n",
      "Input [3]: [DEST_COUNTRY_NAME#247, ORIGIN_COUNTRY_NAME#248, count#249L]\n",
      "Condition : (isnotnull(DEST_COUNTRY_NAME#247) AND (DEST_COUNTRY_NAME#247 = United States))\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights.where(\"DEST_COUNTRY_NAME = 'United States'\").explain(mode='formatted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be635d9",
   "metadata": {},
   "source": [
    "### 10.8 데이터베이스\n",
    "- 스파크에서 실행하는 모든 SQL 명령문은 사용 중인 데이터베이스 범위에서 실행\n",
    "    - 동료와 동일한 컨텍스트나 세션을 공유하는 경우 혼란을 야기할 수 있으므로 사용할 데이터베이스를 반드시 설정해야 함\n",
    "- 전체 데이터 베이스 목록 확인\n",
    "\n",
    "`SHOW DATABASE`\n",
    "\n",
    "### 10.8.1 데이터 베이스 생성하기\n",
    "`CREATE DATABASE some_db`\n",
    "\n",
    "### 10.8.2 데이터 베이스 설정하기\n",
    "`USE some_db`\n",
    "\n",
    "- 현재 어떤 데이터베이스 사용중인지 확인\n",
    "`SELECT current_database()`\n",
    "\n",
    "### 10.8.3 데이터 베이스 제거하기\n",
    "`DROP TABLE IF EXISTS some_db`\n",
    "\n",
    "### 10.9 select 구문\n",
    "### 10.9.1 case when then 구문\n",
    "\n",
    "### 10.10 고급 주제\n",
    "### 10.10.1 복합 데이터 타입\n",
    "- 스파크 SQL에는 구조체, 리스트, 맵 세 가지 핵심 복합 데이터 타입이 존재\n",
    "- 구조체\n",
    "    - 스파크에서 중첩 데이터를 생성하거나 쿼리하는 방법을 제공\n",
    "    - 여러 컬럼이나 표현식을 괄호로 묶어서 생성 가능\n",
    "    - 개별 컬럼 조회 가능: 점(dot) 사용\n",
    "- 리스트\n",
    "    - collect_list 함수 / collect_set 함수 사용 -> 집계 함수\n",
    "    - 위 두 함수는 집계 연산 시에만 사용 가능\n",
    "### 10.10.2 함수\n",
    "- 스파크 SQL이 제공하는 전체 함수 목록을 확인하려면 SHOW FUNCTIONS 구문 사용\n",
    "\n",
    "### 10.10.3 서브쿼리\n",
    "- 쿼리 안에 쿼리를 지정 가능\n",
    "- 상호연관 서브쿼리: 서브쿼리의 정보를 보완하기 위해 쿼리의 외부 범위에 있는 일부 정보 사용 가능\n",
    "- 비상호연관 서브쿼리: 외부 범위에 있는 정보를 사용하지 않음.\n",
    "- 조건절 서브쿼리: 값에 따라 필터링 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "62f200c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = spark.read.format(\"json\").load(\"../../assets/exercises/week10/2015-summary.json\").createOrReplaceTempView(\"flights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0c783690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|           Canada|\n",
      "|           Mexico|\n",
      "|   United Kingdom|\n",
      "|            Japan|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT DEST_COUNTRY_NAME FROM flights GROUP BY DEST_COUNTRY_NAME ORDER BY sum(COUNT) DESC LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b1ad3e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT * FROM flights \n",
    "    WHERE ORIGIN_COUNTRY_NAME IN (\n",
    "        SELECT DEST_COUNTRY_NAME FROM flights GROUP BY DEST_COUNTRY_NAME ORDER BY sum(COUNT) DESC)\n",
    "    LIMIT 5\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7c428917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+\n",
      "|   DEST_COUNTRY_NAME| ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+--------------------+-----+\n",
      "|       United States|             Romania|   15|\n",
      "|       United States|             Croatia|    1|\n",
      "|       United States|             Ireland|  344|\n",
      "|               Egypt|       United States|   15|\n",
      "|       United States|               India|   62|\n",
      "|       United States|           Singapore|    1|\n",
      "|       United States|             Grenada|   62|\n",
      "|          Costa Rica|       United States|  588|\n",
      "|             Senegal|       United States|   40|\n",
      "|       United States|        Sint Maarten|  325|\n",
      "|       United States|    Marshall Islands|   39|\n",
      "|              Guyana|       United States|   64|\n",
      "|               Malta|       United States|    1|\n",
      "|            Anguilla|       United States|   41|\n",
      "|             Bolivia|       United States|   30|\n",
      "|       United States|            Paraguay|    6|\n",
      "|Turks and Caicos ...|       United States|  230|\n",
      "|               Italy|       United States|  382|\n",
      "|       United States|Federated States ...|   69|\n",
      "|       United States|              Russia|  161|\n",
      "+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT * FROM flights f1\n",
    "    WHERE EXISTS (\n",
    "        SELECT 1 FROM flights f2\n",
    "        WHERE f1.DEST_COUNTRY_NAME = f2.ORIGIN_COUNTRY_NAME)\n",
    "    AND EXISTS (\n",
    "        SELECT 1 FROM flights f2\n",
    "        WHERE f2.DEST_COUNTRY_NAME = f1.ORIGIN_COUNTRY_NAME)\n",
    "    \"\"\").show()\n",
    "# EXISTS 키워드는 서브쿼리에 값이 존재하면 true를 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2827b8cf",
   "metadata": {},
   "source": [
    "### 10.11 다양한 기능\n",
    "### 10.11.1 설정\n",
    "<img src=\"../../assets/presentations/week10/chapter10_1.jpeg\" width=\"1000px\" height=\"250px\" title=\"catalyst_optimizer\"/>\n",
    "<img src=\"../../assets/presentations/week10/chapter10_2.jpeg\" width=\"1000px\" height=\"250px\" title=\"catalyst_optimizer\"/>\n",
    "\n",
    "\n",
    "### 10.11.2 SQL에서 설정값 지정하기\n",
    "- 스파크 SQL과 관련된 설정만 가능\n",
    "- 셔플 파티션 수 지정 `SET spark.sql.shuffle.partitions=20`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1552c86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b959882d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
