{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "pediatric-charter",
   "metadata": {},
   "source": [
    "# CHAPTER 7. 집계 연산\n",
    "### 집계란\n",
    "- 무언가를 함께 모으는 행위\n",
    "- 집계를 수행하기 위해서는 **키나 그룹을 지정**하고 **하나 이상의 컬럼을 변환하는 방법을 지정하는 집계 함수를 사용**한다. \n",
    "\n",
    "### 그룹화 데이터 타입\n",
    "- 가장 간단한 형태의 그룹화로 select 구문에서 집계를 수행한다.\n",
    "- `group by`는 하나 이상의 키를 지정할 수 있으며 값을 가진 컬럼을 변환하기 위해 다른 집계 함수를 사용한다.\n",
    "- `window`는 하나 이상의 키를 지정할 수 있으며, 값을 가진 컬럼을 변환하기 위해 다른 집계 함수를 사용할 수 있다. 단, 함수의 입력으로 사용할 로우는 현재 로우와 어느 정도 연관성이 있어야 한다.\n",
    "- `grouping set`은 **서로 다른 레벨의 값을 집계할 때** 사용되며, SQL, DataFrame의 롤업 그리고 큐브를 사용할 수 있다.\n",
    "- `roll up`은 하나 이상의 키를 지정할 수 있다. 컬럼을 변환하는 데 다른 집계 함수를 사용하여 **계층적으로 요약된 값을 구할 수 있다**.\n",
    "- `cube`는 하나 이사의 키를 지정할 수 있으며 값을 가진 컬럼을 변환하기 위해 다른 집계 함수를 사용할 수 있다. 큐브는 모든 컬럼 조합에 대한 요약 값을 계산한다.\n",
    "\n",
    "**지정된 집계 함수에 따라 그룹화된 결과는 RelationalGroupedDataset을 반환한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-lending",
   "metadata": {},
   "source": [
    "## 집계 함수\n",
    "- 집계는 DataFrame의 .stat 속성을 이용하는 특별한 경우를 제외하곤 **함수를 사용**한다.\n",
    "- org.apach.spark.sql.functions 패키지에서 참고\n",
    "\n",
    "## (1) DataFrame 수준의 집계: 단일 컬럼 집계 함수\n",
    "\n",
    "#### count\n",
    "- count 함수는 액션이 아닌 트랜스포메이션으로 동작한다.\n",
    "- 두 가지 방식으로 사용할 수 있다.\n",
    "    - 1. count 함수에 특정 컬럼을 지정하는 방식\n",
    "    - 2. count(*)나 count(1)을 사용하는 방식\n",
    "\n",
    "#### countDistinct, approx_count_distinct\n",
    "- countDistinct 함수는 고유 레코드 수를 구해야 할 때 사용된다. 따라서 **개별 컬럼을 처리하는 데 더 적합하다.**\n",
    "- approx_count_distinct 함수는 최대 추정 오류율이라는 한 가지 파라미터를 더 사용한다.\n",
    "- 예제에서는 0.1 이라는 큰 오류 설정으로 기대치에 크게 벗어나지만, countDistinct 함수보다 더 빠르게 결과를 반환한다. **따라서 이 함수의 성능은 대규모 데이터셋을 사용할 때 훨씬 더 좋아진다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "twenty-sunglasses",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DataFrame을 사용해 기본 집계 수행, count 메서드를 사용한 예제\n",
    "df = spark.range(500).toDF(\"number\")\n",
    "df.count() # count 메서드는 트렌스포메이션이 아닌 액션이므로 결과를 즉시 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "charming-youth",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".load(\"../../assets/exercises/week03/by-day/2010-12-01.csv\")\\\n",
    ".coalesce(5)\n",
    "\n",
    "df.cache()\n",
    "df.createOrReplaceTempView('dfTable')\n",
    "# 구매 이력 데이터를 사용해 파티션을 적은 수로 분할할 수 있도록 리파티셔닝하고\n",
    "# 빠르게 접근할 수 있도록 캐싱한다.\n",
    "# 여기서 파티션 수를 줄이는 이유는 적은 양의 데이터를 가진 수많은 파일이 존재하기 때문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "proper-wallet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['InvoiceNo',\n",
       " 'StockCode',\n",
       " 'Description',\n",
       " 'Quantity',\n",
       " 'InvoiceDate',\n",
       " 'UnitPrice',\n",
       " 'CustomerID',\n",
       " 'Country']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "pressing-bulletin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "intended-touch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|count(StockCode)|\n",
      "+----------------+\n",
      "|            3108|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.sql.functions import count, countDistinct, approx_count_distinct\n",
    "\n",
    "df.select(f.count('StockCode')).show() # 3108"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "pressed-cisco",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT StockCode)|\n",
      "+-------------------------+\n",
      "|                     1351|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(f.countDistinct(\"StockCode\")).show() # 1351"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "italian-catalog",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|approx_count_distinct(StockCode)|\n",
      "+--------------------------------+\n",
      "|                            1382|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(f.approx_count_distinct(\"StockCode\", 0.1)).show() # 1382"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnostic-documentary",
   "metadata": {},
   "source": [
    "#### first와 last\n",
    "- DataFrame의 첫 번째 값이나 마지막 값을 얻을 때 사용\n",
    "- 단, DataFrame의 값이 아닌 로우를 기반으로 동작한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "warming-conclusion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+\n",
      "|first(StockCode)|last(StockCode)|\n",
      "+----------------+---------------+\n",
      "|          85123A|          20755|\n",
      "+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.sql.functions import first, last\n",
    "\n",
    "df.select(f.first(\"StockCode\"), f.last(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twelve-metabolism",
   "metadata": {},
   "source": [
    "#### min과 max\n",
    "- DataFrame에서 최솟값과 최댓값을 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "genuine-backup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|min(Quantity)|max(Quantity)|\n",
      "+-------------+-------------+\n",
      "|          -24|          600|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.sql.functions import min, max\n",
    "df.select(f.min(\"Quantity\"), f.max(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspended-durham",
   "metadata": {},
   "source": [
    "#### sum\n",
    "- 특정 컬럼의 모든 값을 합산\n",
    "\n",
    "#### sum_distinct(Deprecated : sumDistinct)\n",
    "- 특정 컬럼의 고윳값을 합산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "external-guest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------------+\n",
      "|sum(Quantity)|sum(DISTINCT Quantity)|\n",
      "+-------------+----------------------+\n",
      "|        26814|                  4690|\n",
      "+-------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.sql.functions import sum, sumDistinct, sum_distinct\n",
    "\n",
    "# df.select(f.sum(\"Quantity\"), f.sumDistinct(\"Quantity\")).show()\n",
    "# FutureWarning: Deprecated in 3.2, use sum_distinct instead.\n",
    "# warnings.warn(\"Deprecated in 3.2, use sum_distinct instead.\", FutureWarning)\n",
    "\n",
    "df.select(f.sum(\"Quantity\"), f.sum_distinct(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "willing-orbit",
   "metadata": {},
   "source": [
    "#### avg\n",
    "- 평균값\n",
    "- sum 함수의 결과를 count 함수의 결과로 나누어 평균값을 구할 수 있지만 스파크의 avg 함수나 mean 함수를 사용하면 평균값을 더 쉽게 구할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "radical-clerk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+-----------------+-----------------+\n",
      "|(total_purchase / total_transaction)|     avg_purchase|    mean_purchase|\n",
      "+------------------------------------+-----------------+-----------------+\n",
      "|                   8.627413127413128|8.627413127413128|8.627413127413128|\n",
      "+------------------------------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 이 예제는 집계된 컬럼을 재활용 하기 위해 alias 메소드를 사용한다.\n",
    "\n",
    "df.select(\n",
    "    f.count(\"Quantity\").alias(\"total_transaction\"),\n",
    "    f.sum(\"Quantity\").alias(\"total_purchase\"),\n",
    "    f.avg(\"Quantity\").alias(\"avg_purchase\"),\n",
    "    f.expr(\"mean(Quantity)\").alias(\"mean_purchase\"),\n",
    ").selectExpr(\n",
    "    \"total_purchase/total_transaction\",\n",
    "    \"avg_purchase\",\n",
    "    \"mean_purchase\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-realtor",
   "metadata": {},
   "source": [
    "#### 분산과 표준편차\n",
    "- 분산은 평균과의 차이를 제곱한 결과의 평균이며 표준편차는 분산의 제곱근.\n",
    "- 스파크에서는 표본표준편차 뿐만 아니라 모표준편차 방식도 지원하기 때문에 주의가 필요하다.\n",
    "- variance와 stddev 함수를 사용하면 표본표준분산과 표본표준편차 공식을 이용\n",
    "- 모표준분산이나 모표준편차 방식을 사용하려면 var_pop, stddev_pop 함수를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "introductory-drain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+--------------------+---------------------+\n",
      "|var_pop(Quantity)|var_samp(Quantity)|stddev_pop(Quantity)|stddev_samp(Quantity)|\n",
      "+-----------------+------------------+--------------------+---------------------+\n",
      "|695.2492099104054| 695.4729785650273|  26.367578764657278|   26.371821677029203|\n",
      "+-----------------+------------------+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(f.var_pop(\"Quantity\"), f.var_samp(\"Quantity\"),\n",
    "         f.stddev_pop(\"Quantity\"), f.stddev_samp(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-burton",
   "metadata": {},
   "source": [
    "#### 비대칭과 첨도\n",
    "- 데이터의 변곡점을 측정하는 방법\n",
    "- 비대칭도: 데이터 평균의 비대칭 정도를 측정\n",
    "- 첨도: 데이터 끝 부분을 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "curious-tactics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|skewness(Quantity)|kurtosis(Quantity)|\n",
      "+------------------+------------------+\n",
      "|11.384721296581182|182.91886804842397|\n",
      "+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(f.skewness(\"Quantity\"), f.kurtosis(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "printable-syntax",
   "metadata": {},
   "source": [
    "## (2) DataFrame 수준의 집계:  두 컬럼값 사이의 영향도 비교\n",
    "\n",
    "#### 공분산과 상관관계\n",
    "- 공분산, cov\n",
    "- 상관관계, corr : 피어슨 상관계수를 측정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "satellite-installation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-------------------------------+------------------------------+\n",
      "|corr(InvoiceNo, Quantity)|covar_samp(InvoiceNo, Quantity)|covar_pop(InvoiceNo, Quantity)|\n",
      "+-------------------------+-------------------------------+------------------------------+\n",
      "|     -0.12225395743668731|            -235.56327681311157|            -235.4868448608685|\n",
      "+-------------------------+-------------------------------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(f.corr(\"InvoiceNo\", \"Quantity\"), f.covar_samp(\"InvoiceNo\", \"Quantity\"),\n",
    "    f.covar_pop(\"InvoiceNo\", \"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organic-spice",
   "metadata": {},
   "source": [
    "## (3) DataFrame 수준의 집계:  복합 데이터 타입의 집계\n",
    "특정 컬럼의 값을 리스트로 수집하거나 set() 데이터 타입으로 고윳값만 수집할 수 있다. \n",
    "수집된 데이터는 처리 파이프라인에서 다양한 프로그래밍 방식으로 다루거나 사용자 정의 함수를 사용해 전체 데이터에 접근할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "broad-israel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+\n",
      "|collect_set(Country)|collect_list(Country)|\n",
      "+--------------------+---------------------+\n",
      "|[France, Australi...| [United Kingdom, ...|\n",
      "+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg(f.collect_set(\"Country\"), f.collect_list(\"Country\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-position",
   "metadata": {},
   "source": [
    "## (4) 데이터 그룹 기반 집계: 그룹화\n",
    "**단일 컬럼의 데이터를 그룹화**하고 **해당 그룹의 다른 여러 컬럼을 사용해서 계산**하기 위해 카테고리형 데이터를 사용한다.\n",
    "\n",
    "`고유한 송장번호(InvoiceNo)를 기준으로 그룹을 만들고 그룹별 물품 수를 카운트하기 위해서는` 집계 연산을 수행하는 두 단계로 이루어짐\n",
    "- 1. 하나 이상의 컬럼을 그룹화한다.\n",
    "    - RelationalGroupedDataset이 반환된다.\n",
    "- 2. 그룹화 후에 집계 연산을 수행한다.\n",
    "    - DataFrame이 반환된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "injured-coffee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+\n",
      "|InvoiceNo|CustomerId|count|\n",
      "+---------+----------+-----+\n",
      "|   536596|      null|    6|\n",
      "|   536530|   17905.0|   23|\n",
      "|   536414|      null|    1|\n",
      "|   536400|   13448.0|    1|\n",
      "|   536550|      null|    1|\n",
      "|   536522|   15012.0|   54|\n",
      "|   536401|   15862.0|   64|\n",
      "|   536409|   17908.0|   58|\n",
      "|   536404|   16218.0|   28|\n",
      "|   536562|   13468.0|   18|\n",
      "|   536396|   17850.0|   18|\n",
      "|   536539|   15165.0|   27|\n",
      "|   536406|   17850.0|   17|\n",
      "|   536523|   12868.0|   12|\n",
      "|   536551|   17346.0|   35|\n",
      "|   536567|   16048.0|    7|\n",
      "|   536592|      null|  592|\n",
      "|   536560|   13093.0|   13|\n",
      "|   536593|   16835.0|    5|\n",
      "|   536488|   17897.0|   35|\n",
      "+---------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby(\"InvoiceNo\", \"CustomerId\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-bonus",
   "metadata": {},
   "source": [
    "#### 표현식을 이용한 그룹화\n",
    "- 카운팅은 메서드로 사용할 수 있으므로 특별하지만, **메서드 대신 count 함수를 사용할 것을 권장**\n",
    "- count 함수를 select 구문에 표현식으로 지정하는 것보다 agg 메서드를 사용하는 것이 좋다.\n",
    "- agg 메서드는 여러 집계 처리를 한 번에 지정할 수 있으며, 집계에 표현식을 사용할 수 있다.\n",
    "- 트랜스포메이션이 완료된 컬럼에 alias 메서드를 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "handed-phase",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+---------------+\n",
      "|InvoiceNo|quan|count(Quantity)|\n",
      "+---------+----+---------------+\n",
      "|   536596|   6|              6|\n",
      "|   536597|  28|             28|\n",
      "|   536414|   1|              1|\n",
      "|   536550|   1|              1|\n",
      "|   536460|  14|             14|\n",
      "|   536398|  17|             17|\n",
      "|   536523|  12|             12|\n",
      "|   536374|   1|              1|\n",
      "|   536386|   3|              3|\n",
      "|   536577|   4|              4|\n",
      "|   536477|  14|             14|\n",
      "|   536583|   1|              1|\n",
      "|   536528|  57|             57|\n",
      "|   536585|   1|              1|\n",
      "|   536366|   2|              2|\n",
      "|   536592| 592|            592|\n",
      "|   536541|   1|              1|\n",
      "|   536387|   5|              5|\n",
      "|   536385|   7|              7|\n",
      "|   536375|  16|             16|\n",
      "+---------+----+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\").agg(\n",
    "    f.count(\"Quantity\").alias(\"quan\"),\n",
    "    f.expr(\"count(Quantity)\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-barbados",
   "metadata": {},
   "source": [
    "#### 맵을 이용한 그룹화\n",
    "- 컬럼을 키로, 수행할 집계 함수의 문자열을 값으로 하는 맵 타입을 사용해 트랜스포메이션을 정의할 수 있다. 수행할 집계 함수를 한 줄로 작성하면 여러 컬럼명을 재사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "selected-frame",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+--------------------+\n",
      "|InvoiceNo|     avg(Quantity)|stddev_pop(Quantity)|\n",
      "+---------+------------------+--------------------+\n",
      "|   536596|               1.5|  1.1180339887498947|\n",
      "|   536597|2.5357142857142856|  2.7448932175059566|\n",
      "|   536414|              56.0|                 0.0|\n",
      "|   536550|               1.0|                 0.0|\n",
      "|   536460|11.285714285714286|    8.80282885885937|\n",
      "|   536398| 8.823529411764707|  4.9850989029972474|\n",
      "|   536523| 9.333333333333334|   7.487025815072067|\n",
      "|   536374|              32.0|                 0.0|\n",
      "|   536386| 78.66666666666667|  30.169889330626027|\n",
      "|   536577|              97.0|   47.52893855326458|\n",
      "|   536477| 76.42857142857143|  121.20390981772016|\n",
      "|   536583|             120.0|                 0.0|\n",
      "|   536528| 2.175438596491228|   2.240949102012037|\n",
      "|   536585|               2.0|                 0.0|\n",
      "|   536366|               6.0|                 0.0|\n",
      "|   536592|2.4966216216216215|    3.78430106261453|\n",
      "|   536541|              12.0|                 0.0|\n",
      "|   536387|             288.0|  117.57550765359255|\n",
      "|   536385| 7.571428571428571|   4.271404682207444|\n",
      "|   536375|               5.5|  1.5000000000000002|\n",
      "+---------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\").agg(f.expr(\"avg(Quantity)\"), f.expr(\"stddev_pop(Quantity)\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indonesian-gallery",
   "metadata": {},
   "source": [
    "## (5) 윈도우 함수:\n",
    "- 윈도우 함수는 데이터의 특정 '윈도우'를 대상을 고유한 집계 연산을 수행한다. \n",
    "- 데이터 윈도우는 현재 데이터에 대한 참조를 사용해 정의한다.\n",
    "- 윈도우 명세는 함수에 전달될 로우를 결정한다.\n",
    "\n",
    "#### 표준 group-by 함수와 차이점\n",
    "- group-by 함수는 모든 로우 레코드가 단일 그룹으로만 이동한다.\n",
    "- 윈도우 함수는 프레임에 입력되는 모든 로우에 대한 결괏값을 계산한다.\n",
    "    - 프레임: 로우 그룹 기반의 테이블을 의미\n",
    "        - 각 로우는 하나 이상의 프레임에 할당될 수 있다.\n",
    "        - 롤링 평균을 구하기 위해서는 개별 로우가 7개의 다른 프레임으로 구성되어야 한다.\n",
    "        \n",
    "![윈도우 함수 시각화](../../assets/presentations/week07/spark7-visualizing_window_functions.png)\n",
    "      \n",
    "** 로우가 어떻게 여러 프레임에 할당될 수 있는지 나타낸다.\n",
    "\n",
    "\n",
    "#### 랭크 함수(ranking function)\n",
    "- dense_rank\n",
    "    - 모든 고객에 대해 최대 구매 수량을 가진 날짜가 언제인지 확인한다.\n",
    "    - 동일한 값이 나오거나 중복 로우가 발생해 순위가 비어 있을 수 있으므로 danse_rank 함수를 사용한다.\n",
    "\n",
    "#### 분석 함수(analytic function)\n",
    "#### 집계 함수(aggregate function)\n",
    "\n",
    "#### 윈도우 함수 사용하기\n",
    "1. 윈도우 함수를 정의하기 위해 **윈도우 명세**를 만든다.\n",
    "- partitionBy 메서드는 지금까지 사용해온 파티셔닝 스키마의 개념과는 관련 없으며, 그룹을 어떻게 나눌지에 유사한 개념이다. OrderBy 메서드는 파티션의 정렬 방식을 정의한다.\n",
    "- 프레임 명세 (rowsBetween 구문)는 입력된 로우의 참조를 기반으로 프레임에 로우가 포함될 수 있는지 결정한다.\n",
    "- 예제에서는 첫 로우부터 현재 로우까지 확인한다.\n",
    "\n",
    "2. 윈도우 명세를 이용해 확인하고 싶은 데이터를 위한 집계 함수 혹은 윈도우 함수를 준비한다.\n",
    "\n",
    "3. select 구문에서 사용할 수 있는 컬럼을 반환한다. select 메서드를 사용해 계산된 윈도우값을 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "individual-connection",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWithDate = df.withColumn(\"date\", f.to_date(f.col(\"InvoiceDate\"), \"MM/d/yyyy H:mm\"))\n",
    "dfWithDate.cache()\n",
    "dfWithDate.createOrReplaceTempView(\"dfWithDate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "neither-commander",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['InvoiceNo',\n",
       " 'StockCode',\n",
       " 'Description',\n",
       " 'Quantity',\n",
       " 'InvoiceDate',\n",
       " 'UnitPrice',\n",
       " 'CustomerID',\n",
       " 'Country',\n",
       " 'date']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dfWithDate.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "tested-sampling",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "american-telescope",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpec = Window\\\n",
    ".partitionBy(\"CustomerId\", \"date\")\\\n",
    ".orderBy(f.desc(\"Quantity\"))\\\n",
    ".rowsBetween(Window.unboundedPreceding, Window.currentRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dominant-atlas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간대별 최대 구매를 구한다.\n",
    "maxPurchaseQuantity = f.max(f.col(\"Quantity\")).over(windowSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "outer-wonder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구매량 순위\n",
    "# 모든 고객에 대해 최대 구매 수량을 가진 날짜가 언제인지를 확인한다.\n",
    "purchaseDenseRank = f.dense_rank().over(windowSpec)\n",
    "purchaseRank = f.rank().over(windowSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "beautiful-cassette",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+------------+-----------------+-------------------+\n",
      "|CustomerId|      date|Quantity|purchaseRank|purchaseDenseRank|maxPurchaseQuantity|\n",
      "+----------+----------+--------+------------+-----------------+-------------------+\n",
      "|   12431.0|2010-12-01|      24|           1|                1|                 24|\n",
      "|   12431.0|2010-12-01|      24|           1|                1|                 24|\n",
      "|   12431.0|2010-12-01|      12|           3|                2|                 24|\n",
      "|   12431.0|2010-12-01|       8|           4|                3|                 24|\n",
      "|   12431.0|2010-12-01|       6|           5|                4|                 24|\n",
      "|   12431.0|2010-12-01|       6|           5|                4|                 24|\n",
      "|   12431.0|2010-12-01|       6|           5|                4|                 24|\n",
      "|   12431.0|2010-12-01|       4|           8|                5|                 24|\n",
      "|   12431.0|2010-12-01|       4|           8|                5|                 24|\n",
      "|   12431.0|2010-12-01|       4|           8|                5|                 24|\n",
      "|   12431.0|2010-12-01|       3|          11|                6|                 24|\n",
      "|   12431.0|2010-12-01|       2|          12|                7|                 24|\n",
      "|   12431.0|2010-12-01|       2|          12|                7|                 24|\n",
      "|   12431.0|2010-12-01|       2|          12|                7|                 24|\n",
      "|   12433.0|2010-12-01|      96|           1|                1|                 96|\n",
      "|   12433.0|2010-12-01|      72|           2|                2|                 96|\n",
      "|   12433.0|2010-12-01|      72|           2|                2|                 96|\n",
      "|   12433.0|2010-12-01|      50|           4|                3|                 96|\n",
      "|   12433.0|2010-12-01|      48|           5|                4|                 96|\n",
      "|   12433.0|2010-12-01|      48|           5|                4|                 96|\n",
      "+----------+----------+--------+------------+-----------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithDate.where(\"CustomerId IS NOT NULL\").orderBy(\"CustomerId\")\\\n",
    ".select(\n",
    "f.col(\"CustomerId\"),\n",
    "f.col(\"date\"),\n",
    "f.col(\"Quantity\"),\n",
    "purchaseRank.alias(\"purchaseRank\"),\n",
    "purchaseDenseRank.alias(\"purchaseDenseRank\"),\n",
    "maxPurchaseQuantity.alias(\"maxPurchaseQuantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "sapphire-colonial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+----+-----+-----------+\n",
      "|CustomerId|      date|Quantity|rank|dRank|maxPurchase|\n",
      "+----------+----------+--------+----+-----+-----------+\n",
      "|   12431.0|2010-12-01|      24|   1|    1|         24|\n",
      "|   12431.0|2010-12-01|       6|   5|    4|         24|\n",
      "|   12431.0|2010-12-01|      12|   3|    2|         24|\n",
      "|   12431.0|2010-12-01|      24|   1|    1|         24|\n",
      "|   12431.0|2010-12-01|       3|  11|    6|         24|\n",
      "|   12431.0|2010-12-01|       2|  12|    7|         24|\n",
      "|   12431.0|2010-12-01|       2|  12|    7|         24|\n",
      "|   12431.0|2010-12-01|       2|  12|    7|         24|\n",
      "|   12431.0|2010-12-01|       6|   5|    4|         24|\n",
      "|   12431.0|2010-12-01|       4|   8|    5|         24|\n",
      "|   12431.0|2010-12-01|       8|   4|    3|         24|\n",
      "|   12431.0|2010-12-01|       4|   8|    5|         24|\n",
      "|   12431.0|2010-12-01|       4|   8|    5|         24|\n",
      "|   12431.0|2010-12-01|       6|   5|    4|         24|\n",
      "|   12433.0|2010-12-01|      96|   1|    1|         96|\n",
      "|   12433.0|2010-12-01|      72|   2|    2|         96|\n",
      "|   12433.0|2010-12-01|      72|   2|    2|         96|\n",
      "|   12433.0|2010-12-01|      48|   5|    4|         96|\n",
      "|   12433.0|2010-12-01|      50|   4|    3|         96|\n",
      "|   12433.0|2010-12-01|      48|   5|    4|         96|\n",
      "+----------+----------+--------+----+-----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT CustomerId, date, Quantity,\n",
    "    rank(Quantity) OVER (PARTITION BY CustomerId, date\n",
    "                        ORDER BY Quantity DESC NULLS LAST\n",
    "                        ROWS BETWEEN\n",
    "                            UNBOUNDED PRECEDING AND\n",
    "                            CURRENT ROW) as rank,\n",
    "    dense_rank(Quantity) OVER (PARTITION BY CustomerId, date\n",
    "                              ORDER BY Quantity DESC NULLS LAST\n",
    "                              ROWS BETWEEN\n",
    "                                  UNBOUNDED PRECEDING AND\n",
    "                                  CURRENT ROW) as dRank,\n",
    "    max(Quantity) OVER (PARTITION BY CustomerId, date\n",
    "                       ORDER BY Quantity DESC NULLS LAST\n",
    "                       ROWS BETWEEN\n",
    "                           UNBOUNDED PRECEDING AND\n",
    "                                  CURRENT ROW) as maxPurchase\n",
    "FROM dfWithDate WHERE CustomerId IS NOT NULL ORDER BY CustomerId;\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romance-planner",
   "metadata": {},
   "source": [
    "## (6) 여러 그룹에 걸친 집계: 그룹화 셋(명시적인 그룹화)\n",
    "- 여러 집계를 결합하는 저수준 기능이다.\n",
    "- 그룹화 셋을 이용하면 group-by 구문에서 원하는 형태로 집계를 생성할 수 있다.\n",
    "- 그룹화 셋은 null 값에 따라 집계 수준이 달라진다. null 값을 제거하지 않는다면 부정확한 결괏값을 얻게 된다.\n",
    "- SQL 에서만 사용할 수 있다. DataFrame에서 동일한 연산을 수행하려면 rollup 메서드와 cube 메서드를 사용해야 한다.\n",
    "\n",
    "#### group-by VS grouping sets\n",
    "- stockCode와 CustomerId 별 총 수량은 group-by, grouping sets 모두 동일 결과를 얻을 수 있다.\n",
    "- stockCode와 CustomerId에 상관없이 총 수량의 합산 결과를 추가하려면 group-by 구문을 사용해 처리하는 것은 불가능하다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "described-visibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNoNull = dfWithDate.na.drop()\n",
    "dfNoNull.createOrReplaceTempView(\"dfNoNull\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "alien-hardwood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------+\n",
      "|CustomerId|stockCode|sum(Quantity)|\n",
      "+----------+---------+-------------+\n",
      "|   18229.0|    22848|            8|\n",
      "|   18229.0|    22846|            8|\n",
      "|   18229.0|    22730|            4|\n",
      "|   18229.0|    22729|            4|\n",
      "|   18229.0|    22728|            8|\n",
      "|   18229.0|    22726|            8|\n",
      "|   18229.0|    22725|            4|\n",
      "|   18144.0|    84879|           80|\n",
      "|   18144.0|    72817|           12|\n",
      "|   18144.0|    21485|            3|\n",
      "|   18085.0|   84029E|            8|\n",
      "|   18085.0|    22837|            8|\n",
      "|   18085.0|    22632|           12|\n",
      "|   18085.0|    22619|            4|\n",
      "|   18085.0|    22113|            8|\n",
      "|   18085.0|    22111|           16|\n",
      "|   18085.0|    21485|            8|\n",
      "|   18085.0|    21481|            6|\n",
      "|   18085.0|    21479|            8|\n",
      "|   18074.0|    84755|           48|\n",
      "+----------+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# group by 이용\n",
    "spark.sql(\"\"\"\n",
    "SELECT CustomerId, stockCode, sum(Quantity)\n",
    "FROM dfNoNull\n",
    "GROUP BY customerId, stockCode\n",
    "ORDER BY CustomerId DESC, stockCode DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "governmental-japanese",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------+\n",
      "|customerId|stockCode|sum(Quantity)|\n",
      "+----------+---------+-------------+\n",
      "|   18229.0|    22848|            8|\n",
      "|   18229.0|    22846|            8|\n",
      "|   18229.0|    22730|            4|\n",
      "|   18229.0|    22729|            4|\n",
      "|   18229.0|    22728|            8|\n",
      "|   18229.0|    22726|            8|\n",
      "|   18229.0|    22725|            4|\n",
      "|   18144.0|    84879|           80|\n",
      "|   18144.0|    72817|           12|\n",
      "|   18144.0|    21485|            3|\n",
      "|   18085.0|   84029E|            8|\n",
      "|   18085.0|    22837|            8|\n",
      "|   18085.0|    22632|           12|\n",
      "|   18085.0|    22619|            4|\n",
      "|   18085.0|    22113|            8|\n",
      "|   18085.0|    22111|           16|\n",
      "|   18085.0|    21485|            8|\n",
      "|   18085.0|    21481|            6|\n",
      "|   18085.0|    21479|            8|\n",
      "|   18074.0|    84755|           48|\n",
      "+----------+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 그룹화 셋을 사용해 동일한 작업 수행\n",
    "spark.sql(\"\"\"\n",
    "SELECT CustomerId, stockCode, sum(Quantity)\n",
    "FROM dfNoNull\n",
    "GROUP BY customerId, stockCode\n",
    "GROUPING SETS ((customerId, stockCode))\n",
    "ORDER BY CustomerId DESC, stockCode DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dominican-luxury",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------+\n",
      "|customerId|stockCode|sum(Quantity)|\n",
      "+----------+---------+-------------+\n",
      "|      null|     null|        24032|\n",
      "|   13694.0|    17021|          600|\n",
      "|   13777.0|    21232|          504|\n",
      "|   16210.0|    21137|          480|\n",
      "|   13777.0|   84029E|          480|\n",
      "|   16029.0|    22466|          432|\n",
      "|   16029.0|    21731|          432|\n",
      "|   13777.0|    22095|          324|\n",
      "|   13777.0|   85099B|          300|\n",
      "|   17511.0|    20668|          288|\n",
      "|   13777.0|   85123A|          256|\n",
      "|   13694.0|    21154|          200|\n",
      "|   16029.0|    79321|          192|\n",
      "|   16029.0|    22780|          192|\n",
      "|   16029.0|    22779|          192|\n",
      "|   13777.0|    84050|          168|\n",
      "|   17511.0|   84970S|          144|\n",
      "|   17511.0|    21786|          144|\n",
      "|   17181.0|    21326|          144|\n",
      "|   16210.0|    22595|          144|\n",
      "+----------+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 고객이나 재고 코드에 상관없이 총 수량의 합산 결과를 추가하려하면\n",
    "# group-by 구문을 사용해 처리하는 것은 불가능\n",
    "spark.sql(\"\"\"\n",
    "SELECT CustomerId, stockCode, sum(Quantity)\n",
    "FROM dfNoNull\n",
    "GROUP BY customerId, stockCode\n",
    "GROUPING SETS ((customerId, stockCode), ())\n",
    "ORDER BY sum(Quantity) DESC, CustomerId DESC, stockCode DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "following-netherlands",
   "metadata": {},
   "source": [
    "## (7) 여러 그룹에 걸친 집계: 롤업 (다차원 집계 기능)\n",
    "- 다양한 컬럼을 그룹화 키로 설정하면 그룹화 키로 설정된 조합뿐만 아니라 데이터셋에서 볼 수 있는 실제 조합을 모두 살펴볼 수 있다.\n",
    "- 롤업은 group-by 스타일의 다양한 연산을 수행할 수 있는 다차원 집계 기능이다.\n",
    "\n",
    "- null 값은 가진 로우에서 전체 날짜의 합계를 확인할 수 있다.\n",
    "- 롤업된 두 개의 컬럼값이 모두 null인 로우는 두 컬럼에 속한 레코드의 전체 함께를 나타낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "athletic-temple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+\n",
      "|      Date|       Country|totla_quantity|\n",
      "+----------+--------------+--------------+\n",
      "|      null|          null|         24032|\n",
      "|2010-12-01|     Australia|           107|\n",
      "|2010-12-01|        France|           449|\n",
      "|2010-12-01|United Kingdom|         21167|\n",
      "|2010-12-01|          EIRE|           243|\n",
      "|2010-12-01|          null|         24032|\n",
      "|2010-12-01|        Norway|          1852|\n",
      "|2010-12-01|       Germany|           117|\n",
      "|2010-12-01|   Netherlands|            97|\n",
      "+----------+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rolledUpDF = dfNoNull.rollup(\"Date\", \"Country\").agg(f.sum(\"Quantity\"))\\\n",
    ".selectExpr(\"Date\", \"Country\", \"`sum(Quantity)` as totla_quantity\")\\\n",
    ".orderBy(\"Date\")\n",
    "rolledUpDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "vanilla-binary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------+\n",
      "|      Date|Country|totla_quantity|\n",
      "+----------+-------+--------------+\n",
      "|      null|   null|         24032|\n",
      "|2010-12-01|   null|         24032|\n",
      "+----------+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rolledUpDF.where(\"Country IS NULL\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interested-masters",
   "metadata": {},
   "source": [
    "## (8)  여러 그룹에 걸친 집계: 큐브\n",
    "- 롤업을 고차원적으로 사용할 수 있게 해준다.\n",
    "- 큐브는 요소들을 계층적으로 다루는 대신 모든 차원에 대해서 동일한 작업을 수행한다.\n",
    "- 전체 기간에 대해 날짜의 국가별 결과를 얻을 수 있다.\n",
    "    - 다음과 같은 정보를 가진 테이블을 만들 수 있음\n",
    "        - 전체 날짜와 모든 국가에 대한 합계\n",
    "        - 모든 국가의 날짜별 합계\n",
    "        - 날짜별 국가별 합계\n",
    "        - 전체 날짜의 국가별 합계\n",
    "- 큐브를 사용해 테이블에 있는 모든 정보를 빠르고 쉽게 조회할 수 있는 요약 정보 테이블을 만들 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "disabled-edinburgh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+\n",
      "|      Date|       Country|totla_quantity|\n",
      "+----------+--------------+--------------+\n",
      "|      null|        France|           449|\n",
      "|      null|          null|         24032|\n",
      "|      null|     Australia|           107|\n",
      "|      null|       Germany|           117|\n",
      "|      null|        Norway|          1852|\n",
      "|      null|United Kingdom|         21167|\n",
      "|      null|          EIRE|           243|\n",
      "|      null|   Netherlands|            97|\n",
      "|2010-12-01|     Australia|           107|\n",
      "|2010-12-01|United Kingdom|         21167|\n",
      "|2010-12-01|          null|         24032|\n",
      "|2010-12-01|   Netherlands|            97|\n",
      "|2010-12-01|        France|           449|\n",
      "|2010-12-01|        Norway|          1852|\n",
      "|2010-12-01|       Germany|           117|\n",
      "|2010-12-01|          EIRE|           243|\n",
      "+----------+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfNoNull.cube(\"Date\", \"Country\").agg(f.sum(\"Quantity\"))\\\n",
    ".selectExpr(\"Date\", \"Country\", \"`sum(Quantity)` as totla_quantity\")\\\n",
    ".orderBy(\"Date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "funded-seller",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grouping_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-025091670900>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdfNoNull\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcube\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"customerId\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stockCode\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrouping_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Quantity\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grouping_id()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'grouping_id' is not defined"
     ]
    }
   ],
   "source": [
    "dfNoNull.cube(\"customerId\", \"stockCode\").agg(grouping_id(), sum(\"Quantity\"))\\\n",
    ".orderBy(f.col(\"grouping_id()\").desc)\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-going",
   "metadata": {},
   "source": [
    "## 피벗\n",
    "- 피벗을 사용해 로우를 컬럼으로 변환한다.\n",
    "- 피벗을 사용해 국가별로 집계 함수를 적용할 수 있느며 쿼리를 사용해 쉽게 결과를 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "intended-romantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted = dfWithDate.groupBy(\"date\").pivot(\"Country\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "heard-coalition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date',\n",
       " 'Australia_sum(Quantity)',\n",
       " 'Australia_sum(UnitPrice)',\n",
       " 'Australia_sum(CustomerID)',\n",
       " 'EIRE_sum(Quantity)',\n",
       " 'EIRE_sum(UnitPrice)',\n",
       " 'EIRE_sum(CustomerID)',\n",
       " 'France_sum(Quantity)',\n",
       " 'France_sum(UnitPrice)',\n",
       " 'France_sum(CustomerID)',\n",
       " 'Germany_sum(Quantity)',\n",
       " 'Germany_sum(UnitPrice)',\n",
       " 'Germany_sum(CustomerID)',\n",
       " 'Netherlands_sum(Quantity)',\n",
       " 'Netherlands_sum(UnitPrice)',\n",
       " 'Netherlands_sum(CustomerID)',\n",
       " 'Norway_sum(Quantity)',\n",
       " 'Norway_sum(UnitPrice)',\n",
       " 'Norway_sum(CustomerID)',\n",
       " 'United Kingdom_sum(Quantity)',\n",
       " 'United Kingdom_sum(UnitPrice)',\n",
       " 'United Kingdom_sum(CustomerID)']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pivoted.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
