{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97a56c9b",
   "metadata": {},
   "source": [
    "### Chapter4.구조적 API 개요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d13ef4",
   "metadata": {},
   "source": [
    "##### 구조적 API\n",
    "데이터 흐름을 정의하는 기본 추상화 개념.\n",
    "- Dataset(타입형/4.3.1부분 언급)\n",
    "- DataFrame(비타입형/4.3.1부분 언급+다소 부정확할 수도 있음.)\n",
    "- SQL 테이블과 뷰\n",
    "\n",
    "*배치(batch) & 스트리밍(streaming) 처리에서 구조적 API 사용 가능 -> 관련 내용은 5부에서 자세히 다룰 예정\n",
    "\n",
    "<해당 챕터 중점 내용>\n",
    "- 타입형(typed) / 비타입형(untyped) API의 개념과 차이점\n",
    "- 핵심 용어\n",
    "- 스파크가 구조적 API의 데이터 흐름을 해석하고 클러스터에서 실행하는 방식\n",
    "\n",
    "#### 4.1 DataFrame과 Dataset\n",
    "- 분산테이블 형태의 컬렉션\n",
    "- '스키마' 분산 컬렉션에 저장할 데이터 타입을 정의하는 방법\n",
    "\n",
    "#### 4.2 스키마\n",
    "- DataFrame의 컬럼명과 데이터 타입을 정의\n",
    "- 데이터 소스에서 얻거나(schema-on-read) 직접 정의\n",
    "- 어떤 데이터 타입이 어느 위치에 있는지 정의하는 방법 필요\n",
    "\n",
    "#### 4.3 스파크의 구조적 데이터 타입 개요\n",
    "- 프로그래밍 언어\n",
    "- 카탈리스트 엔진 사용(실행 계획 수립과 처리에 사용하는 자체 데이터 타입 정보를 가짐)\n",
    "- 파이썬이나 R을 이용해 스파크의 구조적 API를 사용하더라도 대부분의 연산은 스파크의 데이터 타입을 사용 \n",
    "=> 스파크가 지원하는 언어를 이용해 작성된 표현식을 카탈리스트 엔진에서 스파크의 테이터 타입으로 변환하여 명령을 처리하기 때문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b226463e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.range(500).toDF(\"number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68392cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[(number + 10): bigint]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(df[\"number\"] + 10) #스파크의 덧셈 연산을 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e685e84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row((number + 10)=10), Row((number + 10)=11), Row((number + 10)=12)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.select(df[\"number\"] + 10).take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7083fb9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(number=0), Row(number=1), Row(number=2)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25be78b1",
   "metadata": {},
   "source": [
    "##### 4.3.1 DataFrame과 Dataset 비교\n",
    "- DataFrame : 비타입형\n",
    "    - 데이터 타입의 일치 여부를 '런타임'이 되어서야 확인\n",
    "    - 스파크의 최적화된 내부 포맷을 사용할 수 있다. \n",
    "    - Row 타입으로 구성된 Dataset\n",
    "    - Row 타입\n",
    "        - '연산에 최적화된 인메모리 포맷'의 내부적인 표현 방식\n",
    "        - 자체 데이터 포맷을 사용하기 때문에 효율적인 연산이 가능.\n",
    "\n",
    "- Dataset : 타입형\n",
    "    - 데이터 타입의 일치 여부를 '컴파일 타임'에 확인\n",
    "    - JVM 기반의 언어인 스칼라와 자바에서만 지원\n",
    "    - Dataset의 데이터 타입 정의를 위해서는 스칼라의 케이스 클래스나 자바 빈을 사용해야 한다.\n",
    "\n",
    "##### 4.3.2 컬럼\n",
    "- 단순 데이터 타입 / 복합 데이터 타입 / null 값\n",
    "- 5장에서 자세히\n",
    "\n",
    "##### 4.3.3 로우\n",
    "- 데이터 레코드\n",
    "- 데이터 소스에서 얻거나 직접 생성 가능(ex.range 메소드 등?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "428d5ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=0), Row(id=1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.range(2).collect() #range 메서드로 DataFrame 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b688bfe3",
   "metadata": {},
   "source": [
    "##### 4.3.3 스파크 데이터 타입\n",
    "스파크 지원 언어별 매핑 정보 참고: https://spark.apache.org/docs/latest/sql-ref-datatypes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6188365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a6a1313",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = ByteType()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb25c1cc",
   "metadata": {},
   "source": [
    "#### 4.4 구조적 API의 실행 과정\n",
    "1. DataFrame/Dataset/SQL을 이용해 코드를 작성\n",
    "2. 정상적인 코드라면 스파크가 논리적 실행 계획으로 변환\n",
    "3. 스파크는 논리적 실행 계획을 물리적 실행 계획으로 변환하며, 그 과정에서 추가적 최적화 가능 여부 확인\n",
    "4. 클러스터에서 물리적 실행 계획(RDD 처리)를 실행\n",
    "<img src=\"../../assets/presentations/week04/catalyst_optimizer.png\" width=\"500px\" height=\"250px\" title=\"catalyst_optimizer\"/>\n",
    "\n",
    "##### 4.4.1 논리적 실행 계획\n",
    "<img src=\"../../assets/presentations/week04/the_structured_API_logical_planning_process.png\" width=\"600px\" height=\"300px\" title=\"the_structured_API_logical_planning_process\"/>\n",
    "\n",
    "- 추상적 트랜스포메이션만 표현\n",
    "- 드라이버나 익스큐터의 정보를 고려하지 않음\n",
    "- 검증 전 논리적 실행 계획으로 변환\n",
    "\n",
    "##### 4.4.2 물리적 실행 계획\n",
    "<img src=\"../../assets/presentations/week04/the_physical_planning_process.png\" width=\"700px\" height=\"350px\" title=\"the_physical_planning_process\"/>\n",
    "\n",
    "- 클러스터 환경에서 실행하는 방법을 정의\n",
    "- 사용하려는 테이블의 크기, 파티션 수 등 물리적 속성을 고려해 지정된 조인 연산 수행에 필요한 비용을 계산하고 비교\n",
    "##### 4.4.3 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1181c532",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da281172",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3a3528",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
