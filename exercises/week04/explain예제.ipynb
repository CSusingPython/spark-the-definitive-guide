{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27465c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "staticDataFrame = spark.read.format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".load(\"spark-the-definitive-guide-study/assets/exercises/week03/by-day/2010-12-01.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "889dc134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Relation [InvoiceNo#198,StockCode#199,Description#200,Quantity#201,InvoiceDate#202,UnitPrice#203,CustomerID#204,Country#205] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string\n",
      "Relation [InvoiceNo#198,StockCode#199,Description#200,Quantity#201,InvoiceDate#202,UnitPrice#203,CustomerID#204,Country#205] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Relation [InvoiceNo#198,StockCode#199,Description#200,Quantity#201,InvoiceDate#202,UnitPrice#203,CustomerID#204,Country#205] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "FileScan csv [InvoiceNo#198,StockCode#199,Description#200,Quantity#201,InvoiceDate#202,UnitPrice#203,CustomerID#204,Country#205] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/hjb/Developer/spark-the-definitive-guide-study/assets/exer..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<InvoiceNo:string,StockCode:string,Description:string,Quantity:int,InvoiceDate:timestamp,Un...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "staticDataFrame.explain(mode='extended')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "083ff1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "staticSchema = staticDataFrame.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18b422ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "Scan csv  (1)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [8]: [InvoiceNo#198, StockCode#199, Description#200, Quantity#201, InvoiceDate#202, UnitPrice#203, CustomerID#204, Country#205]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/Users/hjb/Developer/spark-the-definitive-guide-study/assets/exercises/week03/by-day/2010-12-01.csv]\n",
      "ReadSchema: struct<InvoiceNo:string,StockCode:string,Description:string,Quantity:int,InvoiceDate:timestamp,UnitPrice:double,CustomerID:double,Country:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "staticDataFrame.explain(mode='formatted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e3a4031",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingDataFrame = spark.readStream.format(\"csv\")\\\n",
    ".schema(staticSchema)\\\n",
    ".option(\"maxFilesPerTrigger\", 1)\\\n",
    ".format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".load(\"spark-the-definitive-guide-study/assets/exercises/week03/by-day/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08035121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f170737d",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchaseByCustomerPerHour = streamingDataFrame\\\n",
    ".selectExpr(\n",
    "    \"CustomerID\",\n",
    "    \"(UnitPrice * Quantity) as total_cost\",\n",
    "    \"to_date(to_timestamp(InvoiceDate, 'M/d/yyyy H:mm')) as InvoiceDate\")\\\n",
    ".groupBy(\n",
    "    F.col(\"CustomerID\"), F.window(F.col(\"InvoiceDate\"), \"1 day\"))\\\n",
    ".sum(\"total_cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff991150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(4) HashAggregate(keys=[CustomerID#226, window#311], functions=[sum(total_cost#296)])\n",
      "+- StateStoreSave [CustomerID#226, window#311], state info [ checkpoint = <unknown>, runId = bb1dec25-dd9b-4667-b0a4-1f01fbc51c4c, opId = 0, ver = 0, numPartitions = 200], Append, 0, 2\n",
      "   +- *(3) HashAggregate(keys=[CustomerID#226, window#311], functions=[merge_sum(total_cost#296)])\n",
      "      +- StateStoreRestore [CustomerID#226, window#311], state info [ checkpoint = <unknown>, runId = bb1dec25-dd9b-4667-b0a4-1f01fbc51c4c, opId = 0, ver = 0, numPartitions = 200], 2\n",
      "         +- *(2) HashAggregate(keys=[CustomerID#226, window#311], functions=[merge_sum(total_cost#296)])\n",
      "            +- Exchange hashpartitioning(CustomerID#226, window#311, 200), ENSURE_REQUIREMENTS, [plan_id=510]\n",
      "               +- *(1) HashAggregate(keys=[knownfloatingpointnormalized(normalizenanandzero(CustomerID#226)) AS CustomerID#226, window#311], functions=[partial_sum(total_cost#296)])\n",
      "                  +- *(1) Project [named_struct(start, precisetimestampconversion(((precisetimestampconversion(cast(InvoiceDate#297 as timestamp), TimestampType, LongType) - (((precisetimestampconversion(cast(InvoiceDate#297 as timestamp), TimestampType, LongType) - 0) + 86400000000) % 86400000000)) - 0), LongType, TimestampType), end, precisetimestampconversion((((precisetimestampconversion(cast(InvoiceDate#297 as timestamp), TimestampType, LongType) - (((precisetimestampconversion(cast(InvoiceDate#297 as timestamp), TimestampType, LongType) - 0) + 86400000000) % 86400000000)) - 0) + 86400000000), LongType, TimestampType)) AS window#311, CustomerID#226, total_cost#296]\n",
      "                     +- *(1) Project [CustomerID#226, (UnitPrice#225 * cast(Quantity#223 as double)) AS total_cost#296, cast(gettimestamp(InvoiceDate#224, M/d/yyyy H:mm, TimestampType, Some(Asia/Tokyo), false) as date) AS InvoiceDate#297]\n",
      "                        +- *(1) Filter isnotnull(cast(cast(gettimestamp(InvoiceDate#224, M/d/yyyy H:mm, TimestampType, Some(Asia/Tokyo), false) as date) as timestamp))\n",
      "                           +- StreamingRelation FileSource[spark-the-definitive-guide-study/assets/exercises/week03/by-day/*.csv], [InvoiceNo#220, StockCode#221, Description#222, Quantity#223, InvoiceDate#224, UnitPrice#225, CustomerID#226, Country#227]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "purchaseByCustomerPerHour.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fff17227",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['CustomerID, window('InvoiceDate, 86400000000, 86400000000, 0) AS window#242], ['CustomerID, window('InvoiceDate, 86400000000, 86400000000, 0) AS window#242, sum(total_cost#236) AS sum(total_cost)#247]\n",
      "+- Project [CustomerID#226, (UnitPrice#225 * cast(Quantity#223 as double)) AS total_cost#236, to_date(to_timestamp(InvoiceDate#224, Some(M/d/yyyy H:mm), TimestampType, Some(Asia/Tokyo)), None, Some(Asia/Tokyo)) AS InvoiceDate#237]\n",
      "   +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@49f4de59,csv,List(),Some(StructType(StructField(InvoiceNo,StringType,true),StructField(StockCode,StringType,true),StructField(Description,StringType,true),StructField(Quantity,IntegerType,true),StructField(InvoiceDate,TimestampType,true),StructField(UnitPrice,DoubleType,true),StructField(CustomerID,DoubleType,true),StructField(Country,StringType,true))),List(),None,Map(maxFilesPerTrigger -> 1, header -> true, path -> spark-the-definitive-guide-study/assets/exercises/week03/by-day/*.csv),None), FileSource[spark-the-definitive-guide-study/assets/exercises/week03/by-day/*.csv], [InvoiceNo#220, StockCode#221, Description#222, Quantity#223, InvoiceDate#224, UnitPrice#225, CustomerID#226, Country#227]\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "CustomerID: double, window: struct<start:timestamp,end:timestamp>, sum(total_cost): double\n",
      "Aggregate [CustomerID#226, window#258], [CustomerID#226, window#258 AS window#242, sum(total_cost#236) AS sum(total_cost)#247]\n",
      "+- Project [named_struct(start, precisetimestampconversion(((precisetimestampconversion(cast(InvoiceDate#237 as timestamp), TimestampType, LongType) - (((precisetimestampconversion(cast(InvoiceDate#237 as timestamp), TimestampType, LongType) - 0) + 86400000000) % 86400000000)) - 0), LongType, TimestampType), end, precisetimestampconversion((((precisetimestampconversion(cast(InvoiceDate#237 as timestamp), TimestampType, LongType) - (((precisetimestampconversion(cast(InvoiceDate#237 as timestamp), TimestampType, LongType) - 0) + 86400000000) % 86400000000)) - 0) + 86400000000), LongType, TimestampType)) AS window#258, CustomerID#226, total_cost#236, InvoiceDate#237]\n",
      "   +- Filter isnotnull(cast(InvoiceDate#237 as timestamp))\n",
      "      +- Project [CustomerID#226, (UnitPrice#225 * cast(Quantity#223 as double)) AS total_cost#236, to_date(to_timestamp(InvoiceDate#224, Some(M/d/yyyy H:mm), TimestampType, Some(Asia/Tokyo)), None, Some(Asia/Tokyo)) AS InvoiceDate#237]\n",
      "         +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@49f4de59,csv,List(),Some(StructType(StructField(InvoiceNo,StringType,true),StructField(StockCode,StringType,true),StructField(Description,StringType,true),StructField(Quantity,IntegerType,true),StructField(InvoiceDate,TimestampType,true),StructField(UnitPrice,DoubleType,true),StructField(CustomerID,DoubleType,true),StructField(Country,StringType,true))),List(),None,Map(maxFilesPerTrigger -> 1, header -> true, path -> spark-the-definitive-guide-study/assets/exercises/week03/by-day/*.csv),None), FileSource[spark-the-definitive-guide-study/assets/exercises/week03/by-day/*.csv], [InvoiceNo#220, StockCode#221, Description#222, Quantity#223, InvoiceDate#224, UnitPrice#225, CustomerID#226, Country#227]\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [CustomerID#226, window#258], [CustomerID#226, window#258, sum(total_cost#236) AS sum(total_cost)#247]\n",
      "+- Project [named_struct(start, precisetimestampconversion(((precisetimestampconversion(cast(InvoiceDate#237 as timestamp), TimestampType, LongType) - (((precisetimestampconversion(cast(InvoiceDate#237 as timestamp), TimestampType, LongType) - 0) + 86400000000) % 86400000000)) - 0), LongType, TimestampType), end, precisetimestampconversion((((precisetimestampconversion(cast(InvoiceDate#237 as timestamp), TimestampType, LongType) - (((precisetimestampconversion(cast(InvoiceDate#237 as timestamp), TimestampType, LongType) - 0) + 86400000000) % 86400000000)) - 0) + 86400000000), LongType, TimestampType)) AS window#258, CustomerID#226, total_cost#236]\n",
      "   +- Project [CustomerID#226, (UnitPrice#225 * cast(Quantity#223 as double)) AS total_cost#236, cast(gettimestamp(InvoiceDate#224, M/d/yyyy H:mm, TimestampType, Some(Asia/Tokyo), false) as date) AS InvoiceDate#237]\n",
      "      +- Filter isnotnull(cast(cast(gettimestamp(InvoiceDate#224, M/d/yyyy H:mm, TimestampType, Some(Asia/Tokyo), false) as date) as timestamp))\n",
      "         +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@49f4de59,csv,List(),Some(StructType(StructField(InvoiceNo,StringType,true),StructField(StockCode,StringType,true),StructField(Description,StringType,true),StructField(Quantity,IntegerType,true),StructField(InvoiceDate,TimestampType,true),StructField(UnitPrice,DoubleType,true),StructField(CustomerID,DoubleType,true),StructField(Country,StringType,true))),List(),None,Map(maxFilesPerTrigger -> 1, header -> true, path -> spark-the-definitive-guide-study/assets/exercises/week03/by-day/*.csv),None), FileSource[spark-the-definitive-guide-study/assets/exercises/week03/by-day/*.csv], [InvoiceNo#220, StockCode#221, Description#222, Quantity#223, InvoiceDate#224, UnitPrice#225, CustomerID#226, Country#227]\n",
      "\n",
      "== Physical Plan ==\n",
      "*(4) HashAggregate(keys=[CustomerID#226, window#258], functions=[sum(total_cost#236)], output=[CustomerID#226, window#258, sum(total_cost)#247])\n",
      "+- StateStoreSave [CustomerID#226, window#258], state info [ checkpoint = <unknown>, runId = e918f2c4-fece-4f0f-ad4a-ba1b9af1c0c6, opId = 0, ver = 0, numPartitions = 200], Append, 0, 2\n",
      "   +- *(3) HashAggregate(keys=[CustomerID#226, window#258], functions=[merge_sum(total_cost#236)], output=[CustomerID#226, window#258, sum#254])\n",
      "      +- StateStoreRestore [CustomerID#226, window#258], state info [ checkpoint = <unknown>, runId = e918f2c4-fece-4f0f-ad4a-ba1b9af1c0c6, opId = 0, ver = 0, numPartitions = 200], 2\n",
      "         +- *(2) HashAggregate(keys=[CustomerID#226, window#258], functions=[merge_sum(total_cost#236)], output=[CustomerID#226, window#258, sum#254])\n",
      "            +- Exchange hashpartitioning(CustomerID#226, window#258, 200), ENSURE_REQUIREMENTS, [plan_id=344]\n",
      "               +- *(1) HashAggregate(keys=[knownfloatingpointnormalized(normalizenanandzero(CustomerID#226)) AS CustomerID#226, window#258], functions=[partial_sum(total_cost#236)], output=[CustomerID#226, window#258, sum#254])\n",
      "                  +- *(1) Project [named_struct(start, precisetimestampconversion(((precisetimestampconversion(cast(InvoiceDate#237 as timestamp), TimestampType, LongType) - (((precisetimestampconversion(cast(InvoiceDate#237 as timestamp), TimestampType, LongType) - 0) + 86400000000) % 86400000000)) - 0), LongType, TimestampType), end, precisetimestampconversion((((precisetimestampconversion(cast(InvoiceDate#237 as timestamp), TimestampType, LongType) - (((precisetimestampconversion(cast(InvoiceDate#237 as timestamp), TimestampType, LongType) - 0) + 86400000000) % 86400000000)) - 0) + 86400000000), LongType, TimestampType)) AS window#258, CustomerID#226, total_cost#236]\n",
      "                     +- *(1) Project [CustomerID#226, (UnitPrice#225 * cast(Quantity#223 as double)) AS total_cost#236, cast(gettimestamp(InvoiceDate#224, M/d/yyyy H:mm, TimestampType, Some(Asia/Tokyo), false) as date) AS InvoiceDate#237]\n",
      "                        +- *(1) Filter isnotnull(cast(cast(gettimestamp(InvoiceDate#224, M/d/yyyy H:mm, TimestampType, Some(Asia/Tokyo), false) as date) as timestamp))\n",
      "                           +- StreamingRelation FileSource[spark-the-definitive-guide-study/assets/exercises/week03/by-day/*.csv], [InvoiceNo#220, StockCode#221, Description#222, Quantity#223, InvoiceDate#224, UnitPrice#225, CustomerID#226, Country#227]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "purchaseByCustomerPerHour.explain(mode='extended')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "840068af",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Column 'Quantty' does not exist. Did you mean one of the following? [Quantity, Country, CustomerID, InvoiceNo, StockCode, UnitPrice, Description, InvoiceDate]; line 1 pos 13;\n'Project [CustomerID#226, (UnitPrice#225 * 'Quantty) AS total_cost#294, to_date(to_timestamp(InvoiceDate#224, Some(M/d/yyyy H:mm), TimestampType, Some(Asia/Tokyo)), None, Some(Asia/Tokyo)) AS InvoiceDate#295]\n+- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@49f4de59,csv,List(),Some(StructType(StructField(InvoiceNo,StringType,true),StructField(StockCode,StringType,true),StructField(Description,StringType,true),StructField(Quantity,IntegerType,true),StructField(InvoiceDate,TimestampType,true),StructField(UnitPrice,DoubleType,true),StructField(CustomerID,DoubleType,true),StructField(Country,StringType,true))),List(),None,Map(maxFilesPerTrigger -> 1, header -> true, path -> spark-the-definitive-guide-study/assets/exercises/week03/by-day/*.csv),None), FileSource[spark-the-definitive-guide-study/assets/exercises/week03/by-day/*.csv], [InvoiceNo#220, StockCode#221, Description#222, Quantity#223, InvoiceDate#224, UnitPrice#225, CustomerID#226, Country#227]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m purchaseByCustomerPerHourError \u001b[38;5;241m=\u001b[39m \u001b[43mstreamingDataFrame\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselectExpr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCustomerID\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m(UnitPrice * Quantty) as total_cost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mto_date(to_timestamp(InvoiceDate, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mM/d/yyyy H:mm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)) as InvoiceDate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\\\n\u001b[1;32m      6\u001b[0m \u001b[38;5;241m.\u001b[39mgroupBy(\n\u001b[1;32m      7\u001b[0m     F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCustomerID\u001b[39m\u001b[38;5;124m\"\u001b[39m), F\u001b[38;5;241m.\u001b[39mwindow(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvoiceDate\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1 day\u001b[39m\u001b[38;5;124m\"\u001b[39m))\\\n\u001b[1;32m      8\u001b[0m \u001b[38;5;241m.\u001b[39msum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_cost\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.3.1/libexec/python/pyspark/sql/dataframe.py:2048\u001b[0m, in \u001b[0;36mDataFrame.selectExpr\u001b[0;34m(self, *expr)\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(expr) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(expr[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m   2047\u001b[0m     expr \u001b[38;5;241m=\u001b[39m expr[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m-> 2048\u001b[0m jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselectExpr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jseq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2049\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.3.1/libexec/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.3.1/libexec/python/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Column 'Quantty' does not exist. Did you mean one of the following? [Quantity, Country, CustomerID, InvoiceNo, StockCode, UnitPrice, Description, InvoiceDate]; line 1 pos 13;\n'Project [CustomerID#226, (UnitPrice#225 * 'Quantty) AS total_cost#294, to_date(to_timestamp(InvoiceDate#224, Some(M/d/yyyy H:mm), TimestampType, Some(Asia/Tokyo)), None, Some(Asia/Tokyo)) AS InvoiceDate#295]\n+- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@49f4de59,csv,List(),Some(StructType(StructField(InvoiceNo,StringType,true),StructField(StockCode,StringType,true),StructField(Description,StringType,true),StructField(Quantity,IntegerType,true),StructField(InvoiceDate,TimestampType,true),StructField(UnitPrice,DoubleType,true),StructField(CustomerID,DoubleType,true),StructField(Country,StringType,true))),List(),None,Map(maxFilesPerTrigger -> 1, header -> true, path -> spark-the-definitive-guide-study/assets/exercises/week03/by-day/*.csv),None), FileSource[spark-the-definitive-guide-study/assets/exercises/week03/by-day/*.csv], [InvoiceNo#220, StockCode#221, Description#222, Quantity#223, InvoiceDate#224, UnitPrice#225, CustomerID#226, Country#227]\n"
     ]
    }
   ],
   "source": [
    "purchaseByCustomerPerHourError = streamingDataFrame\\\n",
    ".selectExpr(\n",
    "    \"CustomerID\",\n",
    "    \"(UnitPrice * Quantty) as total_cost\", # Quantity 오타 테스트\n",
    "    \"to_date(to_timestamp(InvoiceDate, 'M/d/yyyy H:mm')) as InvoiceDate\")\\\n",
    ".groupBy(\n",
    "    F.col(\"CustomerID\"), F.window(F.col(\"InvoiceDate\"), \"1 day\"))\\\n",
    ".sum(\"total_cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e1d35d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "* HashAggregate (11)\n",
      "+- StateStoreSave (10)\n",
      "   +- * HashAggregate (9)\n",
      "      +- StateStoreRestore (8)\n",
      "         +- * HashAggregate (7)\n",
      "            +- Exchange (6)\n",
      "               +- * HashAggregate (5)\n",
      "                  +- * Project (4)\n",
      "                     +- * Project (3)\n",
      "                        +- * Filter (2)\n",
      "                           +- StreamingRelation (1)\n",
      "\n",
      "\n",
      "(1) StreamingRelation\n",
      "Output [8]: [InvoiceNo#220, StockCode#221, Description#222, Quantity#223, InvoiceDate#224, UnitPrice#225, CustomerID#226, Country#227]\n",
      "Arguments: FileSource[spark-the-definitive-guide-study/assets/exercises/week03/by-day/*.csv], [InvoiceNo#220, StockCode#221, Description#222, Quantity#223, InvoiceDate#224, UnitPrice#225, CustomerID#226, Country#227]\n",
      "\n",
      "(2) Filter [codegen id : 1]\n",
      "Input [8]: [InvoiceNo#220, StockCode#221, Description#222, Quantity#223, InvoiceDate#224, UnitPrice#225, CustomerID#226, Country#227]\n",
      "Condition : isnotnull(cast(cast(gettimestamp(InvoiceDate#224, M/d/yyyy H:mm, TimestampType, Some(Asia/Tokyo), false) as date) as timestamp))\n",
      "\n",
      "(3) Project [codegen id : 1]\n",
      "Output [3]: [CustomerID#226, (UnitPrice#225 * cast(Quantity#223 as double)) AS total_cost#296, cast(gettimestamp(InvoiceDate#224, M/d/yyyy H:mm, TimestampType, Some(Asia/Tokyo), false) as date) AS InvoiceDate#297]\n",
      "Input [8]: [InvoiceNo#220, StockCode#221, Description#222, Quantity#223, InvoiceDate#224, UnitPrice#225, CustomerID#226, Country#227]\n",
      "\n",
      "(4) Project [codegen id : 1]\n",
      "Output [3]: [named_struct(start, precisetimestampconversion(((precisetimestampconversion(cast(InvoiceDate#297 as timestamp), TimestampType, LongType) - (((precisetimestampconversion(cast(InvoiceDate#297 as timestamp), TimestampType, LongType) - 0) + 86400000000) % 86400000000)) - 0), LongType, TimestampType), end, precisetimestampconversion((((precisetimestampconversion(cast(InvoiceDate#297 as timestamp), TimestampType, LongType) - (((precisetimestampconversion(cast(InvoiceDate#297 as timestamp), TimestampType, LongType) - 0) + 86400000000) % 86400000000)) - 0) + 86400000000), LongType, TimestampType)) AS window#315, CustomerID#226, total_cost#296]\n",
      "Input [3]: [CustomerID#226, total_cost#296, InvoiceDate#297]\n",
      "\n",
      "(5) HashAggregate [codegen id : 1]\n",
      "Input [3]: [window#315, CustomerID#226, total_cost#296]\n",
      "Keys [2]: [knownfloatingpointnormalized(normalizenanandzero(CustomerID#226)) AS CustomerID#226, window#315]\n",
      "Functions [1]: [partial_sum(total_cost#296)]\n",
      "Aggregate Attributes [1]: [sum(total_cost#296)#305]\n",
      "Results [3]: [CustomerID#226, window#315, sum#313]\n",
      "\n",
      "(6) Exchange\n",
      "Input [3]: [CustomerID#226, window#315, sum#313]\n",
      "Arguments: hashpartitioning(CustomerID#226, window#315, 200), ENSURE_REQUIREMENTS, [plan_id=593]\n",
      "\n",
      "(7) HashAggregate [codegen id : 2]\n",
      "Input [3]: [CustomerID#226, window#315, sum#313]\n",
      "Keys [2]: [CustomerID#226, window#315]\n",
      "Functions [1]: [merge_sum(total_cost#296)]\n",
      "Aggregate Attributes [1]: [sum(total_cost#296)#305]\n",
      "Results [3]: [CustomerID#226, window#315, sum#313]\n",
      "\n",
      "(8) StateStoreRestore\n",
      "Input [3]: [CustomerID#226, window#315, sum#313]\n",
      "Arguments: [CustomerID#226, window#315], state info [ checkpoint = <unknown>, runId = f913adf7-59bb-46cf-9209-fcc962804624, opId = 0, ver = 0, numPartitions = 200], 2\n",
      "\n",
      "(9) HashAggregate [codegen id : 3]\n",
      "Input [3]: [CustomerID#226, window#315, sum#313]\n",
      "Keys [2]: [CustomerID#226, window#315]\n",
      "Functions [1]: [merge_sum(total_cost#296)]\n",
      "Aggregate Attributes [1]: [sum(total_cost#296)#305]\n",
      "Results [3]: [CustomerID#226, window#315, sum#313]\n",
      "\n",
      "(10) StateStoreSave\n",
      "Input [3]: [CustomerID#226, window#315, sum#313]\n",
      "Arguments: [CustomerID#226, window#315], state info [ checkpoint = <unknown>, runId = f913adf7-59bb-46cf-9209-fcc962804624, opId = 0, ver = 0, numPartitions = 200], Append, 0, 2\n",
      "\n",
      "(11) HashAggregate [codegen id : 4]\n",
      "Input [3]: [CustomerID#226, window#315, sum#313]\n",
      "Keys [2]: [CustomerID#226, window#315]\n",
      "Functions [1]: [sum(total_cost#296)]\n",
      "Aggregate Attributes [1]: [sum(total_cost#296)#305]\n",
      "Results [3]: [CustomerID#226, window#315, sum(total_cost#296)#305 AS sum(total_cost)#306]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "purchaseByCustomerPerHour.explain(mode='formatted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a0515a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 WholeStageCodegen subtrees.\n",
      "== Subtree 1 / 4 (maxMethodCodeSize:556; maxConstantPoolSize:313(0.48% used); numInnerClasses:0) ==\n",
      "*(1) HashAggregate(keys=[knownfloatingpointnormalized(normalizenanandzero(CustomerID#226)) AS CustomerID#226, window#319], functions=[partial_sum(total_cost#296)], output=[CustomerID#226, window#319, sum#313])\n",
      "+- *(1) Project [named_struct(start, precisetimestampconversion(((precisetimestampconversion(cast(InvoiceDate#297 as timestamp), TimestampType, LongType) - (((precisetimestampconversion(cast(InvoiceDate#297 as timestamp), TimestampType, LongType) - 0) + 86400000000) % 86400000000)) - 0), LongType, TimestampType), end, precisetimestampconversion((((precisetimestampconversion(cast(InvoiceDate#297 as timestamp), TimestampType, LongType) - (((precisetimestampconversion(cast(InvoiceDate#297 as timestamp), TimestampType, LongType) - 0) + 86400000000) % 86400000000)) - 0) + 86400000000), LongType, TimestampType)) AS window#319, CustomerID#226, total_cost#296]\n",
      "   +- *(1) Project [CustomerID#226, (UnitPrice#225 * cast(Quantity#223 as double)) AS total_cost#296, cast(gettimestamp(InvoiceDate#224, M/d/yyyy H:mm, TimestampType, Some(Asia/Tokyo), false) as date) AS InvoiceDate#297]\n",
      "      +- *(1) Filter isnotnull(cast(cast(gettimestamp(InvoiceDate#224, M/d/yyyy H:mm, TimestampType, Some(Asia/Tokyo), false) as date) as timestamp))\n",
      "         +- StreamingRelation FileSource[spark-the-definitive-guide-study/assets/exercises/week03/by-day/*.csv], [InvoiceNo#220, StockCode#221, Description#222, Quantity#223, InvoiceDate#224, UnitPrice#225, CustomerID#226, Country#227]\n",
      "\n",
      "Generated code:\n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean hashAgg_initAgg_0;\n",
      "/* 010 */   private org.apache.spark.unsafe.KVIterator hashAgg_mapIter_0;\n",
      "/* 011 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap hashAgg_hashMap_0;\n",
      "/* 012 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter hashAgg_sorter_0;\n",
      "/* 013 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 014 */   private long project_subExprValue_0;\n",
      "/* 015 */   private boolean project_subExprIsNull_0;\n",
      "/* 016 */   private long project_subExprValue_1;\n",
      "/* 017 */   private boolean project_subExprIsNull_1;\n",
      "/* 018 */   private boolean hashAgg_hashAgg_isNull_11_0;\n",
      "/* 019 */   private boolean hashAgg_hashAgg_isNull_13_0;\n",
      "/* 020 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[10];\n",
      "/* 021 */\n",
      "/* 022 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 023 */     this.references = references;\n",
      "/* 024 */   }\n",
      "/* 025 */\n",
      "/* 026 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 027 */     partitionIndex = index;\n",
      "/* 028 */     this.inputs = inputs;\n",
      "/* 029 */     wholestagecodegen_init_0_0();\n",
      "/* 030 */     wholestagecodegen_init_0_1();\n",
      "/* 031 */\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */   private void project_subExpr_1(boolean project_isNull_5, int project_value_5) {\n",
      "/* 035 */     boolean project_isNull_26 = true;\n",
      "/* 036 */     long project_value_26 = -1L;\n",
      "/* 037 */     boolean project_isNull_27 = true;\n",
      "/* 038 */     long project_value_27 = -1L;\n",
      "/* 039 */\n",
      "/* 040 */     if (!project_subExprIsNull_0) {\n",
      "/* 041 */       boolean project_isNull_28 = false;\n",
      "/* 042 */       long project_value_28 = -1L;\n",
      "/* 043 */       if (false || 86400000000L == 0) {\n",
      "/* 044 */         project_isNull_28 = true;\n",
      "/* 045 */       } else {\n",
      "/* 046 */         boolean project_isNull_29 = true;\n",
      "/* 047 */         long project_value_29 = -1L;\n",
      "/* 048 */         boolean project_isNull_30 = true;\n",
      "/* 049 */         long project_value_30 = -1L;\n",
      "/* 050 */\n",
      "/* 051 */         if (!project_subExprIsNull_0) {\n",
      "/* 052 */           project_isNull_30 = false; // resultCode could change nullability.\n",
      "/* 053 */\n",
      "/* 054 */           project_value_30 = project_subExprValue_0 - 0L;\n",
      "/* 055 */\n",
      "/* 056 */         }\n",
      "/* 057 */         if (!project_isNull_30) {\n",
      "/* 058 */           project_isNull_29 = false; // resultCode could change nullability.\n",
      "/* 059 */\n",
      "/* 060 */           project_value_29 = project_value_30 + 86400000000L;\n",
      "/* 061 */\n",
      "/* 062 */         }\n",
      "/* 063 */         if (project_isNull_29) {\n",
      "/* 064 */           project_isNull_28 = true;\n",
      "/* 065 */         } else {\n",
      "/* 066 */           project_value_28 = (long)(project_value_29 % 86400000000L);\n",
      "/* 067 */         }\n",
      "/* 068 */       }\n",
      "/* 069 */       if (!project_isNull_28) {\n",
      "/* 070 */         project_isNull_27 = false; // resultCode could change nullability.\n",
      "/* 071 */\n",
      "/* 072 */         project_value_27 = project_subExprValue_0 - project_value_28;\n",
      "/* 073 */\n",
      "/* 074 */       }\n",
      "/* 075 */\n",
      "/* 076 */     }\n",
      "/* 077 */     if (!project_isNull_27) {\n",
      "/* 078 */       project_isNull_26 = false; // resultCode could change nullability.\n",
      "/* 079 */\n",
      "/* 080 */       project_value_26 = project_value_27 - 0L;\n",
      "/* 081 */\n",
      "/* 082 */     }\n",
      "/* 083 */     project_subExprIsNull_1 = project_isNull_26;\n",
      "/* 084 */     project_subExprValue_1 = project_value_26;\n",
      "/* 085 */   }\n",
      "/* 086 */\n",
      "/* 087 */   private void hashAgg_doAggregateWithKeys_0() throws java.io.IOException {\n",
      "/* 088 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 089 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 090 */\n",
      "/* 091 */       do {\n",
      "/* 092 */         boolean inputadapter_isNull_4 = inputadapter_row_0.isNullAt(4);\n",
      "/* 093 */         long inputadapter_value_4 = inputadapter_isNull_4 ?\n",
      "/* 094 */         -1L : (inputadapter_row_0.getLong(4));\n",
      "/* 095 */\n",
      "/* 096 */         boolean filter_isNull_3 = inputadapter_isNull_4;\n",
      "/* 097 */         long filter_value_3 = -1L;\n",
      "/* 098 */         if (!filter_isNull_3) {\n",
      "/* 099 */           filter_value_3 = inputadapter_value_4 / 1;\n",
      "/* 100 */         }\n",
      "/* 101 */         boolean filter_isNull_2 = filter_isNull_3;\n",
      "/* 102 */         int filter_value_2 = -1;\n",
      "/* 103 */         if (!filter_isNull_3) {\n",
      "/* 104 */           filter_value_2 =\n",
      "/* 105 */           org.apache.spark.sql.catalyst.util.DateTimeUtils.microsToDays(filter_value_3, ((java.time.ZoneId) references[6] /* zoneId */));\n",
      "/* 106 */         }\n",
      "/* 107 */         boolean filter_isNull_1 = filter_isNull_2;\n",
      "/* 108 */         long filter_value_1 = -1L;\n",
      "/* 109 */         if (!filter_isNull_2) {\n",
      "/* 110 */           filter_value_1 =\n",
      "/* 111 */           org.apache.spark.sql.catalyst.util.DateTimeUtils.daysToMicros(filter_value_2, ((java.time.ZoneId) references[7] /* zoneId */));\n",
      "/* 112 */         }\n",
      "/* 113 */         boolean filter_value_5 = !filter_isNull_1;\n",
      "/* 114 */         if (!filter_value_5) continue;\n",
      "/* 115 */\n",
      "/* 116 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* numOutputRows */).add(1);\n",
      "/* 117 */\n",
      "/* 118 */         // common sub-expressions\n",
      "/* 119 */\n",
      "/* 120 */         boolean project_isNull_6 = inputadapter_isNull_4;\n",
      "/* 121 */         long project_value_6 = -1L;\n",
      "/* 122 */         if (!project_isNull_6) {\n",
      "/* 123 */           project_value_6 = inputadapter_value_4 / 1;\n",
      "/* 124 */         }\n",
      "/* 125 */         boolean project_isNull_5 = project_isNull_6;\n",
      "/* 126 */         int project_value_5 = -1;\n",
      "/* 127 */         if (!project_isNull_6) {\n",
      "/* 128 */           project_value_5 =\n",
      "/* 129 */           org.apache.spark.sql.catalyst.util.DateTimeUtils.microsToDays(project_value_6, ((java.time.ZoneId) references[8] /* zoneId */));\n",
      "/* 130 */         }\n",
      "/* 131 */\n",
      "/* 132 */         // common sub-expressions\n",
      "/* 133 */\n",
      "/* 134 */         project_subExpr_0(project_isNull_5, project_value_5);\n",
      "/* 135 */\n",
      "/* 136 */         project_subExpr_1(project_isNull_5, project_value_5);\n",
      "/* 137 */\n",
      "/* 138 */         Object[] project_values_0 = new Object[2];\n",
      "/* 139 */\n",
      "/* 140 */         boolean project_isNull_36 = project_subExprIsNull_1;\n",
      "/* 141 */         long project_value_36 = project_subExprValue_1;\n",
      "/* 142 */         if (project_isNull_36) {\n",
      "/* 143 */           project_values_0[0] = null;\n",
      "/* 144 */         } else {\n",
      "/* 145 */           project_values_0[0] = project_value_36;\n",
      "/* 146 */         }\n",
      "/* 147 */\n",
      "/* 148 */         boolean project_isNull_38 = true;\n",
      "/* 149 */         long project_value_38 = -1L;\n",
      "/* 150 */\n",
      "/* 151 */         if (!project_subExprIsNull_1) {\n",
      "/* 152 */           project_isNull_38 = false; // resultCode could change nullability.\n",
      "/* 153 */\n",
      "/* 154 */           project_value_38 = project_subExprValue_1 + 86400000000L;\n",
      "/* 155 */\n",
      "/* 156 */         }\n",
      "/* 157 */         boolean project_isNull_37 = project_isNull_38;\n",
      "/* 158 */         long project_value_37 = project_value_38;\n",
      "/* 159 */         if (project_isNull_37) {\n",
      "/* 160 */           project_values_0[1] = null;\n",
      "/* 161 */         } else {\n",
      "/* 162 */           project_values_0[1] = project_value_37;\n",
      "/* 163 */         }\n",
      "/* 164 */\n",
      "/* 165 */         final InternalRow project_value_35 = new org.apache.spark.sql.catalyst.expressions.GenericInternalRow(project_values_0);\n",
      "/* 166 */         project_values_0 = null;\n",
      "/* 167 */         boolean inputadapter_isNull_6 = inputadapter_row_0.isNullAt(6);\n",
      "/* 168 */         double inputadapter_value_6 = inputadapter_isNull_6 ?\n",
      "/* 169 */         -1.0 : (inputadapter_row_0.getDouble(6));\n",
      "/* 170 */         boolean project_isNull_1 = true;\n",
      "/* 171 */         double project_value_1 = -1.0;\n",
      "/* 172 */         boolean inputadapter_isNull_5 = inputadapter_row_0.isNullAt(5);\n",
      "/* 173 */         double inputadapter_value_5 = inputadapter_isNull_5 ?\n",
      "/* 174 */         -1.0 : (inputadapter_row_0.getDouble(5));\n",
      "/* 175 */         if (!inputadapter_isNull_5) {\n",
      "/* 176 */           boolean inputadapter_isNull_3 = inputadapter_row_0.isNullAt(3);\n",
      "/* 177 */           int inputadapter_value_3 = inputadapter_isNull_3 ?\n",
      "/* 178 */           -1 : (inputadapter_row_0.getInt(3));\n",
      "/* 179 */           boolean project_isNull_3 = inputadapter_isNull_3;\n",
      "/* 180 */           double project_value_3 = -1.0;\n",
      "/* 181 */           if (!inputadapter_isNull_3) {\n",
      "/* 182 */             project_value_3 = (double) inputadapter_value_3;\n",
      "/* 183 */           }\n",
      "/* 184 */           if (!project_isNull_3) {\n",
      "/* 185 */             project_isNull_1 = false; // resultCode could change nullability.\n",
      "/* 186 */\n",
      "/* 187 */             project_value_1 = inputadapter_value_5 * project_value_3;\n",
      "/* 188 */\n",
      "/* 189 */           }\n",
      "/* 190 */\n",
      "/* 191 */         }\n",
      "/* 192 */\n",
      "/* 193 */         hashAgg_doConsume_0(project_value_35, inputadapter_value_6, inputadapter_isNull_6, project_value_1, project_isNull_1);\n",
      "/* 194 */\n",
      "/* 195 */       } while(false);\n",
      "/* 196 */       // shouldStop check is eliminated\n",
      "/* 197 */     }\n",
      "/* 198 */\n",
      "/* 199 */     hashAgg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(hashAgg_hashMap_0, hashAgg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* avgHashProbe */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* numTasksFallBacked */));\n",
      "/* 200 */   }\n",
      "/* 201 */\n",
      "/* 202 */   private void hashAgg_doConsume_0(InternalRow hashAgg_expr_0_0, double hashAgg_expr_1_0, boolean hashAgg_exprIsNull_1_0, double hashAgg_expr_2_0, boolean hashAgg_exprIsNull_2_0) throws java.io.IOException {\n",
      "/* 203 */     UnsafeRow hashAgg_unsafeRowAggBuffer_0 = null;\n",
      "/* 204 */\n",
      "/* 205 */     // generate grouping key\n",
      "/* 206 */     filter_mutableStateArray_0[6].reset();\n",
      "/* 207 */\n",
      "/* 208 */     filter_mutableStateArray_0[6].zeroOutNullBytes();\n",
      "/* 209 */\n",
      "/* 210 */     boolean hashAgg_isNull_4 = hashAgg_exprIsNull_1_0;\n",
      "/* 211 */     double hashAgg_value_4 = -1.0;\n",
      "/* 212 */\n",
      "/* 213 */     if (!hashAgg_exprIsNull_1_0) {\n",
      "/* 214 */       if (Double.isNaN(hashAgg_expr_1_0)) {\n",
      "/* 215 */         hashAgg_value_4 = Double.NaN;\n",
      "/* 216 */       } else if (hashAgg_expr_1_0 == -0.0d) {\n",
      "/* 217 */         hashAgg_value_4 = 0.0d;\n",
      "/* 218 */       } else {\n",
      "/* 219 */         hashAgg_value_4 = hashAgg_expr_1_0;\n",
      "/* 220 */       }\n",
      "/* 221 */\n",
      "/* 222 */     }\n",
      "/* 223 */     if (hashAgg_isNull_4) {\n",
      "/* 224 */       filter_mutableStateArray_0[6].setNullAt(0);\n",
      "/* 225 */     } else {\n",
      "/* 226 */       filter_mutableStateArray_0[6].write(0, hashAgg_value_4);\n",
      "/* 227 */     }\n",
      "/* 228 */\n",
      "/* 229 */     final InternalRow hashAgg_tmpInput_1 = hashAgg_expr_0_0;\n",
      "/* 230 */     if (hashAgg_tmpInput_1 instanceof UnsafeRow) {\n",
      "/* 231 */       filter_mutableStateArray_0[6].write(1, (UnsafeRow) hashAgg_tmpInput_1);\n",
      "/* 232 */     } else {\n",
      "/* 233 */       // Remember the current cursor so that we can calculate how many bytes are\n",
      "/* 234 */       // written later.\n",
      "/* 235 */       final int hashAgg_previousCursor_1 = filter_mutableStateArray_0[6].cursor();\n",
      "/* 236 */\n",
      "/* 237 */       filter_mutableStateArray_0[7].resetRowWriter();\n",
      "/* 238 */\n",
      "/* 239 */       if ((hashAgg_tmpInput_1.isNullAt(0))) {\n",
      "/* 240 */         filter_mutableStateArray_0[7].setNullAt(0);\n",
      "/* 241 */       } else {\n",
      "/* 242 */         filter_mutableStateArray_0[7].write(0, (hashAgg_tmpInput_1.getLong(0)));\n",
      "/* 243 */       }\n",
      "/* 244 */\n",
      "/* 245 */       if ((hashAgg_tmpInput_1.isNullAt(1))) {\n",
      "/* 246 */         filter_mutableStateArray_0[7].setNullAt(1);\n",
      "/* 247 */       } else {\n",
      "/* 248 */         filter_mutableStateArray_0[7].write(1, (hashAgg_tmpInput_1.getLong(1)));\n",
      "/* 249 */       }\n",
      "/* 250 */\n",
      "/* 251 */       filter_mutableStateArray_0[6].setOffsetAndSizeFromPreviousCursor(1, hashAgg_previousCursor_1);\n",
      "/* 252 */     }\n",
      "/* 253 */     int hashAgg_unsafeRowKeyHash_0 = (filter_mutableStateArray_0[6].getRow()).hashCode();\n",
      "/* 254 */     if (true) {\n",
      "/* 255 */       // try to get the buffer from hash map\n",
      "/* 256 */       hashAgg_unsafeRowAggBuffer_0 =\n",
      "/* 257 */       hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow((filter_mutableStateArray_0[6].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 258 */     }\n",
      "/* 259 */     // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based\n",
      "/* 260 */     // aggregation after processing all input rows.\n",
      "/* 261 */     if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 262 */       if (hashAgg_sorter_0 == null) {\n",
      "/* 263 */         hashAgg_sorter_0 = hashAgg_hashMap_0.destructAndCreateExternalSorter();\n",
      "/* 264 */       } else {\n",
      "/* 265 */         hashAgg_sorter_0.merge(hashAgg_hashMap_0.destructAndCreateExternalSorter());\n",
      "/* 266 */       }\n",
      "/* 267 */\n",
      "/* 268 */       // the hash map had be spilled, it should have enough memory now,\n",
      "/* 269 */       // try to allocate buffer again.\n",
      "/* 270 */       hashAgg_unsafeRowAggBuffer_0 = hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow(\n",
      "/* 271 */         (filter_mutableStateArray_0[6].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 272 */       if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 273 */         // failed to allocate the first page\n",
      "/* 274 */         throw new org.apache.spark.memory.SparkOutOfMemoryError(\"No enough memory for aggregation\");\n",
      "/* 275 */       }\n",
      "/* 276 */     }\n",
      "/* 277 */\n",
      "/* 278 */     // common sub-expressions\n",
      "/* 279 */\n",
      "/* 280 */     // evaluate aggregate functions and update aggregation buffers\n",
      "/* 281 */     hashAgg_doAggregate_sum_0(hashAgg_unsafeRowAggBuffer_0, hashAgg_expr_2_0, hashAgg_exprIsNull_2_0);\n",
      "/* 282 */\n",
      "/* 283 */   }\n",
      "/* 284 */\n",
      "/* 285 */   private void project_subExpr_0(boolean project_isNull_5, int project_value_5) {\n",
      "/* 286 */     boolean project_isNull_24 = project_isNull_5;\n",
      "/* 287 */     long project_value_24 = -1L;\n",
      "/* 288 */     if (!project_isNull_5) {\n",
      "/* 289 */       project_value_24 =\n",
      "/* 290 */       org.apache.spark.sql.catalyst.util.DateTimeUtils.daysToMicros(project_value_5, ((java.time.ZoneId) references[10] /* zoneId */));\n",
      "/* 291 */     }\n",
      "/* 292 */     boolean project_isNull_23 = project_isNull_24;\n",
      "/* 293 */     long project_value_23 = project_value_24;\n",
      "/* 294 */     project_subExprIsNull_0 = project_isNull_23;\n",
      "/* 295 */     project_subExprValue_0 = project_value_23;\n",
      "/* 296 */   }\n",
      "/* 297 */\n",
      "/* 298 */   private void wholestagecodegen_init_0_1() {\n",
      "/* 299 */     filter_mutableStateArray_0[9] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(filter_mutableStateArray_0[8], 2);\n",
      "/* 300 */\n",
      "/* 301 */   }\n",
      "/* 302 */\n",
      "/* 303 */   private void hashAgg_doAggregate_sum_0(org.apache.spark.sql.catalyst.InternalRow hashAgg_unsafeRowAggBuffer_0, double hashAgg_expr_2_0, boolean hashAgg_exprIsNull_2_0) throws java.io.IOException {\n",
      "/* 304 */     hashAgg_hashAgg_isNull_11_0 = true;\n",
      "/* 305 */     double hashAgg_value_11 = -1.0;\n",
      "/* 306 */     do {\n",
      "/* 307 */       boolean hashAgg_isNull_12 = true;\n",
      "/* 308 */       double hashAgg_value_12 = -1.0;\n",
      "/* 309 */       hashAgg_hashAgg_isNull_13_0 = true;\n",
      "/* 310 */       double hashAgg_value_13 = -1.0;\n",
      "/* 311 */       do {\n",
      "/* 312 */         boolean hashAgg_isNull_14 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 313 */         double hashAgg_value_14 = hashAgg_isNull_14 ?\n",
      "/* 314 */         -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 315 */         if (!hashAgg_isNull_14) {\n",
      "/* 316 */           hashAgg_hashAgg_isNull_13_0 = false;\n",
      "/* 317 */           hashAgg_value_13 = hashAgg_value_14;\n",
      "/* 318 */           continue;\n",
      "/* 319 */         }\n",
      "/* 320 */\n",
      "/* 321 */         if (!false) {\n",
      "/* 322 */           hashAgg_hashAgg_isNull_13_0 = false;\n",
      "/* 323 */           hashAgg_value_13 = 0.0D;\n",
      "/* 324 */           continue;\n",
      "/* 325 */         }\n",
      "/* 326 */\n",
      "/* 327 */       } while (false);\n",
      "/* 328 */\n",
      "/* 329 */       if (!hashAgg_exprIsNull_2_0) {\n",
      "/* 330 */         hashAgg_isNull_12 = false; // resultCode could change nullability.\n",
      "/* 331 */\n",
      "/* 332 */         hashAgg_value_12 = hashAgg_value_13 + hashAgg_expr_2_0;\n",
      "/* 333 */\n",
      "/* 334 */       }\n",
      "/* 335 */       if (!hashAgg_isNull_12) {\n",
      "/* 336 */         hashAgg_hashAgg_isNull_11_0 = false;\n",
      "/* 337 */         hashAgg_value_11 = hashAgg_value_12;\n",
      "/* 338 */         continue;\n",
      "/* 339 */       }\n",
      "/* 340 */\n",
      "/* 341 */       boolean hashAgg_isNull_17 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 342 */       double hashAgg_value_17 = hashAgg_isNull_17 ?\n",
      "/* 343 */       -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 344 */       if (!hashAgg_isNull_17) {\n",
      "/* 345 */         hashAgg_hashAgg_isNull_11_0 = false;\n",
      "/* 346 */         hashAgg_value_11 = hashAgg_value_17;\n",
      "/* 347 */         continue;\n",
      "/* 348 */       }\n",
      "/* 349 */\n",
      "/* 350 */     } while (false);\n",
      "/* 351 */\n",
      "/* 352 */     if (!hashAgg_hashAgg_isNull_11_0) {\n",
      "/* 353 */       hashAgg_unsafeRowAggBuffer_0.setDouble(0, hashAgg_value_11);\n",
      "/* 354 */     } else {\n",
      "/* 355 */       hashAgg_unsafeRowAggBuffer_0.setNullAt(0);\n",
      "/* 356 */     }\n",
      "/* 357 */   }\n",
      "/* 358 */\n",
      "/* 359 */   private void hashAgg_doAggregateWithKeysOutput_0(UnsafeRow hashAgg_keyTerm_0, UnsafeRow hashAgg_bufferTerm_0)\n",
      "/* 360 */   throws java.io.IOException {\n",
      "/* 361 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[11] /* numOutputRows */).add(1);\n",
      "/* 362 */\n",
      "/* 363 */     boolean hashAgg_isNull_18 = hashAgg_keyTerm_0.isNullAt(0);\n",
      "/* 364 */     double hashAgg_value_18 = hashAgg_isNull_18 ?\n",
      "/* 365 */     -1.0 : (hashAgg_keyTerm_0.getDouble(0));\n",
      "/* 366 */     InternalRow hashAgg_value_19 = hashAgg_keyTerm_0.getStruct(1, 2);\n",
      "/* 367 */     boolean hashAgg_isNull_20 = hashAgg_bufferTerm_0.isNullAt(0);\n",
      "/* 368 */     double hashAgg_value_20 = hashAgg_isNull_20 ?\n",
      "/* 369 */     -1.0 : (hashAgg_bufferTerm_0.getDouble(0));\n",
      "/* 370 */\n",
      "/* 371 */     filter_mutableStateArray_0[8].reset();\n",
      "/* 372 */\n",
      "/* 373 */     filter_mutableStateArray_0[8].zeroOutNullBytes();\n",
      "/* 374 */\n",
      "/* 375 */     if (hashAgg_isNull_18) {\n",
      "/* 376 */       filter_mutableStateArray_0[8].setNullAt(0);\n",
      "/* 377 */     } else {\n",
      "/* 378 */       filter_mutableStateArray_0[8].write(0, hashAgg_value_18);\n",
      "/* 379 */     }\n",
      "/* 380 */\n",
      "/* 381 */     final InternalRow hashAgg_tmpInput_2 = hashAgg_value_19;\n",
      "/* 382 */     if (hashAgg_tmpInput_2 instanceof UnsafeRow) {\n",
      "/* 383 */       filter_mutableStateArray_0[8].write(1, (UnsafeRow) hashAgg_tmpInput_2);\n",
      "/* 384 */     } else {\n",
      "/* 385 */       // Remember the current cursor so that we can calculate how many bytes are\n",
      "/* 386 */       // written later.\n",
      "/* 387 */       final int hashAgg_previousCursor_2 = filter_mutableStateArray_0[8].cursor();\n",
      "/* 388 */\n",
      "/* 389 */       filter_mutableStateArray_0[9].resetRowWriter();\n",
      "/* 390 */\n",
      "/* 391 */       if ((hashAgg_tmpInput_2.isNullAt(0))) {\n",
      "/* 392 */         filter_mutableStateArray_0[9].setNullAt(0);\n",
      "/* 393 */       } else {\n",
      "/* 394 */         filter_mutableStateArray_0[9].write(0, (hashAgg_tmpInput_2.getLong(0)));\n",
      "/* 395 */       }\n",
      "/* 396 */\n",
      "/* 397 */       if ((hashAgg_tmpInput_2.isNullAt(1))) {\n",
      "/* 398 */         filter_mutableStateArray_0[9].setNullAt(1);\n",
      "/* 399 */       } else {\n",
      "/* 400 */         filter_mutableStateArray_0[9].write(1, (hashAgg_tmpInput_2.getLong(1)));\n",
      "/* 401 */       }\n",
      "/* 402 */\n",
      "/* 403 */       filter_mutableStateArray_0[8].setOffsetAndSizeFromPreviousCursor(1, hashAgg_previousCursor_2);\n",
      "/* 404 */     }\n",
      "/* 405 */\n",
      "/* 406 */     if (hashAgg_isNull_20) {\n",
      "/* 407 */       filter_mutableStateArray_0[8].setNullAt(2);\n",
      "/* 408 */     } else {\n",
      "/* 409 */       filter_mutableStateArray_0[8].write(2, hashAgg_value_20);\n",
      "/* 410 */     }\n",
      "/* 411 */     append((filter_mutableStateArray_0[8].getRow()));\n",
      "/* 412 */\n",
      "/* 413 */   }\n",
      "/* 414 */\n",
      "/* 415 */   protected void processNext() throws java.io.IOException {\n",
      "/* 416 */     if (!hashAgg_initAgg_0) {\n",
      "/* 417 */       hashAgg_initAgg_0 = true;\n",
      "/* 418 */\n",
      "/* 419 */       hashAgg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();\n",
      "/* 420 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();\n",
      "/* 421 */       hashAgg_doAggregateWithKeys_0();\n",
      "/* 422 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[12] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);\n",
      "/* 423 */     }\n",
      "/* 424 */     // output the result\n",
      "/* 425 */\n",
      "/* 426 */     while ( hashAgg_mapIter_0.next()) {\n",
      "/* 427 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_mapIter_0.getKey();\n",
      "/* 428 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_mapIter_0.getValue();\n",
      "/* 429 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 430 */       if (shouldStop()) return;\n",
      "/* 431 */     }\n",
      "/* 432 */     hashAgg_mapIter_0.close();\n",
      "/* 433 */     if (hashAgg_sorter_0 == null) {\n",
      "/* 434 */       hashAgg_hashMap_0.free();\n",
      "/* 435 */     }\n",
      "/* 436 */   }\n",
      "/* 437 */\n",
      "/* 438 */   private void wholestagecodegen_init_0_0() {\n",
      "/* 439 */     inputadapter_input_0 = inputs[0];\n",
      "/* 440 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(8, 128);\n",
      "/* 441 */     filter_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 0);\n",
      "/* 442 */     filter_mutableStateArray_0[2] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 32);\n",
      "/* 443 */     filter_mutableStateArray_0[3] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(filter_mutableStateArray_0[2], 2);\n",
      "/* 444 */     filter_mutableStateArray_0[4] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 32);\n",
      "/* 445 */     filter_mutableStateArray_0[5] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(filter_mutableStateArray_0[4], 2);\n",
      "/* 446 */     filter_mutableStateArray_0[6] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 447 */     filter_mutableStateArray_0[7] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(filter_mutableStateArray_0[6], 2);\n",
      "/* 448 */     filter_mutableStateArray_0[8] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 32);\n",
      "/* 449 */\n",
      "/* 450 */   }\n",
      "/* 451 */\n",
      "/* 452 */ }\n",
      "\n",
      "== Subtree 2 / 4 (maxMethodCodeSize:316; maxConstantPoolSize:243(0.37% used); numInnerClasses:0) ==\n",
      "*(2) HashAggregate(keys=[CustomerID#226, window#319], functions=[merge_sum(total_cost#296)], output=[CustomerID#226, window#319, sum#313])\n",
      "+- Exchange hashpartitioning(CustomerID#226, window#319, 200), ENSURE_REQUIREMENTS, [plan_id=759]\n",
      "   +- *(1) HashAggregate(keys=[knownfloatingpointnormalized(normalizenanandzero(CustomerID#226)) AS CustomerID#226, window#319], functions=[partial_sum(total_cost#296)], output=[CustomerID#226, window#319, sum#313])\n",
      "      +- *(1) Project [named_struct(start, precisetimestampconversion(((precisetimestampconversion(cast(InvoiceDate#297 as timestamp), TimestampType, LongType) - (((precisetimestampconversion(cast(InvoiceDate#297 as timestamp), TimestampType, LongType) - 0) + 86400000000) % 86400000000)) - 0), LongType, TimestampType), end, precisetimestampconversion((((precisetimestampconversion(cast(InvoiceDate#297 as timestamp), TimestampType, LongType) - (((precisetimestampconversion(cast(InvoiceDate#297 as timestamp), TimestampType, LongType) - 0) + 86400000000) % 86400000000)) - 0) + 86400000000), LongType, TimestampType)) AS window#319, CustomerID#226, total_cost#296]\n",
      "         +- *(1) Project [CustomerID#226, (UnitPrice#225 * cast(Quantity#223 as double)) AS total_cost#296, cast(gettimestamp(InvoiceDate#224, M/d/yyyy H:mm, TimestampType, Some(Asia/Tokyo), false) as date) AS InvoiceDate#297]\n",
      "            +- *(1) Filter isnotnull(cast(cast(gettimestamp(InvoiceDate#224, M/d/yyyy H:mm, TimestampType, Some(Asia/Tokyo), false) as date) as timestamp))\n",
      "               +- StreamingRelation FileSource[spark-the-definitive-guide-study/assets/exercises/week03/by-day/*.csv], [InvoiceNo#220, StockCode#221, Description#222, Quantity#223, InvoiceDate#224, UnitPrice#225, CustomerID#226, Country#227]\n",
      "\n",
      "Generated code:\n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage2(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=2\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean hashAgg_initAgg_0;\n",
      "/* 010 */   private org.apache.spark.unsafe.KVIterator hashAgg_mapIter_0;\n",
      "/* 011 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap hashAgg_hashMap_0;\n",
      "/* 012 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter hashAgg_sorter_0;\n",
      "/* 013 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 014 */   private boolean hashAgg_hashAgg_isNull_4_0;\n",
      "/* 015 */   private boolean hashAgg_hashAgg_isNull_6_0;\n",
      "/* 016 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] hashAgg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[4];\n",
      "/* 017 */\n",
      "/* 018 */   public GeneratedIteratorForCodegenStage2(Object[] references) {\n",
      "/* 019 */     this.references = references;\n",
      "/* 020 */   }\n",
      "/* 021 */\n",
      "/* 022 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 023 */     partitionIndex = index;\n",
      "/* 024 */     this.inputs = inputs;\n",
      "/* 025 */\n",
      "/* 026 */     inputadapter_input_0 = inputs[0];\n",
      "/* 027 */     hashAgg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 028 */     hashAgg_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(hashAgg_mutableStateArray_0[0], 2);\n",
      "/* 029 */     hashAgg_mutableStateArray_0[2] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 32);\n",
      "/* 030 */     hashAgg_mutableStateArray_0[3] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(hashAgg_mutableStateArray_0[2], 2);\n",
      "/* 031 */\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */   private void hashAgg_doAggregateWithKeys_0() throws java.io.IOException {\n",
      "/* 035 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 036 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 037 */\n",
      "/* 038 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 039 */       double inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 040 */       -1.0 : (inputadapter_row_0.getDouble(0));\n",
      "/* 041 */       InternalRow inputadapter_value_1 = inputadapter_row_0.getStruct(1, 2);\n",
      "/* 042 */       boolean inputadapter_isNull_2 = inputadapter_row_0.isNullAt(2);\n",
      "/* 043 */       double inputadapter_value_2 = inputadapter_isNull_2 ?\n",
      "/* 044 */       -1.0 : (inputadapter_row_0.getDouble(2));\n",
      "/* 045 */\n",
      "/* 046 */       hashAgg_doConsume_0(inputadapter_row_0, inputadapter_value_0, inputadapter_isNull_0, inputadapter_value_1, inputadapter_value_2, inputadapter_isNull_2);\n",
      "/* 047 */       // shouldStop check is eliminated\n",
      "/* 048 */     }\n",
      "/* 049 */\n",
      "/* 050 */     hashAgg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(hashAgg_hashMap_0, hashAgg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* avgHashProbe */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* numTasksFallBacked */));\n",
      "/* 051 */   }\n",
      "/* 052 */\n",
      "/* 053 */   private void hashAgg_doConsume_0(InternalRow inputadapter_row_0, double hashAgg_expr_0_0, boolean hashAgg_exprIsNull_0_0, InternalRow hashAgg_expr_1_0, double hashAgg_expr_2_0, boolean hashAgg_exprIsNull_2_0) throws java.io.IOException {\n",
      "/* 054 */     UnsafeRow hashAgg_unsafeRowAggBuffer_0 = null;\n",
      "/* 055 */\n",
      "/* 056 */     // generate grouping key\n",
      "/* 057 */     hashAgg_mutableStateArray_0[0].reset();\n",
      "/* 058 */\n",
      "/* 059 */     hashAgg_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 060 */\n",
      "/* 061 */     if (hashAgg_exprIsNull_0_0) {\n",
      "/* 062 */       hashAgg_mutableStateArray_0[0].setNullAt(0);\n",
      "/* 063 */     } else {\n",
      "/* 064 */       hashAgg_mutableStateArray_0[0].write(0, hashAgg_expr_0_0);\n",
      "/* 065 */     }\n",
      "/* 066 */\n",
      "/* 067 */     final InternalRow hashAgg_tmpInput_0 = hashAgg_expr_1_0;\n",
      "/* 068 */     if (hashAgg_tmpInput_0 instanceof UnsafeRow) {\n",
      "/* 069 */       hashAgg_mutableStateArray_0[0].write(1, (UnsafeRow) hashAgg_tmpInput_0);\n",
      "/* 070 */     } else {\n",
      "/* 071 */       // Remember the current cursor so that we can calculate how many bytes are\n",
      "/* 072 */       // written later.\n",
      "/* 073 */       final int hashAgg_previousCursor_0 = hashAgg_mutableStateArray_0[0].cursor();\n",
      "/* 074 */\n",
      "/* 075 */       hashAgg_mutableStateArray_0[1].resetRowWriter();\n",
      "/* 076 */\n",
      "/* 077 */       if ((hashAgg_tmpInput_0.isNullAt(0))) {\n",
      "/* 078 */         hashAgg_mutableStateArray_0[1].setNullAt(0);\n",
      "/* 079 */       } else {\n",
      "/* 080 */         hashAgg_mutableStateArray_0[1].write(0, (hashAgg_tmpInput_0.getLong(0)));\n",
      "/* 081 */       }\n",
      "/* 082 */\n",
      "/* 083 */       if ((hashAgg_tmpInput_0.isNullAt(1))) {\n",
      "/* 084 */         hashAgg_mutableStateArray_0[1].setNullAt(1);\n",
      "/* 085 */       } else {\n",
      "/* 086 */         hashAgg_mutableStateArray_0[1].write(1, (hashAgg_tmpInput_0.getLong(1)));\n",
      "/* 087 */       }\n",
      "/* 088 */\n",
      "/* 089 */       hashAgg_mutableStateArray_0[0].setOffsetAndSizeFromPreviousCursor(1, hashAgg_previousCursor_0);\n",
      "/* 090 */     }\n",
      "/* 091 */     int hashAgg_unsafeRowKeyHash_0 = (hashAgg_mutableStateArray_0[0].getRow()).hashCode();\n",
      "/* 092 */     if (true) {\n",
      "/* 093 */       // try to get the buffer from hash map\n",
      "/* 094 */       hashAgg_unsafeRowAggBuffer_0 =\n",
      "/* 095 */       hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow((hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 096 */     }\n",
      "/* 097 */     // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based\n",
      "/* 098 */     // aggregation after processing all input rows.\n",
      "/* 099 */     if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 100 */       if (hashAgg_sorter_0 == null) {\n",
      "/* 101 */         hashAgg_sorter_0 = hashAgg_hashMap_0.destructAndCreateExternalSorter();\n",
      "/* 102 */       } else {\n",
      "/* 103 */         hashAgg_sorter_0.merge(hashAgg_hashMap_0.destructAndCreateExternalSorter());\n",
      "/* 104 */       }\n",
      "/* 105 */\n",
      "/* 106 */       // the hash map had be spilled, it should have enough memory now,\n",
      "/* 107 */       // try to allocate buffer again.\n",
      "/* 108 */       hashAgg_unsafeRowAggBuffer_0 = hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow(\n",
      "/* 109 */         (hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 110 */       if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 111 */         // failed to allocate the first page\n",
      "/* 112 */         throw new org.apache.spark.memory.SparkOutOfMemoryError(\"No enough memory for aggregation\");\n",
      "/* 113 */       }\n",
      "/* 114 */     }\n",
      "/* 115 */\n",
      "/* 116 */     // common sub-expressions\n",
      "/* 117 */\n",
      "/* 118 */     // evaluate aggregate functions and update aggregation buffers\n",
      "/* 119 */     hashAgg_doAggregate_sum_0(hashAgg_unsafeRowAggBuffer_0, hashAgg_expr_2_0, hashAgg_exprIsNull_2_0);\n",
      "/* 120 */\n",
      "/* 121 */   }\n",
      "/* 122 */\n",
      "/* 123 */   private void hashAgg_doAggregate_sum_0(org.apache.spark.sql.catalyst.InternalRow hashAgg_unsafeRowAggBuffer_0, double hashAgg_expr_2_0, boolean hashAgg_exprIsNull_2_0) throws java.io.IOException {\n",
      "/* 124 */     hashAgg_hashAgg_isNull_4_0 = true;\n",
      "/* 125 */     double hashAgg_value_4 = -1.0;\n",
      "/* 126 */     do {\n",
      "/* 127 */       boolean hashAgg_isNull_5 = true;\n",
      "/* 128 */       double hashAgg_value_5 = -1.0;\n",
      "/* 129 */       hashAgg_hashAgg_isNull_6_0 = true;\n",
      "/* 130 */       double hashAgg_value_6 = -1.0;\n",
      "/* 131 */       do {\n",
      "/* 132 */         boolean hashAgg_isNull_7 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 133 */         double hashAgg_value_7 = hashAgg_isNull_7 ?\n",
      "/* 134 */         -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 135 */         if (!hashAgg_isNull_7) {\n",
      "/* 136 */           hashAgg_hashAgg_isNull_6_0 = false;\n",
      "/* 137 */           hashAgg_value_6 = hashAgg_value_7;\n",
      "/* 138 */           continue;\n",
      "/* 139 */         }\n",
      "/* 140 */\n",
      "/* 141 */         if (!false) {\n",
      "/* 142 */           hashAgg_hashAgg_isNull_6_0 = false;\n",
      "/* 143 */           hashAgg_value_6 = 0.0D;\n",
      "/* 144 */           continue;\n",
      "/* 145 */         }\n",
      "/* 146 */\n",
      "/* 147 */       } while (false);\n",
      "/* 148 */\n",
      "/* 149 */       if (!hashAgg_exprIsNull_2_0) {\n",
      "/* 150 */         hashAgg_isNull_5 = false; // resultCode could change nullability.\n",
      "/* 151 */\n",
      "/* 152 */         hashAgg_value_5 = hashAgg_value_6 + hashAgg_expr_2_0;\n",
      "/* 153 */\n",
      "/* 154 */       }\n",
      "/* 155 */       if (!hashAgg_isNull_5) {\n",
      "/* 156 */         hashAgg_hashAgg_isNull_4_0 = false;\n",
      "/* 157 */         hashAgg_value_4 = hashAgg_value_5;\n",
      "/* 158 */         continue;\n",
      "/* 159 */       }\n",
      "/* 160 */\n",
      "/* 161 */       boolean hashAgg_isNull_10 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 162 */       double hashAgg_value_10 = hashAgg_isNull_10 ?\n",
      "/* 163 */       -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 164 */       if (!hashAgg_isNull_10) {\n",
      "/* 165 */         hashAgg_hashAgg_isNull_4_0 = false;\n",
      "/* 166 */         hashAgg_value_4 = hashAgg_value_10;\n",
      "/* 167 */         continue;\n",
      "/* 168 */       }\n",
      "/* 169 */\n",
      "/* 170 */     } while (false);\n",
      "/* 171 */\n",
      "/* 172 */     if (!hashAgg_hashAgg_isNull_4_0) {\n",
      "/* 173 */       hashAgg_unsafeRowAggBuffer_0.setDouble(0, hashAgg_value_4);\n",
      "/* 174 */     } else {\n",
      "/* 175 */       hashAgg_unsafeRowAggBuffer_0.setNullAt(0);\n",
      "/* 176 */     }\n",
      "/* 177 */   }\n",
      "/* 178 */\n",
      "/* 179 */   private void hashAgg_doAggregateWithKeysOutput_0(UnsafeRow hashAgg_keyTerm_0, UnsafeRow hashAgg_bufferTerm_0)\n",
      "/* 180 */   throws java.io.IOException {\n",
      "/* 181 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* numOutputRows */).add(1);\n",
      "/* 182 */\n",
      "/* 183 */     boolean hashAgg_isNull_11 = hashAgg_keyTerm_0.isNullAt(0);\n",
      "/* 184 */     double hashAgg_value_11 = hashAgg_isNull_11 ?\n",
      "/* 185 */     -1.0 : (hashAgg_keyTerm_0.getDouble(0));\n",
      "/* 186 */     InternalRow hashAgg_value_12 = hashAgg_keyTerm_0.getStruct(1, 2);\n",
      "/* 187 */     boolean hashAgg_isNull_13 = hashAgg_bufferTerm_0.isNullAt(0);\n",
      "/* 188 */     double hashAgg_value_13 = hashAgg_isNull_13 ?\n",
      "/* 189 */     -1.0 : (hashAgg_bufferTerm_0.getDouble(0));\n",
      "/* 190 */\n",
      "/* 191 */     hashAgg_mutableStateArray_0[2].reset();\n",
      "/* 192 */\n",
      "/* 193 */     hashAgg_mutableStateArray_0[2].zeroOutNullBytes();\n",
      "/* 194 */\n",
      "/* 195 */     if (hashAgg_isNull_11) {\n",
      "/* 196 */       hashAgg_mutableStateArray_0[2].setNullAt(0);\n",
      "/* 197 */     } else {\n",
      "/* 198 */       hashAgg_mutableStateArray_0[2].write(0, hashAgg_value_11);\n",
      "/* 199 */     }\n",
      "/* 200 */\n",
      "/* 201 */     final InternalRow hashAgg_tmpInput_1 = hashAgg_value_12;\n",
      "/* 202 */     if (hashAgg_tmpInput_1 instanceof UnsafeRow) {\n",
      "/* 203 */       hashAgg_mutableStateArray_0[2].write(1, (UnsafeRow) hashAgg_tmpInput_1);\n",
      "/* 204 */     } else {\n",
      "/* 205 */       // Remember the current cursor so that we can calculate how many bytes are\n",
      "/* 206 */       // written later.\n",
      "/* 207 */       final int hashAgg_previousCursor_1 = hashAgg_mutableStateArray_0[2].cursor();\n",
      "/* 208 */\n",
      "/* 209 */       hashAgg_mutableStateArray_0[3].resetRowWriter();\n",
      "/* 210 */\n",
      "/* 211 */       if ((hashAgg_tmpInput_1.isNullAt(0))) {\n",
      "/* 212 */         hashAgg_mutableStateArray_0[3].setNullAt(0);\n",
      "/* 213 */       } else {\n",
      "/* 214 */         hashAgg_mutableStateArray_0[3].write(0, (hashAgg_tmpInput_1.getLong(0)));\n",
      "/* 215 */       }\n",
      "/* 216 */\n",
      "/* 217 */       if ((hashAgg_tmpInput_1.isNullAt(1))) {\n",
      "/* 218 */         hashAgg_mutableStateArray_0[3].setNullAt(1);\n",
      "/* 219 */       } else {\n",
      "/* 220 */         hashAgg_mutableStateArray_0[3].write(1, (hashAgg_tmpInput_1.getLong(1)));\n",
      "/* 221 */       }\n",
      "/* 222 */\n",
      "/* 223 */       hashAgg_mutableStateArray_0[2].setOffsetAndSizeFromPreviousCursor(1, hashAgg_previousCursor_1);\n",
      "/* 224 */     }\n",
      "/* 225 */\n",
      "/* 226 */     if (hashAgg_isNull_13) {\n",
      "/* 227 */       hashAgg_mutableStateArray_0[2].setNullAt(2);\n",
      "/* 228 */     } else {\n",
      "/* 229 */       hashAgg_mutableStateArray_0[2].write(2, hashAgg_value_13);\n",
      "/* 230 */     }\n",
      "/* 231 */     append((hashAgg_mutableStateArray_0[2].getRow()));\n",
      "/* 232 */\n",
      "/* 233 */   }\n",
      "/* 234 */\n",
      "/* 235 */   protected void processNext() throws java.io.IOException {\n",
      "/* 236 */     if (!hashAgg_initAgg_0) {\n",
      "/* 237 */       hashAgg_initAgg_0 = true;\n",
      "/* 238 */\n",
      "/* 239 */       hashAgg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();\n",
      "/* 240 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();\n",
      "/* 241 */       hashAgg_doAggregateWithKeys_0();\n",
      "/* 242 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[6] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);\n",
      "/* 243 */     }\n",
      "/* 244 */     // output the result\n",
      "/* 245 */\n",
      "/* 246 */     while ( hashAgg_mapIter_0.next()) {\n",
      "/* 247 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_mapIter_0.getKey();\n",
      "/* 248 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_mapIter_0.getValue();\n",
      "/* 249 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 250 */       if (shouldStop()) return;\n",
      "/* 251 */     }\n",
      "/* 252 */     hashAgg_mapIter_0.close();\n",
      "/* 253 */     if (hashAgg_sorter_0 == null) {\n",
      "/* 254 */       hashAgg_hashMap_0.free();\n",
      "/* 255 */     }\n",
      "/* 256 */   }\n",
      "/* 257 */\n",
      "/* 258 */ }\n",
      "\n",
      "== Subtree 3 / 4 (maxMethodCodeSize:316; maxConstantPoolSize:243(0.37% used); numInnerClasses:0) ==\n",
      "*(3) HashAggregate(keys=[CustomerID#226, window#319], functions=[merge_sum(total_cost#296)], output=[CustomerID#226, window#319, sum#313])\n",
      "+- StateStoreRestore [CustomerID#226, window#319], state info [ checkpoint = <unknown>, runId = 0e067d86-f5a4-424b-bb8b-d84d10289e65, opId = 0, ver = 0, numPartitions = 200], 2\n",
      "   +- *(2) HashAggregate(keys=[CustomerID#226, window#319], functions=[merge_sum(total_cost#296)], output=[CustomerID#226, window#319, sum#313])\n",
      "      +- Exchange hashpartitioning(CustomerID#226, window#319, 200), ENSURE_REQUIREMENTS, [plan_id=759]\n",
      "         +- *(1) HashAggregate(keys=[knownfloatingpointnormalized(normalizenanandzero(CustomerID#226)) AS CustomerID#226, window#319], functions=[partial_sum(total_cost#296)], output=[CustomerID#226, window#319, sum#313])\n",
      "            +- *(1) Project [named_struct(start, precisetimestampconversion(((precisetimestampconversion(cast(InvoiceDate#297 as timestamp), TimestampType, LongType) - (((precisetimestampconversion(cast(InvoiceDate#297 as timestamp), TimestampType, LongType) - 0) + 86400000000) % 86400000000)) - 0), LongType, TimestampType), end, precisetimestampconversion((((precisetimestampconversion(cast(InvoiceDate#297 as timestamp), TimestampType, LongType) - (((precisetimestampconversion(cast(InvoiceDate#297 as timestamp), TimestampType, LongType) - 0) + 86400000000) % 86400000000)) - 0) + 86400000000), LongType, TimestampType)) AS window#319, CustomerID#226, total_cost#296]\n",
      "               +- *(1) Project [CustomerID#226, (UnitPrice#225 * cast(Quantity#223 as double)) AS total_cost#296, cast(gettimestamp(InvoiceDate#224, M/d/yyyy H:mm, TimestampType, Some(Asia/Tokyo), false) as date) AS InvoiceDate#297]\n",
      "                  +- *(1) Filter isnotnull(cast(cast(gettimestamp(InvoiceDate#224, M/d/yyyy H:mm, TimestampType, Some(Asia/Tokyo), false) as date) as timestamp))\n",
      "                     +- StreamingRelation FileSource[spark-the-definitive-guide-study/assets/exercises/week03/by-day/*.csv], [InvoiceNo#220, StockCode#221, Description#222, Quantity#223, InvoiceDate#224, UnitPrice#225, CustomerID#226, Country#227]\n",
      "\n",
      "Generated code:\n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage3(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=3\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage3 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean hashAgg_initAgg_0;\n",
      "/* 010 */   private org.apache.spark.unsafe.KVIterator hashAgg_mapIter_0;\n",
      "/* 011 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap hashAgg_hashMap_0;\n",
      "/* 012 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter hashAgg_sorter_0;\n",
      "/* 013 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 014 */   private boolean hashAgg_hashAgg_isNull_4_0;\n",
      "/* 015 */   private boolean hashAgg_hashAgg_isNull_6_0;\n",
      "/* 016 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] hashAgg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[4];\n",
      "/* 017 */\n",
      "/* 018 */   public GeneratedIteratorForCodegenStage3(Object[] references) {\n",
      "/* 019 */     this.references = references;\n",
      "/* 020 */   }\n",
      "/* 021 */\n",
      "/* 022 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 023 */     partitionIndex = index;\n",
      "/* 024 */     this.inputs = inputs;\n",
      "/* 025 */\n",
      "/* 026 */     inputadapter_input_0 = inputs[0];\n",
      "/* 027 */     hashAgg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 028 */     hashAgg_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(hashAgg_mutableStateArray_0[0], 2);\n",
      "/* 029 */     hashAgg_mutableStateArray_0[2] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 32);\n",
      "/* 030 */     hashAgg_mutableStateArray_0[3] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(hashAgg_mutableStateArray_0[2], 2);\n",
      "/* 031 */\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */   private void hashAgg_doAggregateWithKeys_0() throws java.io.IOException {\n",
      "/* 035 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 036 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 037 */\n",
      "/* 038 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 039 */       double inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 040 */       -1.0 : (inputadapter_row_0.getDouble(0));\n",
      "/* 041 */       InternalRow inputadapter_value_1 = inputadapter_row_0.getStruct(1, 2);\n",
      "/* 042 */       boolean inputadapter_isNull_2 = inputadapter_row_0.isNullAt(2);\n",
      "/* 043 */       double inputadapter_value_2 = inputadapter_isNull_2 ?\n",
      "/* 044 */       -1.0 : (inputadapter_row_0.getDouble(2));\n",
      "/* 045 */\n",
      "/* 046 */       hashAgg_doConsume_0(inputadapter_row_0, inputadapter_value_0, inputadapter_isNull_0, inputadapter_value_1, inputadapter_value_2, inputadapter_isNull_2);\n",
      "/* 047 */       // shouldStop check is eliminated\n",
      "/* 048 */     }\n",
      "/* 049 */\n",
      "/* 050 */     hashAgg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(hashAgg_hashMap_0, hashAgg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* avgHashProbe */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* numTasksFallBacked */));\n",
      "/* 051 */   }\n",
      "/* 052 */\n",
      "/* 053 */   private void hashAgg_doConsume_0(InternalRow inputadapter_row_0, double hashAgg_expr_0_0, boolean hashAgg_exprIsNull_0_0, InternalRow hashAgg_expr_1_0, double hashAgg_expr_2_0, boolean hashAgg_exprIsNull_2_0) throws java.io.IOException {\n",
      "/* 054 */     UnsafeRow hashAgg_unsafeRowAggBuffer_0 = null;\n",
      "/* 055 */\n",
      "/* 056 */     // generate grouping key\n",
      "/* 057 */     hashAgg_mutableStateArray_0[0].reset();\n",
      "/* 058 */\n",
      "/* 059 */     hashAgg_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 060 */\n",
      "/* 061 */     if (hashAgg_exprIsNull_0_0) {\n",
      "/* 062 */       hashAgg_mutableStateArray_0[0].setNullAt(0);\n",
      "/* 063 */     } else {\n",
      "/* 064 */       hashAgg_mutableStateArray_0[0].write(0, hashAgg_expr_0_0);\n",
      "/* 065 */     }\n",
      "/* 066 */\n",
      "/* 067 */     final InternalRow hashAgg_tmpInput_0 = hashAgg_expr_1_0;\n",
      "/* 068 */     if (hashAgg_tmpInput_0 instanceof UnsafeRow) {\n",
      "/* 069 */       hashAgg_mutableStateArray_0[0].write(1, (UnsafeRow) hashAgg_tmpInput_0);\n",
      "/* 070 */     } else {\n",
      "/* 071 */       // Remember the current cursor so that we can calculate how many bytes are\n",
      "/* 072 */       // written later.\n",
      "/* 073 */       final int hashAgg_previousCursor_0 = hashAgg_mutableStateArray_0[0].cursor();\n",
      "/* 074 */\n",
      "/* 075 */       hashAgg_mutableStateArray_0[1].resetRowWriter();\n",
      "/* 076 */\n",
      "/* 077 */       if ((hashAgg_tmpInput_0.isNullAt(0))) {\n",
      "/* 078 */         hashAgg_mutableStateArray_0[1].setNullAt(0);\n",
      "/* 079 */       } else {\n",
      "/* 080 */         hashAgg_mutableStateArray_0[1].write(0, (hashAgg_tmpInput_0.getLong(0)));\n",
      "/* 081 */       }\n",
      "/* 082 */\n",
      "/* 083 */       if ((hashAgg_tmpInput_0.isNullAt(1))) {\n",
      "/* 084 */         hashAgg_mutableStateArray_0[1].setNullAt(1);\n",
      "/* 085 */       } else {\n",
      "/* 086 */         hashAgg_mutableStateArray_0[1].write(1, (hashAgg_tmpInput_0.getLong(1)));\n",
      "/* 087 */       }\n",
      "/* 088 */\n",
      "/* 089 */       hashAgg_mutableStateArray_0[0].setOffsetAndSizeFromPreviousCursor(1, hashAgg_previousCursor_0);\n",
      "/* 090 */     }\n",
      "/* 091 */     int hashAgg_unsafeRowKeyHash_0 = (hashAgg_mutableStateArray_0[0].getRow()).hashCode();\n",
      "/* 092 */     if (true) {\n",
      "/* 093 */       // try to get the buffer from hash map\n",
      "/* 094 */       hashAgg_unsafeRowAggBuffer_0 =\n",
      "/* 095 */       hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow((hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 096 */     }\n",
      "/* 097 */     // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based\n",
      "/* 098 */     // aggregation after processing all input rows.\n",
      "/* 099 */     if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 100 */       if (hashAgg_sorter_0 == null) {\n",
      "/* 101 */         hashAgg_sorter_0 = hashAgg_hashMap_0.destructAndCreateExternalSorter();\n",
      "/* 102 */       } else {\n",
      "/* 103 */         hashAgg_sorter_0.merge(hashAgg_hashMap_0.destructAndCreateExternalSorter());\n",
      "/* 104 */       }\n",
      "/* 105 */\n",
      "/* 106 */       // the hash map had be spilled, it should have enough memory now,\n",
      "/* 107 */       // try to allocate buffer again.\n",
      "/* 108 */       hashAgg_unsafeRowAggBuffer_0 = hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow(\n",
      "/* 109 */         (hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 110 */       if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 111 */         // failed to allocate the first page\n",
      "/* 112 */         throw new org.apache.spark.memory.SparkOutOfMemoryError(\"No enough memory for aggregation\");\n",
      "/* 113 */       }\n",
      "/* 114 */     }\n",
      "/* 115 */\n",
      "/* 116 */     // common sub-expressions\n",
      "/* 117 */\n",
      "/* 118 */     // evaluate aggregate functions and update aggregation buffers\n",
      "/* 119 */     hashAgg_doAggregate_sum_0(hashAgg_unsafeRowAggBuffer_0, hashAgg_expr_2_0, hashAgg_exprIsNull_2_0);\n",
      "/* 120 */\n",
      "/* 121 */   }\n",
      "/* 122 */\n",
      "/* 123 */   private void hashAgg_doAggregate_sum_0(org.apache.spark.sql.catalyst.InternalRow hashAgg_unsafeRowAggBuffer_0, double hashAgg_expr_2_0, boolean hashAgg_exprIsNull_2_0) throws java.io.IOException {\n",
      "/* 124 */     hashAgg_hashAgg_isNull_4_0 = true;\n",
      "/* 125 */     double hashAgg_value_4 = -1.0;\n",
      "/* 126 */     do {\n",
      "/* 127 */       boolean hashAgg_isNull_5 = true;\n",
      "/* 128 */       double hashAgg_value_5 = -1.0;\n",
      "/* 129 */       hashAgg_hashAgg_isNull_6_0 = true;\n",
      "/* 130 */       double hashAgg_value_6 = -1.0;\n",
      "/* 131 */       do {\n",
      "/* 132 */         boolean hashAgg_isNull_7 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 133 */         double hashAgg_value_7 = hashAgg_isNull_7 ?\n",
      "/* 134 */         -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 135 */         if (!hashAgg_isNull_7) {\n",
      "/* 136 */           hashAgg_hashAgg_isNull_6_0 = false;\n",
      "/* 137 */           hashAgg_value_6 = hashAgg_value_7;\n",
      "/* 138 */           continue;\n",
      "/* 139 */         }\n",
      "/* 140 */\n",
      "/* 141 */         if (!false) {\n",
      "/* 142 */           hashAgg_hashAgg_isNull_6_0 = false;\n",
      "/* 143 */           hashAgg_value_6 = 0.0D;\n",
      "/* 144 */           continue;\n",
      "/* 145 */         }\n",
      "/* 146 */\n",
      "/* 147 */       } while (false);\n",
      "/* 148 */\n",
      "/* 149 */       if (!hashAgg_exprIsNull_2_0) {\n",
      "/* 150 */         hashAgg_isNull_5 = false; // resultCode could change nullability.\n",
      "/* 151 */\n",
      "/* 152 */         hashAgg_value_5 = hashAgg_value_6 + hashAgg_expr_2_0;\n",
      "/* 153 */\n",
      "/* 154 */       }\n",
      "/* 155 */       if (!hashAgg_isNull_5) {\n",
      "/* 156 */         hashAgg_hashAgg_isNull_4_0 = false;\n",
      "/* 157 */         hashAgg_value_4 = hashAgg_value_5;\n",
      "/* 158 */         continue;\n",
      "/* 159 */       }\n",
      "/* 160 */\n",
      "/* 161 */       boolean hashAgg_isNull_10 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 162 */       double hashAgg_value_10 = hashAgg_isNull_10 ?\n",
      "/* 163 */       -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 164 */       if (!hashAgg_isNull_10) {\n",
      "/* 165 */         hashAgg_hashAgg_isNull_4_0 = false;\n",
      "/* 166 */         hashAgg_value_4 = hashAgg_value_10;\n",
      "/* 167 */         continue;\n",
      "/* 168 */       }\n",
      "/* 169 */\n",
      "/* 170 */     } while (false);\n",
      "/* 171 */\n",
      "/* 172 */     if (!hashAgg_hashAgg_isNull_4_0) {\n",
      "/* 173 */       hashAgg_unsafeRowAggBuffer_0.setDouble(0, hashAgg_value_4);\n",
      "/* 174 */     } else {\n",
      "/* 175 */       hashAgg_unsafeRowAggBuffer_0.setNullAt(0);\n",
      "/* 176 */     }\n",
      "/* 177 */   }\n",
      "/* 178 */\n",
      "/* 179 */   private void hashAgg_doAggregateWithKeysOutput_0(UnsafeRow hashAgg_keyTerm_0, UnsafeRow hashAgg_bufferTerm_0)\n",
      "/* 180 */   throws java.io.IOException {\n",
      "/* 181 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* numOutputRows */).add(1);\n",
      "/* 182 */\n",
      "/* 183 */     boolean hashAgg_isNull_11 = hashAgg_keyTerm_0.isNullAt(0);\n",
      "/* 184 */     double hashAgg_value_11 = hashAgg_isNull_11 ?\n",
      "/* 185 */     -1.0 : (hashAgg_keyTerm_0.getDouble(0));\n",
      "/* 186 */     InternalRow hashAgg_value_12 = hashAgg_keyTerm_0.getStruct(1, 2);\n",
      "/* 187 */     boolean hashAgg_isNull_13 = hashAgg_bufferTerm_0.isNullAt(0);\n",
      "/* 188 */     double hashAgg_value_13 = hashAgg_isNull_13 ?\n",
      "/* 189 */     -1.0 : (hashAgg_bufferTerm_0.getDouble(0));\n",
      "/* 190 */\n",
      "/* 191 */     hashAgg_mutableStateArray_0[2].reset();\n",
      "/* 192 */\n",
      "/* 193 */     hashAgg_mutableStateArray_0[2].zeroOutNullBytes();\n",
      "/* 194 */\n",
      "/* 195 */     if (hashAgg_isNull_11) {\n",
      "/* 196 */       hashAgg_mutableStateArray_0[2].setNullAt(0);\n",
      "/* 197 */     } else {\n",
      "/* 198 */       hashAgg_mutableStateArray_0[2].write(0, hashAgg_value_11);\n",
      "/* 199 */     }\n",
      "/* 200 */\n",
      "/* 201 */     final InternalRow hashAgg_tmpInput_1 = hashAgg_value_12;\n",
      "/* 202 */     if (hashAgg_tmpInput_1 instanceof UnsafeRow) {\n",
      "/* 203 */       hashAgg_mutableStateArray_0[2].write(1, (UnsafeRow) hashAgg_tmpInput_1);\n",
      "/* 204 */     } else {\n",
      "/* 205 */       // Remember the current cursor so that we can calculate how many bytes are\n",
      "/* 206 */       // written later.\n",
      "/* 207 */       final int hashAgg_previousCursor_1 = hashAgg_mutableStateArray_0[2].cursor();\n",
      "/* 208 */\n",
      "/* 209 */       hashAgg_mutableStateArray_0[3].resetRowWriter();\n",
      "/* 210 */\n",
      "/* 211 */       if ((hashAgg_tmpInput_1.isNullAt(0))) {\n",
      "/* 212 */         hashAgg_mutableStateArray_0[3].setNullAt(0);\n",
      "/* 213 */       } else {\n",
      "/* 214 */         hashAgg_mutableStateArray_0[3].write(0, (hashAgg_tmpInput_1.getLong(0)));\n",
      "/* 215 */       }\n",
      "/* 216 */\n",
      "/* 217 */       if ((hashAgg_tmpInput_1.isNullAt(1))) {\n",
      "/* 218 */         hashAgg_mutableStateArray_0[3].setNullAt(1);\n",
      "/* 219 */       } else {\n",
      "/* 220 */         hashAgg_mutableStateArray_0[3].write(1, (hashAgg_tmpInput_1.getLong(1)));\n",
      "/* 221 */       }\n",
      "/* 222 */\n",
      "/* 223 */       hashAgg_mutableStateArray_0[2].setOffsetAndSizeFromPreviousCursor(1, hashAgg_previousCursor_1);\n",
      "/* 224 */     }\n",
      "/* 225 */\n",
      "/* 226 */     if (hashAgg_isNull_13) {\n",
      "/* 227 */       hashAgg_mutableStateArray_0[2].setNullAt(2);\n",
      "/* 228 */     } else {\n",
      "/* 229 */       hashAgg_mutableStateArray_0[2].write(2, hashAgg_value_13);\n",
      "/* 230 */     }\n",
      "/* 231 */     append((hashAgg_mutableStateArray_0[2].getRow()));\n",
      "/* 232 */\n",
      "/* 233 */   }\n",
      "/* 234 */\n",
      "/* 235 */   protected void processNext() throws java.io.IOException {\n",
      "/* 236 */     if (!hashAgg_initAgg_0) {\n",
      "/* 237 */       hashAgg_initAgg_0 = true;\n",
      "/* 238 */\n",
      "/* 239 */       hashAgg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();\n",
      "/* 240 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();\n",
      "/* 241 */       hashAgg_doAggregateWithKeys_0();\n",
      "/* 242 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[6] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);\n",
      "/* 243 */     }\n",
      "/* 244 */     // output the result\n",
      "/* 245 */\n",
      "/* 246 */     while ( hashAgg_mapIter_0.next()) {\n",
      "/* 247 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_mapIter_0.getKey();\n",
      "/* 248 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_mapIter_0.getValue();\n",
      "/* 249 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 250 */       if (shouldStop()) return;\n",
      "/* 251 */     }\n",
      "/* 252 */     hashAgg_mapIter_0.close();\n",
      "/* 253 */     if (hashAgg_sorter_0 == null) {\n",
      "/* 254 */       hashAgg_hashMap_0.free();\n",
      "/* 255 */     }\n",
      "/* 256 */   }\n",
      "/* 257 */\n",
      "/* 258 */ }\n",
      "\n",
      "== Subtree 4 / 4 (maxMethodCodeSize:316; maxConstantPoolSize:243(0.37% used); numInnerClasses:0) ==\n",
      "*(4) HashAggregate(keys=[CustomerID#226, window#319], functions=[sum(total_cost#296)], output=[CustomerID#226, window#319, sum(total_cost)#306])\n",
      "+- StateStoreSave [CustomerID#226, window#319], state info [ checkpoint = <unknown>, runId = 0e067d86-f5a4-424b-bb8b-d84d10289e65, opId = 0, ver = 0, numPartitions = 200], Append, 0, 2\n",
      "   +- *(3) HashAggregate(keys=[CustomerID#226, window#319], functions=[merge_sum(total_cost#296)], output=[CustomerID#226, window#319, sum#313])\n",
      "      +- StateStoreRestore [CustomerID#226, window#319], state info [ checkpoint = <unknown>, runId = 0e067d86-f5a4-424b-bb8b-d84d10289e65, opId = 0, ver = 0, numPartitions = 200], 2\n",
      "         +- *(2) HashAggregate(keys=[CustomerID#226, window#319], functions=[merge_sum(total_cost#296)], output=[CustomerID#226, window#319, sum#313])\n",
      "            +- Exchange hashpartitioning(CustomerID#226, window#319, 200), ENSURE_REQUIREMENTS, [plan_id=759]\n",
      "               +- *(1) HashAggregate(keys=[knownfloatingpointnormalized(normalizenanandzero(CustomerID#226)) AS CustomerID#226, window#319], functions=[partial_sum(total_cost#296)], output=[CustomerID#226, window#319, sum#313])\n",
      "                  +- *(1) Project [named_struct(start, precisetimestampconversion(((precisetimestampconversion(cast(InvoiceDate#297 as timestamp), TimestampType, LongType) - (((precisetimestampconversion(cast(InvoiceDate#297 as timestamp), TimestampType, LongType) - 0) + 86400000000) % 86400000000)) - 0), LongType, TimestampType), end, precisetimestampconversion((((precisetimestampconversion(cast(InvoiceDate#297 as timestamp), TimestampType, LongType) - (((precisetimestampconversion(cast(InvoiceDate#297 as timestamp), TimestampType, LongType) - 0) + 86400000000) % 86400000000)) - 0) + 86400000000), LongType, TimestampType)) AS window#319, CustomerID#226, total_cost#296]\n",
      "                     +- *(1) Project [CustomerID#226, (UnitPrice#225 * cast(Quantity#223 as double)) AS total_cost#296, cast(gettimestamp(InvoiceDate#224, M/d/yyyy H:mm, TimestampType, Some(Asia/Tokyo), false) as date) AS InvoiceDate#297]\n",
      "                        +- *(1) Filter isnotnull(cast(cast(gettimestamp(InvoiceDate#224, M/d/yyyy H:mm, TimestampType, Some(Asia/Tokyo), false) as date) as timestamp))\n",
      "                           +- StreamingRelation FileSource[spark-the-definitive-guide-study/assets/exercises/week03/by-day/*.csv], [InvoiceNo#220, StockCode#221, Description#222, Quantity#223, InvoiceDate#224, UnitPrice#225, CustomerID#226, Country#227]\n",
      "\n",
      "Generated code:\n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage4(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=4\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage4 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean hashAgg_initAgg_0;\n",
      "/* 010 */   private org.apache.spark.unsafe.KVIterator hashAgg_mapIter_0;\n",
      "/* 011 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap hashAgg_hashMap_0;\n",
      "/* 012 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter hashAgg_sorter_0;\n",
      "/* 013 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 014 */   private boolean hashAgg_hashAgg_isNull_4_0;\n",
      "/* 015 */   private boolean hashAgg_hashAgg_isNull_6_0;\n",
      "/* 016 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] hashAgg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[4];\n",
      "/* 017 */\n",
      "/* 018 */   public GeneratedIteratorForCodegenStage4(Object[] references) {\n",
      "/* 019 */     this.references = references;\n",
      "/* 020 */   }\n",
      "/* 021 */\n",
      "/* 022 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 023 */     partitionIndex = index;\n",
      "/* 024 */     this.inputs = inputs;\n",
      "/* 025 */\n",
      "/* 026 */     inputadapter_input_0 = inputs[0];\n",
      "/* 027 */     hashAgg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 028 */     hashAgg_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(hashAgg_mutableStateArray_0[0], 2);\n",
      "/* 029 */     hashAgg_mutableStateArray_0[2] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 32);\n",
      "/* 030 */     hashAgg_mutableStateArray_0[3] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(hashAgg_mutableStateArray_0[2], 2);\n",
      "/* 031 */\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */   private void hashAgg_doAggregateWithKeys_0() throws java.io.IOException {\n",
      "/* 035 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 036 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 037 */\n",
      "/* 038 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 039 */       double inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 040 */       -1.0 : (inputadapter_row_0.getDouble(0));\n",
      "/* 041 */       InternalRow inputadapter_value_1 = inputadapter_row_0.getStruct(1, 2);\n",
      "/* 042 */       boolean inputadapter_isNull_2 = inputadapter_row_0.isNullAt(2);\n",
      "/* 043 */       double inputadapter_value_2 = inputadapter_isNull_2 ?\n",
      "/* 044 */       -1.0 : (inputadapter_row_0.getDouble(2));\n",
      "/* 045 */\n",
      "/* 046 */       hashAgg_doConsume_0(inputadapter_row_0, inputadapter_value_0, inputadapter_isNull_0, inputadapter_value_1, inputadapter_value_2, inputadapter_isNull_2);\n",
      "/* 047 */       // shouldStop check is eliminated\n",
      "/* 048 */     }\n",
      "/* 049 */\n",
      "/* 050 */     hashAgg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(hashAgg_hashMap_0, hashAgg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* avgHashProbe */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* numTasksFallBacked */));\n",
      "/* 051 */   }\n",
      "/* 052 */\n",
      "/* 053 */   private void hashAgg_doConsume_0(InternalRow inputadapter_row_0, double hashAgg_expr_0_0, boolean hashAgg_exprIsNull_0_0, InternalRow hashAgg_expr_1_0, double hashAgg_expr_2_0, boolean hashAgg_exprIsNull_2_0) throws java.io.IOException {\n",
      "/* 054 */     UnsafeRow hashAgg_unsafeRowAggBuffer_0 = null;\n",
      "/* 055 */\n",
      "/* 056 */     // generate grouping key\n",
      "/* 057 */     hashAgg_mutableStateArray_0[0].reset();\n",
      "/* 058 */\n",
      "/* 059 */     hashAgg_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 060 */\n",
      "/* 061 */     if (hashAgg_exprIsNull_0_0) {\n",
      "/* 062 */       hashAgg_mutableStateArray_0[0].setNullAt(0);\n",
      "/* 063 */     } else {\n",
      "/* 064 */       hashAgg_mutableStateArray_0[0].write(0, hashAgg_expr_0_0);\n",
      "/* 065 */     }\n",
      "/* 066 */\n",
      "/* 067 */     final InternalRow hashAgg_tmpInput_0 = hashAgg_expr_1_0;\n",
      "/* 068 */     if (hashAgg_tmpInput_0 instanceof UnsafeRow) {\n",
      "/* 069 */       hashAgg_mutableStateArray_0[0].write(1, (UnsafeRow) hashAgg_tmpInput_0);\n",
      "/* 070 */     } else {\n",
      "/* 071 */       // Remember the current cursor so that we can calculate how many bytes are\n",
      "/* 072 */       // written later.\n",
      "/* 073 */       final int hashAgg_previousCursor_0 = hashAgg_mutableStateArray_0[0].cursor();\n",
      "/* 074 */\n",
      "/* 075 */       hashAgg_mutableStateArray_0[1].resetRowWriter();\n",
      "/* 076 */\n",
      "/* 077 */       if ((hashAgg_tmpInput_0.isNullAt(0))) {\n",
      "/* 078 */         hashAgg_mutableStateArray_0[1].setNullAt(0);\n",
      "/* 079 */       } else {\n",
      "/* 080 */         hashAgg_mutableStateArray_0[1].write(0, (hashAgg_tmpInput_0.getLong(0)));\n",
      "/* 081 */       }\n",
      "/* 082 */\n",
      "/* 083 */       if ((hashAgg_tmpInput_0.isNullAt(1))) {\n",
      "/* 084 */         hashAgg_mutableStateArray_0[1].setNullAt(1);\n",
      "/* 085 */       } else {\n",
      "/* 086 */         hashAgg_mutableStateArray_0[1].write(1, (hashAgg_tmpInput_0.getLong(1)));\n",
      "/* 087 */       }\n",
      "/* 088 */\n",
      "/* 089 */       hashAgg_mutableStateArray_0[0].setOffsetAndSizeFromPreviousCursor(1, hashAgg_previousCursor_0);\n",
      "/* 090 */     }\n",
      "/* 091 */     int hashAgg_unsafeRowKeyHash_0 = (hashAgg_mutableStateArray_0[0].getRow()).hashCode();\n",
      "/* 092 */     if (true) {\n",
      "/* 093 */       // try to get the buffer from hash map\n",
      "/* 094 */       hashAgg_unsafeRowAggBuffer_0 =\n",
      "/* 095 */       hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow((hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 096 */     }\n",
      "/* 097 */     // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based\n",
      "/* 098 */     // aggregation after processing all input rows.\n",
      "/* 099 */     if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 100 */       if (hashAgg_sorter_0 == null) {\n",
      "/* 101 */         hashAgg_sorter_0 = hashAgg_hashMap_0.destructAndCreateExternalSorter();\n",
      "/* 102 */       } else {\n",
      "/* 103 */         hashAgg_sorter_0.merge(hashAgg_hashMap_0.destructAndCreateExternalSorter());\n",
      "/* 104 */       }\n",
      "/* 105 */\n",
      "/* 106 */       // the hash map had be spilled, it should have enough memory now,\n",
      "/* 107 */       // try to allocate buffer again.\n",
      "/* 108 */       hashAgg_unsafeRowAggBuffer_0 = hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow(\n",
      "/* 109 */         (hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 110 */       if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 111 */         // failed to allocate the first page\n",
      "/* 112 */         throw new org.apache.spark.memory.SparkOutOfMemoryError(\"No enough memory for aggregation\");\n",
      "/* 113 */       }\n",
      "/* 114 */     }\n",
      "/* 115 */\n",
      "/* 116 */     // common sub-expressions\n",
      "/* 117 */\n",
      "/* 118 */     // evaluate aggregate functions and update aggregation buffers\n",
      "/* 119 */     hashAgg_doAggregate_sum_0(hashAgg_unsafeRowAggBuffer_0, hashAgg_expr_2_0, hashAgg_exprIsNull_2_0);\n",
      "/* 120 */\n",
      "/* 121 */   }\n",
      "/* 122 */\n",
      "/* 123 */   private void hashAgg_doAggregate_sum_0(org.apache.spark.sql.catalyst.InternalRow hashAgg_unsafeRowAggBuffer_0, double hashAgg_expr_2_0, boolean hashAgg_exprIsNull_2_0) throws java.io.IOException {\n",
      "/* 124 */     hashAgg_hashAgg_isNull_4_0 = true;\n",
      "/* 125 */     double hashAgg_value_4 = -1.0;\n",
      "/* 126 */     do {\n",
      "/* 127 */       boolean hashAgg_isNull_5 = true;\n",
      "/* 128 */       double hashAgg_value_5 = -1.0;\n",
      "/* 129 */       hashAgg_hashAgg_isNull_6_0 = true;\n",
      "/* 130 */       double hashAgg_value_6 = -1.0;\n",
      "/* 131 */       do {\n",
      "/* 132 */         boolean hashAgg_isNull_7 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 133 */         double hashAgg_value_7 = hashAgg_isNull_7 ?\n",
      "/* 134 */         -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 135 */         if (!hashAgg_isNull_7) {\n",
      "/* 136 */           hashAgg_hashAgg_isNull_6_0 = false;\n",
      "/* 137 */           hashAgg_value_6 = hashAgg_value_7;\n",
      "/* 138 */           continue;\n",
      "/* 139 */         }\n",
      "/* 140 */\n",
      "/* 141 */         if (!false) {\n",
      "/* 142 */           hashAgg_hashAgg_isNull_6_0 = false;\n",
      "/* 143 */           hashAgg_value_6 = 0.0D;\n",
      "/* 144 */           continue;\n",
      "/* 145 */         }\n",
      "/* 146 */\n",
      "/* 147 */       } while (false);\n",
      "/* 148 */\n",
      "/* 149 */       if (!hashAgg_exprIsNull_2_0) {\n",
      "/* 150 */         hashAgg_isNull_5 = false; // resultCode could change nullability.\n",
      "/* 151 */\n",
      "/* 152 */         hashAgg_value_5 = hashAgg_value_6 + hashAgg_expr_2_0;\n",
      "/* 153 */\n",
      "/* 154 */       }\n",
      "/* 155 */       if (!hashAgg_isNull_5) {\n",
      "/* 156 */         hashAgg_hashAgg_isNull_4_0 = false;\n",
      "/* 157 */         hashAgg_value_4 = hashAgg_value_5;\n",
      "/* 158 */         continue;\n",
      "/* 159 */       }\n",
      "/* 160 */\n",
      "/* 161 */       boolean hashAgg_isNull_10 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 162 */       double hashAgg_value_10 = hashAgg_isNull_10 ?\n",
      "/* 163 */       -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 164 */       if (!hashAgg_isNull_10) {\n",
      "/* 165 */         hashAgg_hashAgg_isNull_4_0 = false;\n",
      "/* 166 */         hashAgg_value_4 = hashAgg_value_10;\n",
      "/* 167 */         continue;\n",
      "/* 168 */       }\n",
      "/* 169 */\n",
      "/* 170 */     } while (false);\n",
      "/* 171 */\n",
      "/* 172 */     if (!hashAgg_hashAgg_isNull_4_0) {\n",
      "/* 173 */       hashAgg_unsafeRowAggBuffer_0.setDouble(0, hashAgg_value_4);\n",
      "/* 174 */     } else {\n",
      "/* 175 */       hashAgg_unsafeRowAggBuffer_0.setNullAt(0);\n",
      "/* 176 */     }\n",
      "/* 177 */   }\n",
      "/* 178 */\n",
      "/* 179 */   private void hashAgg_doAggregateWithKeysOutput_0(UnsafeRow hashAgg_keyTerm_0, UnsafeRow hashAgg_bufferTerm_0)\n",
      "/* 180 */   throws java.io.IOException {\n",
      "/* 181 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* numOutputRows */).add(1);\n",
      "/* 182 */\n",
      "/* 183 */     boolean hashAgg_isNull_11 = hashAgg_keyTerm_0.isNullAt(0);\n",
      "/* 184 */     double hashAgg_value_11 = hashAgg_isNull_11 ?\n",
      "/* 185 */     -1.0 : (hashAgg_keyTerm_0.getDouble(0));\n",
      "/* 186 */     InternalRow hashAgg_value_12 = hashAgg_keyTerm_0.getStruct(1, 2);\n",
      "/* 187 */     boolean hashAgg_isNull_13 = hashAgg_bufferTerm_0.isNullAt(0);\n",
      "/* 188 */     double hashAgg_value_13 = hashAgg_isNull_13 ?\n",
      "/* 189 */     -1.0 : (hashAgg_bufferTerm_0.getDouble(0));\n",
      "/* 190 */\n",
      "/* 191 */     hashAgg_mutableStateArray_0[2].reset();\n",
      "/* 192 */\n",
      "/* 193 */     hashAgg_mutableStateArray_0[2].zeroOutNullBytes();\n",
      "/* 194 */\n",
      "/* 195 */     if (hashAgg_isNull_11) {\n",
      "/* 196 */       hashAgg_mutableStateArray_0[2].setNullAt(0);\n",
      "/* 197 */     } else {\n",
      "/* 198 */       hashAgg_mutableStateArray_0[2].write(0, hashAgg_value_11);\n",
      "/* 199 */     }\n",
      "/* 200 */\n",
      "/* 201 */     final InternalRow hashAgg_tmpInput_1 = hashAgg_value_12;\n",
      "/* 202 */     if (hashAgg_tmpInput_1 instanceof UnsafeRow) {\n",
      "/* 203 */       hashAgg_mutableStateArray_0[2].write(1, (UnsafeRow) hashAgg_tmpInput_1);\n",
      "/* 204 */     } else {\n",
      "/* 205 */       // Remember the current cursor so that we can calculate how many bytes are\n",
      "/* 206 */       // written later.\n",
      "/* 207 */       final int hashAgg_previousCursor_1 = hashAgg_mutableStateArray_0[2].cursor();\n",
      "/* 208 */\n",
      "/* 209 */       hashAgg_mutableStateArray_0[3].resetRowWriter();\n",
      "/* 210 */\n",
      "/* 211 */       if ((hashAgg_tmpInput_1.isNullAt(0))) {\n",
      "/* 212 */         hashAgg_mutableStateArray_0[3].setNullAt(0);\n",
      "/* 213 */       } else {\n",
      "/* 214 */         hashAgg_mutableStateArray_0[3].write(0, (hashAgg_tmpInput_1.getLong(0)));\n",
      "/* 215 */       }\n",
      "/* 216 */\n",
      "/* 217 */       if ((hashAgg_tmpInput_1.isNullAt(1))) {\n",
      "/* 218 */         hashAgg_mutableStateArray_0[3].setNullAt(1);\n",
      "/* 219 */       } else {\n",
      "/* 220 */         hashAgg_mutableStateArray_0[3].write(1, (hashAgg_tmpInput_1.getLong(1)));\n",
      "/* 221 */       }\n",
      "/* 222 */\n",
      "/* 223 */       hashAgg_mutableStateArray_0[2].setOffsetAndSizeFromPreviousCursor(1, hashAgg_previousCursor_1);\n",
      "/* 224 */     }\n",
      "/* 225 */\n",
      "/* 226 */     if (hashAgg_isNull_13) {\n",
      "/* 227 */       hashAgg_mutableStateArray_0[2].setNullAt(2);\n",
      "/* 228 */     } else {\n",
      "/* 229 */       hashAgg_mutableStateArray_0[2].write(2, hashAgg_value_13);\n",
      "/* 230 */     }\n",
      "/* 231 */     append((hashAgg_mutableStateArray_0[2].getRow()));\n",
      "/* 232 */\n",
      "/* 233 */   }\n",
      "/* 234 */\n",
      "/* 235 */   protected void processNext() throws java.io.IOException {\n",
      "/* 236 */     if (!hashAgg_initAgg_0) {\n",
      "/* 237 */       hashAgg_initAgg_0 = true;\n",
      "/* 238 */\n",
      "/* 239 */       hashAgg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();\n",
      "/* 240 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();\n",
      "/* 241 */       hashAgg_doAggregateWithKeys_0();\n",
      "/* 242 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[6] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);\n",
      "/* 243 */     }\n",
      "/* 244 */     // output the result\n",
      "/* 245 */\n",
      "/* 246 */     while ( hashAgg_mapIter_0.next()) {\n",
      "/* 247 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_mapIter_0.getKey();\n",
      "/* 248 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_mapIter_0.getValue();\n",
      "/* 249 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 250 */       if (shouldStop()) return;\n",
      "/* 251 */     }\n",
      "/* 252 */     hashAgg_mapIter_0.close();\n",
      "/* 253 */     if (hashAgg_sorter_0 == null) {\n",
      "/* 254 */       hashAgg_hashMap_0.free();\n",
      "/* 255 */     }\n",
      "/* 256 */   }\n",
      "/* 257 */\n",
      "/* 258 */ }\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "purchaseByCustomerPerHour.explain(mode='codegen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "53b3cfa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Optimized Logical Plan ==\n",
      "Aggregate [CustomerID#226, window#321], [CustomerID#226, window#321, sum(total_cost#296) AS sum(total_cost)#306], Statistics(sizeInBytes=2.8 EiB)\n",
      "+- Project [named_struct(start, precisetimestampconversion(((precisetimestampconversion(cast(InvoiceDate#297 as timestamp), TimestampType, LongType) - (((precisetimestampconversion(cast(InvoiceDate#297 as timestamp), TimestampType, LongType) - 0) + 86400000000) % 86400000000)) - 0), LongType, TimestampType), end, precisetimestampconversion((((precisetimestampconversion(cast(InvoiceDate#297 as timestamp), TimestampType, LongType) - (((precisetimestampconversion(cast(InvoiceDate#297 as timestamp), TimestampType, LongType) - 0) + 86400000000) % 86400000000)) - 0) + 86400000000), LongType, TimestampType)) AS window#321, CustomerID#226, total_cost#296], Statistics(sizeInBytes=2.8 EiB)\n",
      "   +- Project [CustomerID#226, (UnitPrice#225 * cast(Quantity#223 as double)) AS total_cost#296, cast(gettimestamp(InvoiceDate#224, M/d/yyyy H:mm, TimestampType, Some(Asia/Tokyo), false) as date) AS InvoiceDate#297], Statistics(sizeInBytes=1977.4 PiB)\n",
      "      +- Filter isnotnull(cast(cast(gettimestamp(InvoiceDate#224, M/d/yyyy H:mm, TimestampType, Some(Asia/Tokyo), false) as date) as timestamp)), Statistics(sizeInBytes=8.0 EiB)\n",
      "         +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@49f4de59,csv,List(),Some(StructType(StructField(InvoiceNo,StringType,true),StructField(StockCode,StringType,true),StructField(Description,StringType,true),StructField(Quantity,IntegerType,true),StructField(InvoiceDate,TimestampType,true),StructField(UnitPrice,DoubleType,true),StructField(CustomerID,DoubleType,true),StructField(Country,StringType,true))),List(),None,Map(maxFilesPerTrigger -> 1, header -> true, path -> spark-the-definitive-guide-study/assets/exercises/week03/by-day/*.csv),None), FileSource[spark-the-definitive-guide-study/assets/exercises/week03/by-day/*.csv], [InvoiceNo#220, StockCode#221, Description#222, Quantity#223, InvoiceDate#224, UnitPrice#225, CustomerID#226, Country#227], Statistics(sizeInBytes=8.0 EiB)\n",
      "\n",
      "== Physical Plan ==\n",
      "*(4) HashAggregate(keys=[CustomerID#226, window#321], functions=[sum(total_cost#296)], output=[CustomerID#226, window#321, sum(total_cost)#306])\n",
      "+- StateStoreSave [CustomerID#226, window#321], state info [ checkpoint = <unknown>, runId = 97a982a0-3b70-48d2-aece-9e038305eb2a, opId = 0, ver = 0, numPartitions = 200], Append, 0, 2\n",
      "   +- *(3) HashAggregate(keys=[CustomerID#226, window#321], functions=[merge_sum(total_cost#296)], output=[CustomerID#226, window#321, sum#313])\n",
      "      +- StateStoreRestore [CustomerID#226, window#321], state info [ checkpoint = <unknown>, runId = 97a982a0-3b70-48d2-aece-9e038305eb2a, opId = 0, ver = 0, numPartitions = 200], 2\n",
      "         +- *(2) HashAggregate(keys=[CustomerID#226, window#321], functions=[merge_sum(total_cost#296)], output=[CustomerID#226, window#321, sum#313])\n",
      "            +- Exchange hashpartitioning(CustomerID#226, window#321, 200), ENSURE_REQUIREMENTS, [plan_id=842]\n",
      "               +- *(1) HashAggregate(keys=[knownfloatingpointnormalized(normalizenanandzero(CustomerID#226)) AS CustomerID#226, window#321], functions=[partial_sum(total_cost#296)], output=[CustomerID#226, window#321, sum#313])\n",
      "                  +- *(1) Project [named_struct(start, precisetimestampconversion(((precisetimestampconversion(cast(InvoiceDate#297 as timestamp), TimestampType, LongType) - (((precisetimestampconversion(cast(InvoiceDate#297 as timestamp), TimestampType, LongType) - 0) + 86400000000) % 86400000000)) - 0), LongType, TimestampType), end, precisetimestampconversion((((precisetimestampconversion(cast(InvoiceDate#297 as timestamp), TimestampType, LongType) - (((precisetimestampconversion(cast(InvoiceDate#297 as timestamp), TimestampType, LongType) - 0) + 86400000000) % 86400000000)) - 0) + 86400000000), LongType, TimestampType)) AS window#321, CustomerID#226, total_cost#296]\n",
      "                     +- *(1) Project [CustomerID#226, (UnitPrice#225 * cast(Quantity#223 as double)) AS total_cost#296, cast(gettimestamp(InvoiceDate#224, M/d/yyyy H:mm, TimestampType, Some(Asia/Tokyo), false) as date) AS InvoiceDate#297]\n",
      "                        +- *(1) Filter isnotnull(cast(cast(gettimestamp(InvoiceDate#224, M/d/yyyy H:mm, TimestampType, Some(Asia/Tokyo), false) as date) as timestamp))\n",
      "                           +- StreamingRelation FileSource[spark-the-definitive-guide-study/assets/exercises/week03/by-day/*.csv], [InvoiceNo#220, StockCode#221, Description#222, Quantity#223, InvoiceDate#224, UnitPrice#225, CustomerID#226, Country#227]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "purchaseByCustomerPerHour.explain(mode='cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a0a6d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
