{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "otherwise-museum",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "incoming-maryland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로컬 모드로 이 코드를 실행하려면 로컬 모드에 적합한 셔플 파티션 수를 설정해 주는 것이 좋다.\n",
    "# 기본 200이지만, 로컬 모드에서는 많은 익스큐터가 필요하지 않기 때문에 5로 설정한다.\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\n",
    "\n",
    "staticDataFrame = spark.read.format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".load(\"../../assets/exercises/week03/all/online-retail-dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operating-bridges",
   "metadata": {},
   "source": [
    "## 학습 데이터셋 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "absent-awareness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 날짜 데이터 변환\n",
    "preppedDataFrame = staticDataFrame\\\n",
    ".na.fill(0)\\\n",
    ".withColumn(\"day_of_week\", F.date_format(F.to_timestamp(F.col(\"InvoiceDate\"), \"M/d/yyyy H:mm\"), \"EEEE\"))\\\n",
    ".coalesce(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "illegal-feeding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(InvoiceNo='536365', StockCode='85123A', Description='WHITE HANGING HEART T-LIGHT HOLDER', Quantity=6, InvoiceDate='12/1/2010 8:26', UnitPrice=2.55, CustomerID=17850, Country='United Kingdom', day_of_week='Wednesday'),\n",
       " Row(InvoiceNo='536365', StockCode='71053', Description='WHITE METAL LANTERN', Quantity=6, InvoiceDate='12/1/2010 8:26', UnitPrice=3.39, CustomerID=17850, Country='United Kingdom', day_of_week='Wednesday'),\n",
       " Row(InvoiceNo='536365', StockCode='84406B', Description='CREAM CUPID HEARTS COAT HANGER', Quantity=8, InvoiceDate='12/1/2010 8:26', UnitPrice=2.75, CustomerID=17850, Country='United Kingdom', day_of_week='Wednesday')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preppedDataFrame.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "addressed-resource",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(date)=datetime.date(2011, 12, 9))]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preppedDataFrame\\\n",
    ".withColumn(\"date\", F.to_date(F.to_timestamp(F.col(\"InvoiceDate\"), \"M/d/yyyy H:mm\")))\\\n",
    ".select(F.max(\"date\")).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "virgin-duration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(min(date)=datetime.date(2010, 12, 1))]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preppedDataFrame\\\n",
    ".withColumn(\"date\", F.to_date(F.to_timestamp(F.col(\"InvoiceDate\"), \"M/d/yyyy H:mm\")))\\\n",
    ".select(F.min(\"date\")).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "framed-insulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터셋과 테스트 데이터셋 분리\n",
    "trainDataFrame = preppedDataFrame\\\n",
    ".where(\"InvoiceDate < '2011-11-05'\")\n",
    "       \n",
    "testDataFrame = preppedDataFrame\\\n",
    "       .where(\"InvoiceDate >= '2011-11-05'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "super-german",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Dataset: 276313\n",
      "test Dataset: 265596\n"
     ]
    }
   ],
   "source": [
    "print(f'train Dataset: {trainDataFrame.count()}')\n",
    "print(f'test Dataset: {testDataFrame.count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "homeless-northwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLlib은 일번적인 트랜스포이션을 자동화하는 다양한 트랜스포메이션 제공\n",
    "# 요일을 수치형으로 반환\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer()\\\n",
    ".setInputCol(\"day_of_week\")\\\n",
    ".setOutputCol(\"day_of_week_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "alert-amsterdam",
   "metadata": {},
   "outputs": [],
   "source": [
    "# onehotencoding\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder()\\\n",
    ".setInputCol(\"day_of_week_index\")\\\n",
    ".setOutputCol(\"day_of_week_encoded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "pretty-roller",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터화\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vectorAssembler = VectorAssembler()\\\n",
    ".setInputCols([\"UnitPrice\", \"Quantity\", \"day_of_week_encoded\"])\\\n",
    ".setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dependent-dealing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변환자를 사용해서 데이터셋에 변환자를 fit 시킨다.\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "transformationPipeline = Pipeline()\\\n",
    ".setStages([indexer, encoder, vectorAssembler])\n",
    "\n",
    "fittedPipeline = transformationPipeline.fit(trainDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "artificial-stereo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위의 과정에서 학습을 위한 맞춤 파이프라인이 준비되면, 이것을 사용해서 일관되고 반복적인 방식으로 모든 데이터 변환이 가능하다.\n",
    "transformedTraning = fittedPipeline.transform(trainDataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lightweight-timeline",
   "metadata": {},
   "source": [
    "## 모델 학습\n",
    "1. 아직 학습되지 않은 모델을 초기화\n",
    "2. 해당 모델을 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "historical-syracuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://spark.apache.org/docs/latest/ml-clustering.html\n",
    "# 캐싱\n",
    "transformedTraning.cache()\n",
    "\n",
    "# 모델 학습\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "kmeans = KMeans().setK(20).setSeed(1)\n",
    "kmModel = kmeans.fit(transformedTraning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "quality-hurricane",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeansModel: uid=KMeans_7aed4674187c, k=20, distanceMeasure=euclidean, numFeatures=7"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "productive-livestock",
   "metadata": {},
   "source": [
    "## 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "empty-wireless",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.9570197876949359\n"
     ]
    }
   ],
   "source": [
    "transformedTest = fittedPipeline.transform(testDataFrame)\n",
    "# kmModel.computeCost(transformedTest)\n",
    "# ---------------------------------------------------------------------------\n",
    "# AttributeError                            Traceback (most recent call last)\n",
    "# <ipython-input-49-1a60a70baffb> in <module>\n",
    "#       1 transformedTest = fittedPipeline.transform(testDataFrame)\n",
    "# ----> 2 kmModel.computeCost(transformedTest)\n",
    "\n",
    "# AttributeError: 'KMeansModel' object has no attribute 'computeCost'\n",
    "\n",
    "####### ####### ####### #######\n",
    "# \"Deprecated in 3.0.0. It will be removed in future versions. Use \"\n",
    "#             \"ClusteringEvaluator instead. You can also get the cost on the training \"\n",
    "#             \"dataset in the summary.\"\n",
    "####### ####### ####### #######\n",
    "\n",
    "\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "pdt = kmModel.transform(transformedTest)\n",
    "# Evaluate clustering by computing Silhouette score\n",
    "evaluator = ClusteringEvaluator()\n",
    "silhouette = evaluator.evaluate(pdt)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-juvenile",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
